import asyncio
import json
import logging
import os
from datetime import datetime, timedelta
from typing import List, Dict

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

from app.core.config import config
from app.memory.vector_store import vector_db
from app.core.global_store import global_store
from app.utils.cache import cached_llm_invoke

logger = logging.getLogger("DreamCycle")

# --- è®°å¿†å›ºåŒ– Prompt ---
CONSOLIDATION_PROMPT = """
ä½ æ˜¯ Alice çš„æ½œæ„è¯†æ•´ç†è€…ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç¢ç‰‡åŒ–çš„çŸ­æœŸè®°å¿†åˆå¹¶ä¸ºæœ‰ä»·å€¼çš„é•¿æœŸè®°å¿†ã€‚

ã€å¾…æ•´ç†çš„è®°å¿†ç¢ç‰‡ã€‘
{fragments}

ã€ä»»åŠ¡è¦æ±‚ã€‘
1. åˆ†æè¿™äº›ç¢ç‰‡ä¹‹é—´æ˜¯å¦å­˜åœ¨å…³è”ï¼ˆä¾‹å¦‚ï¼šéƒ½æ˜¯å…³äºé¥®é£Ÿåå¥½ã€éƒ½æ˜¯å…³äºæŸä¸ªç‰¹å®šé¡¹ç›®ã€æˆ–è€…æ˜¯è¿ç»­çš„äº‹ä»¶ï¼‰ã€‚
2. å¦‚æœå­˜åœ¨å…³è”ï¼Œè¯·å°†å®ƒä»¬**æ¦‚æ‹¬**ä¸ºä¸€æ¡ç®€æ´çš„ã€åŒ…å«æ ¸å¿ƒä¿¡æ¯çš„é™ˆè¿°å¥ã€‚
3. æ¦‚æ‹¬åçš„è®°å¿†åº”å½“å»é™¤æ—¶é—´çŠ¶è¯­ï¼ˆå¦‚â€œåˆšæ‰â€ã€â€œä»Šå¤©â€ï¼‰ï¼Œè½¬å˜ä¸ºæŒä¹…çš„äº‹å®æè¿°ã€‚
4. å¦‚æœç¢ç‰‡ä¹‹é—´æ²¡æœ‰æ˜æ˜¾å…³è”ï¼Œæˆ–è€…ä¿¡æ¯å¤ªæ‚ä¹±æ— æ³•åˆå¹¶ï¼Œè¯·è¾“å‡º "SKIP"ã€‚

ã€è¾“å‡ºç¤ºä¾‹ã€‘
è¾“å…¥ç¢ç‰‡: ["ç”¨æˆ·è¯´ä»Šå¤©æƒ³åƒè¾£", "ä¸­åˆç‚¹äº†éº»è¾£çƒ«", "æ™šä¸Šè¿˜åœ¨æ‰¾ç«é”…åº—"]
è¾“å‡º: ç”¨æˆ·éå¸¸å–œæ¬¢åƒè¾£çš„é£Ÿç‰©ï¼Œå°¤å…¶æ˜¯éº»è¾£çƒ«å’Œç«é”…ã€‚

è¯·è¾“å‡ºç»“æœ (çº¯æ–‡æœ¬):
"""


class DreamCycle:
    def __init__(self, interval_seconds=1800):
        """
        :param interval_seconds: åšæ¢¦å¾ªç¯çš„é—´éš”ï¼Œé»˜è®¤ 30 åˆ†é’Ÿ (1800ç§’)
        """
        self.interval = interval_seconds
        self.running = False
        self._task = None

        # ä¸“é—¨ç”¨äºæ•´ç†è®°å¿†çš„ LLMï¼Œå¯ä»¥ä½¿ç”¨ä¾¿å®œçš„æ¨¡å‹ (å¦‚ gpt-3.5-turbo æˆ– qwen-turbo)
        self.llm = ChatOpenAI(
            model=config.MODEL_NAME,
            temperature=0.1,
            api_key=config.MODEL_API_KEY,
            base_url=config.MODEL_URL
        )

    async def start(self):
        # åœ¨Windowsä¸Šä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿åªæœ‰ä¸€ä¸ªè¿›ç¨‹èƒ½å¯åŠ¨DreamCycle
        lock_file_path = os.path.join(os.path.dirname(__file__), "dream_lock.lock")
        lock_file = None
        
        try:
            # å°è¯•æ‰“å¼€é”æ–‡ä»¶
            lock_file = open(lock_file_path, 'w')
            
            # æ£€æŸ¥æ“ä½œç³»ç»Ÿç±»å‹
            if os.name == 'nt':  # Windows
                # åœ¨Windowsä¸Šä½¿ç”¨msvcrt.lockæ¥è·å–æ–‡ä»¶é”
                import msvcrt
                try:
                    msvcrt.locking(lock_file.fileno(), msvcrt.LK_NBLCK, 1)
                    # å¦‚æœæˆåŠŸè·å–é”ï¼Œä¿å­˜æ–‡ä»¶å¯¹è±¡å¹¶å¯åŠ¨DreamCycle
                    self._lock_file = lock_file
                    self.running = True
                    self._task = asyncio.create_task(self._dream_loop())
                    logger.info("ğŸ’¤ [Dream] Background memory consolidation module started.")
                except IOError:
                    # æ— æ³•è·å–é”ï¼Œè¯´æ˜å·²ç»æœ‰å…¶ä»–è¿›ç¨‹åœ¨è¿è¡ŒDreamCycle
                    logger.info("ğŸ’¤ [DreamCycle] Already running in another process. Skipping startup.")
                    lock_file.close()
                    return
            else:  # éWindows
                # å¦‚æœæ˜¯åœ¨éWindowså¹³å°ï¼Œä½¿ç”¨fcntl
                try:
                    import fcntl
                    fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
                    # å¦‚æœæˆåŠŸè·å–é”ï¼Œä¿å­˜æ–‡ä»¶å¯¹è±¡å¹¶å¯åŠ¨DreamCycle
                    self._lock_file = lock_file
                    self.running = True
                    self._task = asyncio.create_task(self._dream_loop())
                    logger.info("ğŸ’¤ [Dream] Background memory consolidation module started.")
                except (BlockingIOError, IOError):
                    # æ— æ³•è·å–é”ï¼Œè¯´æ˜å·²ç»æœ‰å…¶ä»–è¿›ç¨‹åœ¨è¿è¡ŒDreamCycle
                    logger.info("ğŸ’¤ [DreamCycle] Already running in another process. Skipping startup.")
                    lock_file.close()
                    return
        except Exception as e:
            # å¤„ç†å…¶ä»–å¯èƒ½çš„å¼‚å¸¸
            logger.error(f"ğŸ’¤ [DreamCycle] Error during startup: {e}")
            if lock_file:
                lock_file.close()
            return

    async def stop(self):
        self.running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        
        # é‡Šæ”¾æ–‡ä»¶é”
        if hasattr(self, '_lock_file') and self._lock_file:
            try:
                import msvcrt
                msvcrt.locking(self._lock_file.fileno(), msvcrt.LK_UNLCK, 1)
            except ImportError:
                try:
                    import fcntl
                    fcntl.flock(self._lock_file.fileno(), fcntl.LOCK_UN)
                except:
                    pass
            finally:
                self._lock_file.close()
                logger.info("ğŸ’¤ [Dream] Background memory consolidation module stopped.")

    async def _dream_loop(self):
        while self.running:
            try:
                # ç­‰å¾…ä¸‹ä¸€ä¸ªå‘¨æœŸ
                await asyncio.sleep(self.interval)

                # 1. æ£€æŸ¥æ´»è·ƒåº¦ï¼šå¦‚æœç”¨æˆ·æœ€è¿‘ 5 åˆ†é’Ÿè¿˜åœ¨è¯´è¯ï¼Œä¸è¦åšæ¢¦ï¼Œé¿å…æ•°æ®åº“é”å†²çª
                last_active_str = global_store.get_emotion_snapshot().last_updated
                last_active = datetime.strptime(last_active_str, "%Y-%m-%d %H:%M:%S")
                if (datetime.now() - last_active).total_seconds() < 300:
                    logger.info("ğŸ’¤ [Dream] User is active. Postponing dream cycle.")
                    continue

                logger.info("ğŸ’¤ [Dream] Entering REM sleep (Memory Optimization)...")

                # 2. æ‰§è¡Œæ¸…ç†é€»è¾‘
                deleted_count = self._prune_garbage_memories(days_threshold=3)

                # 3. æ‰§è¡Œå›ºåŒ–é€»è¾‘
                consolidated_count = await self._consolidate_memories()

                # 4. æ¢å¤ä½“åŠ› (ä½œä¸ºå¥–åŠ±)
                if deleted_count > 0 or consolidated_count > 0:
                    global_store.update_emotion(0, 0, stamina_delta=30.0)
                    logger.info(
                        f"ğŸ’¤ [Dream] Cycle Done. Pruned: {deleted_count}, Consolidated: {consolidated_count}. Stamina Recovered.")
                else:
                    logger.info("ğŸ’¤ [Dream] Deep sleep. No memories needed processing.")

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ [Dream Error] {e}", exc_info=True)

    def _prune_garbage_memories(self, days_threshold: int = 3) -> int:
        """
        æ¸…ç†é€»è¾‘ï¼šåˆ é™¤ [importance=1] ä¸” [åˆ›å»ºæ—¶é—´ > 3å¤©] çš„è®°å¿†
        """
        try:
            # Chroma API è·å–æ‰€æœ‰ metadata (limit è®¾å¤§ä¸€ç‚¹ä»¥è¦†ç›–)
            # æ³¨æ„ï¼šå¦‚æœæ•°æ®é‡å·¨å¤§ï¼Œè¿™é‡Œéœ€è¦åˆ†é¡µå¤„ç†ï¼ŒDemo ä¸­æš‚ä¸”ä¸€æ¬¡æ€§è·å–
            result = vector_db.collection.get(include=["metadatas"])

            ids = result["ids"]
            metadatas = result["metadatas"]

            ids_to_delete = []
            now = datetime.now()
            cutoff_date = now - timedelta(days=days_threshold)

            for i, meta in enumerate(metadatas):
                # æ£€æŸ¥ Importance (å¦‚æœæ²¡æœ‰å­—æ®µï¼Œé»˜è®¤ä¸º 1)
                importance = meta.get("importance", 1)

                # åªæ¸…ç†ä½æƒé‡è®°å¿†
                if importance > 1:
                    continue

                # æ£€æŸ¥æ—¶é—´
                created_at_str = meta.get("created_at")
                if created_at_str:
                    try:
                        mem_time = datetime.strptime(created_at_str, "%Y-%m-%d %H:%M:%S")
                        if mem_time < cutoff_date:
                            ids_to_delete.append(ids[i])
                    except ValueError:
                        continue  # æ—¶é—´æ ¼å¼ä¸å¯¹åˆ™è·³è¿‡

            if ids_to_delete:
                logger.info(f"ğŸ§¹ [Dream] Pruning {len(ids_to_delete)} garbage memories...")
                vector_db.collection.delete(ids=ids_to_delete)
                return len(ids_to_delete)

            return 0

        except Exception as e:
            logger.error(f"Error in pruning: {e}")
            return 0

    async def _consolidate_memories(self) -> int:
        """
        å›ºåŒ–é€»è¾‘ï¼š
        1. æ‰¾å‡ºæœ€è¿‘ 24 å°æ—¶äº§ç”Ÿçš„ã€importance=2 (Context) æˆ– 3 (Preference) çš„è®°å¿†ã€‚
        2. å¦‚æœç¢ç‰‡æ•°é‡ > 3ï¼Œå°è¯•è®© LLM æ€»ç»“ã€‚
        3. å¦‚æœæ€»ç»“æˆåŠŸï¼Œå†™å…¥ä¸€æ¡ importance=4 çš„æ–°è®°å¿†ï¼Œå¹¶åˆ é™¤æ—§ç¢ç‰‡ã€‚
        """
        try:
            # 1. è·å–æœ€è¿‘è®°å¿†
            result = vector_db.collection.get(include=["documents", "metadatas"])
            ids = result["ids"]
            docs = result["documents"]
            metadatas = result["metadatas"]

            candidates = []  # list of (id, doc, meta)
            now = datetime.now()

            # ç­›é€‰ï¼šæœ€è¿‘ 24 å°æ—¶ ä¸” é‡è¦æ€§ä¸º 2 æˆ– 3
            for i, meta in enumerate(metadatas):
                imp = meta.get("importance", 1)
                if imp not in [2, 3]:
                    continue

                c_time_str = meta.get("created_at")
                if not c_time_str: continue

                try:
                    mem_time = datetime.strptime(c_time_str, "%Y-%m-%d %H:%M:%S")
                    # åªçœ‹æœ€è¿‘ 24 å°æ—¶
                    if (now - mem_time).total_seconds() < 86400:
                        candidates.append((ids[i], docs[i]))
                except:
                    continue

            # å¦‚æœç¢ç‰‡å¤ªå°‘ï¼Œæ²¡å¿…è¦æ€»ç»“
            if len(candidates) < 4:
                return 0

            # 2. å‡†å¤‡ Prompt æ•°æ® (å–å‰ 10 æ¡å¤„ç†ï¼Œé¿å… token çˆ†ç‚¸)
            batch = candidates[:10]
            batch_texts = [item[1] for item in batch]
            batch_ids = [item[0] for item in batch]

            fragments_text = json.dumps(batch_texts, ensure_ascii=False, indent=2)

            # 3. LLM æ€è€ƒ
            logger.info(f"ğŸ§  [Dream] Attempting to consolidate {len(batch)} fragments...")

            prompt = CONSOLIDATION_PROMPT.format(fragments=fragments_text)
            response = await cached_llm_invoke(self.llm, [SystemMessage(content=prompt)], temperature=self.llm.temperature)
            result_text = response.content.strip()

            # 4. å¤„ç†ç»“æœ
            if "SKIP" in result_text or len(result_text) < 5:
                # æ— æ³•åˆå¹¶ï¼Œä¿æŒåŸæ ·
                return 0

            # 5. æ‰§è¡Œâ€œæ–°é™ˆä»£è°¢â€
            logger.info(f"âœ¨ [Dream] Consolidation Success: '{result_text}'")

            # A. å†™å…¥æ–°è®°å¿† (Importance = 4, è¡¨ç¤ºè¿™æ˜¯ç»è¿‡æ·±æ€ç†Ÿè™‘çš„äº‹å®)
            new_metadata = {
                "source": "dream_consolidation",
                "importance": 4,
                "created_at": now.strftime("%Y-%m-%d %H:%M:%S"),
                "consolidated_from_count": len(batch)
            }
            await vector_db.add_texts([result_text], [new_metadata])

            # B. åˆ é™¤æ—§ç¢ç‰‡ (ç‰©ç†åˆ é™¤ï¼Œé‡Šæ”¾ç©ºé—´)
            # vector_db.collection.delete(ids=batch_ids) # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œä¸ºäº†è°ƒè¯•å®‰å…¨ã€‚ç¡®è®¤ç¨³å®šåå–æ¶ˆæ³¨é‡Šã€‚
            # è¿™é‡Œæˆ‘ä»¬åšä¸€ä¸ªæŠ˜ä¸­ï¼šä¸åˆ é™¤ï¼Œè€Œæ˜¯å°†å…¶ importance é™çº§ä¸º 0ï¼Œç­‰å¾…ä¸‹æ¬¡ Pruning æ¸…ç†
            # ä½† Chroma update æ¯”è¾ƒéº»çƒ¦ï¼Œæ‰€ä»¥ç›´æ¥åˆ é™¤æ˜¯æ¯”è¾ƒå¹²å‡€çš„åšæ³•ã€‚
            # ç”Ÿäº§ç¯å¢ƒå»ºè®®å¼€å¯åˆ é™¤ï¼š
            vector_db.collection.delete(ids=batch_ids)

            return 1

        except Exception as e:
            logger.error(f"Error in consolidation: {e}")
            return 0


# å•ä¾‹å¯¼å‡º
dream_machine = DreamCycle(interval_seconds=1800)
