import json
import time
import logging
from datetime import datetime
from typing import List, Any
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from app.core.state import AgentState
from app.core.config import config
from app.core.global_store import global_store
from app.memory.relation_db import relation_db
from app.core.prompts import SOCIAL_VOLITION_PROMPT
from app.utils.cache import cached_llm_invoke

# é…ç½®æ—¥å¿—
logger = logging.getLogger("ProactiveAgent")

# å»ºè®®ä½¿ç”¨é€»è¾‘èƒ½åŠ›è¾ƒå¼ºçš„æ¨¡å‹ (å¦‚ GPT-4o, Qwen-72B)
llm = ChatOpenAI(
    model=config.MODEL_NAME,
    temperature=0.8,  # ç¨å¾®é«˜ä¸€ç‚¹çš„æ¸©åº¦ï¼Œè®©ä¸»åŠ¨å‘è¨€æ›´æœ‰çµæ€§
    api_key=config.MODEL_API_KEY,
    base_url=config.MODEL_URL
)


def _get_time_period(dt: datetime) -> str:
    h = dt.hour
    if 0 <= h < 5: return "æ·±å¤œ/å‡Œæ™¨"
    if 5 <= h < 9: return "æ—©æ™¨"
    if 9 <= h < 12: return "ä¸Šåˆ"
    if 12 <= h < 14: return "ä¸­åˆ"
    if 14 <= h < 18: return "ä¸‹åˆ"
    if 18 <= h < 23: return "æ™šä¸Š"
    return "æ·±å¤œ"


async def proactive_node(state: AgentState):
    """
    ä¸»åŠ¨ç¤¾äº¤å¼•æ“ (Social Volition Engine) - è§†è§‰å¢å¼ºç‰ˆ
    ç»¼åˆåˆ¤æ–­å›¾ç‰‡æ€§è´¨(å®å›¾/è¡¨æƒ…åŒ…)ã€æœ€è¿‘æ–‡æœ¬æ¶ˆæ¯ã€æ²‰é»˜æ—¶é•¿å’Œå¥½æ„Ÿåº¦ã€‚
    """
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    logger.info(f"[{ts}] --- [Proactive] Analyzing Social Context... ---")

    # è·å–åŸºç¡€ä¸Šä¸‹æ–‡
    context = _get_basic_context(state, ts)
    if not context:
        return {"next_step": "silent"}
    
    user_id, user_display_name, is_group, session_id, msgs, ts = context
    
    # è®¡ç®—æ²‰é»˜æ—¶é•¿
    silence_info = _calculate_silence_duration(state)
    silence_seconds, silence_str = silence_info
    
    # å¤„ç†å†å²æ¶ˆæ¯
    history_str = _process_history_messages(msgs)
    
    # å¤„ç†è§†è§‰ä¿¡æ¯
    visual_info = _process_visual_information(state)
    image_data, visual_type, vision_desc = visual_info
    
    # è·å–ç”¨æˆ·å…³ç³»æ•°æ®
    user_relation = await _get_user_relation_data(user_id)
    profile, rel, user_tags, user_birthday, user_hobbies = user_relation
    
    # è·å–ç¯å¢ƒå’Œæƒ…ç»ªä¿¡æ¯
    environment_info = _get_environment_info(state)
    emotion, now_dt, summary = environment_info
    
    # æ„å»ºä¸ªæ€§åŒ–ä¿¡æ¯
    personalized_info = _build_personalized_info(is_group, user_hobbies, user_birthday, user_tags)
    
    # æ„é€ ç³»ç»Ÿæç¤º
    prompt = _build_system_prompt(
        now_dt, silence_str, emotion, user_display_name, rel, vision_desc, 
        summary, is_group, personalized_info
    )
    
    # æ„å»ºè¾“å…¥æ¶ˆæ¯
    input_msgs = await _build_input_messages(
        prompt, visual_type, image_data, history_str, is_group, silence_seconds, 
        silence_str, user_display_name, rel, ts
    )
    
    # è°ƒç”¨LLMå¹¶å¤„ç†å“åº”
    llm_response = await _process_llm_response(input_msgs, ts)
    if not llm_response:
        return {"next_step": "silent"}
    
    intent, reply_content, reason = llm_response
    
    # è¿‡æ»¤ä¸åˆé€‚çš„å›å¤
    if is_group and intent != "silent":
        if not _filter_group_reply(reply_content, ts):
            return {"next_step": "silent"}
    
    # ä¸ªæ€§åŒ–å›å¤å†…å®¹
    if intent != "silent" and reply_content:
        reply_content = _personalize_reply_content(
            reply_content, is_group, rel, user_hobbies
        )
    
    # å¤„ç†æ²‰é»˜æ„å›¾
    if intent == "silent" or not reply_content:
        return {"next_step": "silent"}
    
    # æ¶ˆè€—ä½“åŠ›å¹¶æ„å»ºAIæ¶ˆæ¯
    return _finalize_response(
        intent, reply_content, reason, is_group, msgs, rel, ts
    )


def _get_basic_context(state: AgentState, ts: str):
    """
    è·å–åŸºç¡€ä¸Šä¸‹æ–‡ä¿¡æ¯
    """
    try:
        user_id = state.get("sender_qq", "unknown")
        user_display_name = state.get("sender_name", "User")
        is_group = state.get("is_group", False)
        session_id = state.get("session_id", "unknown")
        msgs = state.get("messages", [])
        return user_id, user_display_name, is_group, session_id, msgs, ts
    except Exception as e:
        logger.error(f"[{ts}] âŒ [Proactive] Failed to get basic context: {e}")
        return None


def _calculate_silence_duration(state: AgentState):
    """
    è®¡ç®—æ²‰é»˜æ—¶é•¿
    """
    last_ts = state.get("last_interaction_ts", time.time())
    now_ts = time.time()
    silence_seconds = now_ts - last_ts

    if silence_seconds < 60:
        silence_str = "åˆšåˆš (ç”¨æˆ·å¯èƒ½è¿˜åœ¨è¾“å…¥ä¸­æˆ–åˆšå‘å®Œæ¶ˆæ¯)"
    elif silence_seconds < 3600:
        silence_str = f"{int(silence_seconds // 60)}åˆ†é’Ÿå‰"
    else:
        silence_str = f"{int(silence_seconds // 3600)}å°æ—¶å‰"
    
    return silence_seconds, silence_str


def _process_history_messages(msgs: List[Any]):
    """
    å¤„ç†å†å²æ¶ˆæ¯ï¼Œç”Ÿæˆå†å²å­—ç¬¦ä¸²
    """
    history_str = ""
    for i, m in enumerate(msgs):
        role = "AI(Alice)" if isinstance(m, (SystemMessage, dict)) or getattr(m, 'type', '') == "ai" else "User"
        content = m.content
        if isinstance(content, list):
            text_part = next((x['text'] for x in content if x.get('type') == 'text'), "")
            if not text_part: text_part = "[Image/RichMedia]"
            content = text_part

        # ç®€å•æˆªæ–­è¿‡é•¿æ¶ˆæ¯é˜²æ­¢ Prompt çˆ†ç‚¸
        content_str = str(content)
        if len(content_str) > 100: content_str = content_str[:100] + "..."

        prefix = ">> [LATEST MSG] " if i == len(msgs) - 1 else ""
        history_str += f"{prefix}[{role}]: {content_str}\n"
    
    return history_str


def _process_visual_information(state: AgentState):
    """
    å¤„ç†è§†è§‰ä¿¡æ¯
    """
    # image_data ä»…åœ¨ visual_type='photo' æ—¶ç”± perception èŠ‚ç‚¹å¡«å……ï¼Œè¡¨æƒ…åŒ…æ—¶ä¸å¡«å……ä»¥èŠ‚çœå†…å­˜
    image_data = state.get("current_image_artifact")
    visual_type = state.get("visual_type", "none")  # 'photo', 'sticker', 'icon', 'none'

    vision_desc = "æ— å›¾ç‰‡"
    if visual_type == "photo":
        vision_desc = "ã€ç”¨æˆ·å‘é€äº†ä¸€å¼ å«æœ‰å…·ä½“ä¿¡æ¯çš„å›¾ç‰‡/æˆªå›¾ï¼Œè¯·ç»“åˆå›¾ç‰‡å†…å®¹åˆ†æã€‘"
    elif visual_type == "sticker":
        vision_desc = "ã€ç”¨æˆ·å‘é€äº†ä¸€ä¸ªè¡¨æƒ…åŒ…/Stickerï¼Œé€šå¸¸ç”¨äºè¡¨è¾¾æƒ…ç»ªæˆ–ç©ç¬‘ã€‘"
    
    return image_data, visual_type, vision_desc


async def _get_user_relation_data(user_id: str):
    """
    è·å–ç”¨æˆ·å…³ç³»æ•°æ®
    """
    profile = await relation_db.get_user_profile(user_id)
    rel = profile.relationship
    # è·å–ç”¨æˆ·çš„ä¸ªæ€§åŒ–ä¿¡æ¯
    user_tags = rel.tags if rel.tags else []
    user_birthday = getattr(profile, 'birthday', None)
    user_hobbies = getattr(profile, 'hobbies', [])
    
    return profile, rel, user_tags, user_birthday, user_hobbies


def _get_environment_info(state: AgentState):
    """
    è·å–ç¯å¢ƒå’Œæƒ…ç»ªä¿¡æ¯
    """
    emotion = global_store.get_emotion_snapshot()
    now_dt = datetime.now()
    summary = state.get("conversation_summary", "æ— æœ€è¿‘å¯¹è¯è®°å½•")
    
    return emotion, now_dt, summary


def _build_personalized_info(is_group: bool, user_hobbies: List[str], 
                            user_birthday: str, user_tags: List[str]):
    """
    æ„å»ºä¸ªæ€§åŒ–ä¿¡æ¯
    """
    if not is_group:
        # ç§èŠåœºæ™¯ï¼šæ·»åŠ ç”¨æˆ·ä¸ªæ€§åŒ–ä¿¡æ¯
        personalized_info = ""
        if user_hobbies:
            personalized_info += f"ç”¨æˆ·çš„å…´è¶£çˆ±å¥½åŒ…æ‹¬ï¼š{', '.join(user_hobbies)}ã€‚\n"
        if user_birthday:
            try:
                birthday = datetime.strptime(user_birthday, "%Y-%m-%d")
                today = datetime.now()
                days_until_birthday = (birthday.replace(year=today.year) - today).days
                if 0 <= days_until_birthday <= 7:
                    personalized_info += f"ç”¨æˆ·çš„ç”Ÿæ—¥å¿«åˆ°äº†ï¼ˆ{user_birthday}ï¼‰ï¼Œå¯ä»¥é€‚å½“è¡¨ç¤ºå…³å¿ƒã€‚\n"
            except:
                pass
        if user_tags:
            personalized_info += f"ç”¨æˆ·çš„æ ‡ç­¾åŒ…æ‹¬ï¼š{', '.join(user_tags)}ã€‚\n"
    else:
        personalized_info = ""
    
    return personalized_info


def _build_system_prompt(now_dt: datetime, silence_str: str, emotion: Any, 
                         user_display_name: str, rel: Any, vision_desc: str, 
                         summary: str, is_group: bool, personalized_info: str):
    """
    æ„å»ºç³»ç»Ÿæç¤º
    """
    return SOCIAL_VOLITION_PROMPT.format(
        current_time=now_dt.strftime("%H:%M"),
        time_period=_get_time_period(now_dt),
        silence_duration=silence_str,
        mood=emotion.primary_emotion,
        stamina=emotion.stamina,
        user_name=user_display_name,
        intimacy=rel.intimacy,
        familiarity=rel.familiarity,
        trust=rel.trust,
        interest_match=rel.interest_match,
        relation_tags=", ".join(rel.tags) if rel.tags else "æ— ",
        relation_notes=rel.notes or "æ— ",
        vision_desc=vision_desc,
        conversation_summary=summary[-400:] if summary else "æ— ",
        chat_type="ç¾¤èŠ" if is_group else "ç§èŠ",
        personalized_info=personalized_info
    )


async def _build_input_messages(prompt: str, visual_type: str, image_data: str, 
                              history_str: str, is_group: bool, silence_seconds: float, 
                              silence_str: str, user_display_name: str, rel: Any, ts: str):
    """
    æ„å»ºè¾“å…¥æ¶ˆæ¯
    """
    input_msgs = [SystemMessage(content=prompt)]

    # åœºæ™¯ A: æœ‰æ„ä¹‰çš„å›¾ç‰‡ (Photo) -> å‘é€ Base64 ç»™ LLM
    if visual_type == "photo" and image_data:
        logger.info(f"[{ts}] ğŸ” [Proactive] Injecting IMAGE payload for analysis.")
        input_msgs.append(HumanMessage(content=[
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}},
            {"type": "text",
             "text": f"ç”¨æˆ·åˆšå‘äº†è¿™å¼ å›¾ã€‚å†å²ä¿¡æ¯æ˜¯: '{history_str}'ã€‚è¯·åˆ¤æ–­å›¾ç‰‡å†…å®¹æ˜¯å¦é‡è¦ï¼Ÿæˆ‘è¯¥å¦‚ä½•è¯„è®ºï¼Ÿ"}
        ]))

    # åœºæ™¯ B: è¡¨æƒ…åŒ… (Sticker) -> æ‹¦æˆª Base64ï¼Œä»…å‘é€æ–‡æœ¬æç¤º
    elif visual_type == "sticker":
        logger.info(f"[{ts}] ğŸ­ [Proactive] Handling STICKER (Skipping visual payload).")
        # å‘Šè¯‰ LLM è¿™æ˜¯ä¸ªè¡¨æƒ…åŒ…ï¼Œä¸éœ€è¦æ·±åº¦åˆ†æï¼Œåªéœ€è¦ç¤¾äº¤å›åº”
        sticker_prompt = (
            f"[ç³»ç»Ÿé€šçŸ¥] ç”¨æˆ·å‘é€äº†ä¸€ä¸ªè¡¨æƒ…åŒ… (Sticker)ã€‚\n"
            f"å†å²ä¿¡æ¯æ˜¯: '{history_str}'ã€‚\n"
            f"æ— éœ€åˆ†æå›¾ç‰‡å†…å®¹ï¼ˆæœªä¸Šä¼ ï¼‰ã€‚è¯·æ ¹æ®å½“å‰äº²å¯†åº¦ ({rel.intimacy}) å†³å®šæ˜¯å›å¤ä¸€ä¸ªè¡¨æƒ…ã€ç®€çŸ­åæ§½è¿˜æ˜¯ä¿æŒæ²‰é»˜ã€‚"
        )
        input_msgs.append(HumanMessage(content=sticker_prompt))

    # åœºæ™¯ C: æ— å›¾ (çº¯æ–‡æœ¬æˆ–é™é»˜)
    else:
        # æ ¹æ®ç¾¤èŠ/ç§èŠåœºæ™¯è®¾ç½®ä¸åŒçš„å›å¤ç­–ç•¥
        user_input_prompt = _build_text_prompt(
            is_group, silence_seconds, silence_str, history_str, user_display_name, rel
        )
        input_msgs.append(HumanMessage(content=user_input_prompt))
    
    return input_msgs


def _build_text_prompt(is_group: bool, silence_seconds: float, 
                       silence_str: str, history_str: str, user_display_name: str, rel: Any):
    """
    æ„å»ºæ–‡æœ¬æç¤º
    """
    if is_group:
        # ç¾¤èŠåœºæ™¯ï¼šæ›´åŠ è°¨æ…ï¼Œé¿å…æ‰“æ‰°ï¼Œä¸»è¦é’ˆå¯¹æœ€è¿‘è¯é¢˜è¿›è¡Œè‡ªç„¶å»¶ä¼¸
        if silence_seconds < 300:
            # C1: ç¾¤èŠåˆšåˆšæœ‰æ´»åŠ¨ -> å¯ä»¥å¯¹æœ€è¿‘çš„è¯é¢˜è¿›è¡Œè¡¥å……æˆ–è¯„è®ºï¼Œä½†ä¸è¦è¿‡äºé¢‘ç¹
            return (
                f"This is a group chat environment. "
                f"History Conversation Context: '{history_str}'. "
                f"Should I naturally join the conversation with a relevant comment or observation? "
                f"Keep it brief, friendly, and avoid dominating the discussion."
            )
        else:
            # C2: ç¾¤èŠæ²‰é»˜è¾ƒä¹… -> å¯ä»¥å‘èµ·è½»æ¾è¯é¢˜ï¼Œä½†ä¸è¦æ˜¾å¾—çªå…€
            return (
                f"This is a group chat environment that's been quiet for {silence_str}. "
                f"History Conversation Context was: '{history_str}'. "
                f"Would a light, friendly comment or question be appropriate to re-engage the group? "
                f"Avoid being pushy or too personal."
            )
    else:
        # ç§èŠåœºæ™¯ï¼šæ›´åŠ äº²å¯†å’Œä¸ªæ€§åŒ–
        if silence_seconds < 120:
            # C1: åˆšåˆšèŠè¿‡å¤© -> åˆ¤æ–­æ˜¯å¦è¿½è¯„/æ¥è¯ï¼Œå¢åŠ äº²å¯†æ„Ÿ
            return (
                f"This is a private chat with {user_display_name}. "
                f"History Conversation Context: '{history_str}'. "
                f"Based on our intimacy ({rel.intimacy}) and the text content, "
                f"should I voluntarily add a warm comment, follow up, or ask a personal question?"
            )
        else:
            # C2: æ²‰é»˜å¾ˆä¹… -> ç ´å†°ï¼Œæ›´åŠ ä¸ªæ€§åŒ–
            return (
                f"This is a private chat with {user_display_name} that's been quiet for {silence_str}. "
                f"Our relationship intimacy is {rel.intimacy}. "
                f"History Conversation Context was: '{history_str}'. "
                f"Should I initiate a new conversation with a warm, personal message? "
                f"Consider our relationship, shared topics, and the time of day."
            )


async def _process_llm_response(input_msgs: List[Any], ts: str):
    """
    è°ƒç”¨LLMå¹¶å¤„ç†å“åº”
    """
    try:
        response = await cached_llm_invoke(llm, input_msgs, temperature=llm.temperature)
        content = response.content.strip()

        # JSON æ¸…æ´—
        if "```json" in content:
            content = content.replace("```json", "").replace("```", "")

        # æœ‰äº›æ¨¡å‹å¯èƒ½ä¼šè¾“å‡º Markdown æ ¼å¼ï¼Œå†åŠ ä¸€å±‚æ¸…æ´—
        content = content.strip('`')

        try:
            decision = json.loads(content)
        except:
            logger.warning(f"[{ts}] âš ï¸ [Proactive] JSON Parse fail: {content[:30]}...")
            # è¿™é‡Œçš„ fallback ç­–ç•¥å¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼Œé»˜è®¤ä¿æŒæ²‰é»˜æ¯”è¾ƒå®‰å…¨
            return None

        intent = decision.get("intent", "silent")
        reply_content = decision.get("content", "")
        reason = decision.get("reason", "")

        logger.info(f"[{ts}] ğŸ¤– [Proactive Decision] {intent.upper()} | Reason: {reason}")
        return intent, reply_content, reason
        
    except Exception as e:
        logger.error(f"[{ts}] âŒ [Proactive Error] {e}")
        return None


def _filter_group_reply(reply_content: str, ts: str):
    """
    è¿‡æ»¤ç¾¤èŠä¸­çš„ä¸åˆé€‚å›å¤
    """
    # æ£€æŸ¥ç¾¤èŠå›å¤æ˜¯å¦åˆé€‚
    lower_content = reply_content.lower()
    # é¿å…åœ¨ç¾¤èŠä¸­è¯¢é—®è¿‡äºç§äººçš„é—®é¢˜
    private_questions = ["ä½ æœ€è¿‘æ€ä¹ˆæ ·", "ä½ åœ¨å¹²ä»€ä¹ˆ", "ä½ çš„éšç§", "ä½ å®¶é‡Œ", "ä½ çš„æ„Ÿæƒ…", "ä½ å·¥èµ„", "ä½ å¹´é¾„", "ä½ å¯¹è±¡"]
    if any(q in lower_content for q in private_questions):
        logger.warning(f"[{ts}] âš ï¸ [Proactive] Filtered private content in group chat: {reply_content[:30]}...")
        return False
    # é¿å…åœ¨ç¾¤èŠä¸­ä½¿ç”¨è¿‡äºäº²å¯†çš„ç§°å‘¼
    intimate_terms = ["äº²çˆ±çš„", "å®è´", "è€å…¬", "è€å©†", "å“¥å“¥", "å§å§", "å¼Ÿå¼Ÿ", "å¦¹å¦¹"]
    if any(term in reply_content for term in intimate_terms):
        logger.warning(f"[{ts}] âš ï¸ [Proactive] Filtered intimate term in group chat: {reply_content[:30]}...")
        return False
    # ç¾¤èŠå›å¤ä¿æŒç®€æ´
    if len(reply_content) > 100:
        reply_content = reply_content[:100] + "..."
    
    return True


def _personalize_reply_content(reply_content: str, is_group: bool, 
                               rel: Any, user_hobbies: List[str]):
    """
    ä¸ªæ€§åŒ–å›å¤å†…å®¹
    """
    import random
    
    # ä¸ºä¸åŒåœºæ™¯æ·»åŠ è¯­æ°”è¯æˆ–è¡¨æƒ…ï¼Œå¢åŠ è‡ªç„¶æ„Ÿ
    if is_group:
        # ç¾¤èŠåœºæ™¯ï¼šä½è°ƒã€å¹³æ·¡çš„è¯­æ°”ï¼Œé¿å…å¤ªçªå…€
        # ä½¿ç”¨ç¬¦åˆAliceäº‘æ·¡é£è½»æ€§æ ¼çš„å¼€å¤´å’Œç»“å°¾
        group_intros = ["", "å¯¹äº†ï¼Œ", "è¯è¯´ï¼Œ", "çªç„¶æƒ³åˆ°ï¼Œ", "å…¶å®å§ï¼Œ", "æˆ‘è§‰å¾—", "è¯è¯´å›æ¥ï¼Œ", "ä¹‹å‰çœ‹åˆ°", "å“ï¼Œ", "é‚£ä¸ª", "åˆšæ‰", "çªç„¶å‘ç°", "å“¦å¯¹äº†ï¼Œ", "è¯è¯´é‚£ä¸ª", "çªç„¶æƒ³é—®"]
        group_endings = ["", "~", "", "", "ğŸ¤”", "", "", "~", "~"]
        
        # éšæœºé€‰æ‹©å¼€å¤´å’Œç»“å°¾
        intro = random.choice(group_intros)
        ending = random.choice(group_endings)
        
        # ç¾¤èŠåœºæ™¯ï¼šå¯ä»¥åœ¨å†…å®¹ä¸­é—´æ·»åŠ ä¸€äº›åœé¡¿æˆ–è¯­æ°”è¯ï¼Œå¢åŠ è‡ªç„¶æ„Ÿ
        if len(reply_content) > 20:
            # åœ¨ä¸­é—´ä½ç½®æ’å…¥ä¸€ä¸ªè‡ªç„¶çš„åœé¡¿æˆ–è¯­æ°”è¯
            middle_pos = len(reply_content) // 2
            natural_pauses = ["", "ï¼Œ", "ï¼Œå…¶å®", "ï¼Œè¯è¯´", "ï¼Œå¯¹å§", "ï¼Œæˆ‘è§‰å¾—", "ï¼Œä½ ä»¬çœ‹"]
            pause = random.choice(natural_pauses)
            if pause:  # é¿å…ç©ºå­—ç¬¦ä¸²
                reply_content = reply_content[:middle_pos] + pause + reply_content[middle_pos:]
        
        return f"{intro}{reply_content}{ending}"
    else:
        # ç§èŠåœºæ™¯ï¼šæ¸©å’Œã€ç¤¼è²Œçš„è¯­æ°”ï¼Œç¬¦åˆAliceäº‘æ·¡é£è½»çš„æ€§æ ¼
        # æ ¹æ®äº²å¯†åº¦è°ƒæ•´è¯­æ°”ï¼Œä½†ä¿æŒè‡ªç„¶ä¸å¤¸å¼ 
        private_intros = ["", "å“ï¼Œ", "å¯¹äº†ï¼Œ", "ä½ çŸ¥é“å—ï¼Ÿ", "çªç„¶æƒ³é—®ä½ ï¼Œ", "è¯è¯´å›æ¥ï¼Œ", "å…¶å®å§ï¼Œ", "æˆ‘è§‰å¾—", "åˆšæ‰æƒ³åˆ°", "æœ€è¿‘", "ä¹‹å‰", "é‚£ä¸ª", "å“å¯¹äº†ï¼Œ", "çªç„¶å‘ç°", "è¯è¯´", "å…¶å®"]
        
        # æ ¹æ®äº²å¯†åº¦é€‰æ‹©ä¸åŒçš„è¯­æ°”ï¼Œé¿å…è¿‡äºäº²å¯†æˆ–å¤¸å¼ 
        if rel.intimacy > 85:
            # è¶…é«˜äº²å¯†åº¦ï¼šç¨å¾®äº²å¯†ä½†ä¸å¤¸å¼ çš„è¡¨è¾¾
            intimate_intros = ["é‚£ä¸ªï¼Œ", "å“ï¼Œ", "å¯¹äº†ï¼Œ", "è¯è¯´ï¼Œ"]
            private_endings = ["", "~", "", "~", "~"]
        elif rel.intimacy > 70:
            # é«˜äº²å¯†åº¦ï¼šå‹å¥½ä½†ä¿æŒè·ç¦»çš„è¡¨è¾¾
            intimate_intros = ["é‚£ä¸ªï¼Œ", "å“ï¼Œ", "å¯¹äº†ï¼Œ"]
            private_endings = ["", "~", "", "~"]
        elif rel.intimacy > 50:
            # ä¸­ç­‰äº²å¯†åº¦ï¼šæ™®é€šå‹å¥½çš„è¡¨è¾¾
            intimate_intros = ["é‚£ä¸ªï¼Œ", "å“ï¼Œ"]
            private_endings = ["", "~", "", ""]
        else:
            # ä½äº²å¯†åº¦ï¼šç¤¼è²Œã€å¹³æ·¡çš„è¡¨è¾¾
            intimate_intros = []
            private_endings = ["", "~", "", ""]
        
        # éšæœºé€‰æ‹©å¼€å¤´å’Œç»“å°¾
        available_intros = private_intros + intimate_intros
        intro = random.choice(available_intros)
        ending = random.choice(private_endings)
        
        # ç§èŠåœºæ™¯ï¼šå¯ä»¥æ·»åŠ æ›´å¤šä¸ªæ€§åŒ–çš„å…ƒç´ ï¼Œæ¯”å¦‚ç”¨æˆ·çš„å…´è¶£çˆ±å¥½
        # å¦‚æœç”¨æˆ·æœ‰æ˜ç¡®çš„å…´è¶£çˆ±å¥½ï¼Œå¯ä»¥åœ¨å›å¤ä¸­è‡ªç„¶æåŠ
        if user_hobbies and len(user_hobbies) > 0 and random.random() > 0.5:
            # éšæœºé€‰æ‹©ä¸€ä¸ªç”¨æˆ·çš„å…´è¶£çˆ±å¥½
            hobby = random.choice(user_hobbies)
            # æ·»åŠ ä¸€äº›ä¸å…´è¶£ç›¸å…³çš„äº’åŠ¨å†…å®¹ï¼Œä¿æŒå¹³æ·¡è‡ªç„¶çš„è¯­æ°”
            hobby_related_phrases = [
                f"å¯¹äº†ï¼Œä½ ä¹‹å‰è¯´å–œæ¬¢{hobby}...",
                f"è¯´åˆ°è¿™ä¸ªï¼Œçªç„¶æƒ³åˆ°ä½ å–œæ¬¢{hobby}...",
                f"å“ï¼Œæˆ‘è®°å¾—ä½ å–œæ¬¢{hobby}...",
                f"å¯¹äº†ï¼Œå…³äº{hobby}...",
                f"çªç„¶æƒ³èµ·ä½ å–œæ¬¢{hobby}...",
            ]
            # éšæœºé€‰æ‹©ä¸€ä¸ªç›¸å…³çŸ­è¯­ï¼Œæ·»åŠ åˆ°å›å¤å†…å®¹ä¸­
            if random.random() > 0.3:
                # åœ¨å¼€å¤´æ·»åŠ 
                intro += random.choice(hobby_related_phrases)
            else:
                # åœ¨å†…å®¹ä¸­é—´æ·»åŠ 
                if len(reply_content) > 20:
                    middle_pos = len(reply_content) // 2
                    reply_content = reply_content[:middle_pos] + f" {random.choice(hobby_related_phrases)} " + reply_content[middle_pos:]
        
        return f"{intro}{reply_content}{ending}"


def _finalize_response(intent: str, reply_content: str, reason: str, 
                       is_group: bool, msgs: List[Any], rel: Any, ts: str):
    """
    æœ€ç»ˆå¤„ç†å¹¶è¿”å›å“åº”
    """
    # å†³å®šè¯´è¯ -> æ¶ˆè€—ä½“åŠ›
    # ç¾¤èŠå’Œç§èŠæ¶ˆè€—ä¸åŒçš„ä½“åŠ›å€¼
    stamina_cost = -2.0 if is_group else -3.0  # ç¾¤èŠæ¶ˆè€—è¾ƒå°‘ä½“åŠ›
    global_store.update_emotion(0, 0, stamina_delta=stamina_cost)

    ai_msg = AIMessage(content=reply_content)

    # è¿”å›æ›´æ–°åçš„çŠ¶æ€
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ msgs (ä» state è·å–çš„åˆ—è¡¨) + æ–°æ¶ˆæ¯
    # å…·ä½“æ˜¯è¿”å›å®Œæ•´åˆ—è¡¨è¿˜æ˜¯å¢é‡åˆ—è¡¨å–å†³äºä½ çš„ Graph Reducer å®šä¹‰ï¼Œè¿™é‡Œä¿æŒåŸé€»è¾‘é£æ ¼è¿”å›å®Œæ•´åˆ—è¡¨
    return {
        "messages": msgs + [ai_msg],
        "next_step": "speak",
        "internal_monologue": f"[Social Volition] Intent: {intent}, Reason: {reason}, ChatType: {'Group' if is_group else 'Private'}"
    }
