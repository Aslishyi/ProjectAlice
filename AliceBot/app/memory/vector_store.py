import chromadb
import hashlib
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import math
import threading
from openai import OpenAI
from langchain_core.vectorstores import VectorStore
from langchain_core.documents import Document
from app.core.config import config
from app.utils.cache import cached_embedding_get, cached_embedding_set

# é…ç½®æ—¥å¿—
logger = logging.getLogger("VectorStore")


class VectorMemory(VectorStore):
    """
    å‘é‡å­˜å‚¨å™¨ï¼Œå®ç°äº†LangChainçš„VectorStoreæ¥å£
    """
    
    def __init__(self):
        # 1. æ£€æŸ¥å¹¶åˆ›å»ºå‘é‡æ•°æ®åº“ç›®å½•
        import os
        import tempfile
        db_path = config.VECTOR_DB_PATH
        
        # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not os.path.exists(db_path):
            try:
                os.makedirs(db_path, exist_ok=True)
                logger.info(f"[VectorStore] Created vector database directory: {db_path}")
            except OSError as e:
                logger.error(f"[VectorStore] Failed to create vector database directory: {e}")
                raise
        
        # æ£€æŸ¥ç›®å½•æ˜¯å¦å¯å†™
        try:
            with tempfile.NamedTemporaryFile(dir=db_path, delete=True):
                pass
            logger.info(f"[VectorStore] Vector database directory is writable: {db_path}")
        except OSError as e:
            logger.error(f"[VectorStore] Vector database directory is not writable: {e}")
            logger.error(f"[VectorStore] Please check directory permissions and ensure no other process is locking the database")
            raise
        
        # 2. åˆå§‹åŒ– ChromaDBï¼Œç¦ç”¨é¥æµ‹å¹¶ç¡®ä¿å¯å†™
        self.client = chromadb.PersistentClient(
            path=db_path,
            settings=chromadb.config.Settings(
                anonymized_telemetry=False
            )
        )
        self._lock = threading.Lock()  # åˆå§‹åŒ–äº’æ–¥é”

        # 2. åˆå§‹åŒ–å¼‚æ­¥ OpenAI å®¢æˆ·ç«¯
        from openai import AsyncOpenAI
        self.openai_client = AsyncOpenAI(
            api_key=config.SILICONFLOW_API_KEY,
            base_url=config.SILICONFLOW_BASE_URL
        )

        # 3. åˆå§‹åŒ–åµŒå…¥å‡½æ•°
        self.embedding_model = config.EMBEDDING_MODEL_NAME

        self.collection = self.client.get_or_create_collection(
            name=config.COLLECTION_NAME
        )
        
        # 4. æ ‡è®°æ¸…ç†ä»»åŠ¡æœªå¯åŠ¨
        self._cleanup_task_started = False
    
    def start_cleanup_task(self):
        """æ‰‹åŠ¨å¯åŠ¨å®šæ—¶æ¸…ç†ä»»åŠ¡
        
        åªæœ‰å½“æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯æ—¶æ‰èƒ½è°ƒç”¨æ­¤æ–¹æ³•
        """
        if not self._cleanup_task_started:
            import asyncio
            asyncio.create_task(self._start_cleanup_task())
            self._cleanup_task_started = True
            logger.info("[VectorStore] å®šæ—¶æ¸…ç†ä»»åŠ¡å·²å¯åŠ¨")

    async def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """æ‰‹åŠ¨ç”ŸæˆåµŒå…¥å‘é‡ï¼Œå¸¦ç¼“å­˜"""
        import asyncio
        from app.utils.cache import cached_embedding_get, cached_embedding_set
        
        texts = [t.replace("\n", " ") for t in texts]
        embeddings = []
        uncached_texts = []
        uncached_indices = []
        
        # å…ˆæ£€æŸ¥ç¼“å­˜
        for i, text in enumerate(texts):
            cached_emb = await cached_embedding_get(text, self.embedding_model)
            if cached_emb:
                embeddings.append(cached_emb)
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
        
        # å¦‚æœæœ‰æœªç¼“å­˜çš„æ–‡æœ¬ï¼Œè°ƒç”¨APIè·å–åµŒå…¥å‘é‡
        if uncached_texts:
            response = await self.openai_client.embeddings.create(
                input=uncached_texts,
                model=self.embedding_model
            )
            uncached_embeddings = [data.embedding for data in response.data]
            
            # å°†æ–°è·å–çš„åµŒå…¥å‘é‡åŠ å…¥ç»“æœå¹¶ç¼“å­˜
            for idx, text, emb in zip(uncached_indices, uncached_texts, uncached_embeddings):
                embeddings.insert(idx, emb)
                await cached_embedding_set(text, self.embedding_model, emb)
        
        return embeddings

    async def add_texts(self, texts: List[str], metadatas: Optional[List[dict]] = None,
                  ids: Optional[List[str]] = None, **kwargs) -> List[str]:
        """æ·»åŠ æ–‡æœ¬åˆ°å‘é‡å­˜å‚¨"""
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        if not texts: return []

        # å¦‚æœæ²¡æœ‰æä¾›IDï¼Œç”Ÿæˆå”¯ä¸€ID
        if not ids:
            ids = [f"mem_{hash(t)}" for t in texts]

        final_metadatas = []
        if metadatas:
            for m in metadatas:
                if "importance" not in m: m["importance"] = 1
                if "created_at" not in m: m["created_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                final_metadatas.append(m)
        else:
            now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            final_metadatas = [{"source": "interaction", "importance": 1, "created_at": now_str}] * len(texts)

        # æ‰‹åŠ¨ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆå¼‚æ­¥ï¼‰
        embeddings = await self._generate_embeddings(texts)

        # åŠ é”å†™å…¥
        with self._lock:
            try:
                self.collection.upsert(
                    documents=texts,
                    embeddings=embeddings,
                    metadatas=final_metadatas,
                    ids=ids
                )
                return ids
            except Exception as e:
                logger.error(f"[{ts}] âŒ [VectorStore Write Error] {e}")
                return []

    async def similarity_search(self, query: str, k: int = 4, **kwargs) -> List[Document]:
        """ç›¸ä¼¼æ€§æœç´¢"""
        with self._lock:
            try:
                # æ‰‹åŠ¨ç”ŸæˆæŸ¥è¯¢åµŒå…¥å‘é‡ï¼ˆå¼‚æ­¥ï¼‰
                query_embedding = await self._generate_embeddings([query])[0]
                
                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=k * 3,
                    include=["documents", "metadatas", "distances"]
                )
            except Exception as e:
                logger.error(f"[VectorStore Error] Search failed: {e}")
                return []

        if not results["documents"]:
            return []

        docs = results["documents"][0]
        metas = results["metadatas"][0]
        dists = results["distances"][0]

        scored_candidates = []

        for doc, meta, dist in zip(docs, metas, dists):
            # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆè·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼‰
            semantic_score = 1.0 / (1.0 + dist)
            document = Document(page_content=doc, metadata=meta)
            scored_candidates.append((document, semantic_score))

        # æŒ‰å¾—åˆ†æ’åºï¼Œå–å‰kä¸ª
        scored_candidates.sort(key=lambda x: x[1], reverse=True)
        return [doc_score[0] for doc_score in scored_candidates[:k]]

    async def similarity_search_with_score(self, query: str, k: int = 4, **kwargs) -> List[tuple[Document, float]]:
        """å¸¦åˆ†æ•°çš„ç›¸ä¼¼æ€§æœç´¢"""
        with self._lock:
            try:
                # æ‰‹åŠ¨ç”ŸæˆæŸ¥è¯¢åµŒå…¥å‘é‡ï¼ˆå¼‚æ­¥ï¼‰
                query_embedding = await self._generate_embeddings([query])[0]
                
                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=k * 3,
                    include=["documents", "metadatas", "distances"]
                )
            except Exception as e:
                logger.error(f"[VectorStore Error] Search failed: {e}")
                return []

        if not results["documents"]:
            return []

        docs = results["documents"][0]
        metas = results["metadatas"][0]
        dists = results["distances"][0]

        scored_candidates = []

        for doc, meta, dist in zip(docs, metas, dists):
            # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆè·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼‰
            semantic_score = 1.0 / (1.0 + dist)
            document = Document(page_content=doc, metadata=meta)
            scored_candidates.append((document, semantic_score))

        # æŒ‰å¾—åˆ†æ’åºï¼Œå–å‰kä¸ª
        scored_candidates.sort(key=lambda x: x[1], reverse=True)
        return scored_candidates[:k]

    def _calculate_time_decay(self, created_at_str: str, half_life_hours: float = 48.0) -> float:
        """
        è®¡ç®—æ—¶é—´è¡°å‡å› å­ï¼Œä¼˜åŒ–çš„è¡°å‡ç®—æ³•
        """
        try:
            mem_time = datetime.strptime(created_at_str, "%Y-%m-%d %H:%M:%S")
            delta_hours = (datetime.now() - mem_time).total_seconds() / 3600.0
            
            # ä¼˜åŒ–çš„è¡°å‡ç®—æ³•ï¼šå‰24å°æ—¶è¡°å‡è¾ƒæ…¢ï¼Œä¹‹ååŠ é€Ÿè¡°å‡
            if delta_hours < 24:
                # å‰24å°æ—¶è¡°å‡è¾ƒæ…¢ï¼Œhalf_lifeä¸º96å°æ—¶
                decay = max(0.2, math.pow(0.5, delta_hours / 96.0))
            else:
                # 24å°æ—¶ååŠ é€Ÿè¡°å‡ï¼Œä½¿ç”¨æŒ‡å®šçš„half_life
                decay = max(0.2, math.pow(0.5, delta_hours / half_life_hours))
            
            return decay
        except:
            return 1.0

    async def search(self, query: str, k: int = 3, categories: List[str] = None, source_boosts: Dict[str, float] = None, importance_threshold: float = 0.5) -> List[str]:
        """
        è‡ªå®šä¹‰æœç´¢ï¼Œè€ƒè™‘æ—¶é—´è¡°å‡å’Œé‡è¦æ€§ï¼Œå¸¦ç¼“å­˜ï¼Œæ”¯æŒåˆ†ç±»ç­›é€‰å’Œè‡ªå®šä¹‰æ¥æºæƒé‡
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            k: è¿”å›ç»“æœæ•°é‡
            categories: å¯é€‰ï¼ŒæŒ‡å®šè¦æœç´¢çš„åˆ†ç±»
            source_boosts: å¯é€‰ï¼Œè‡ªå®šä¹‰æ¥æºæƒé‡
            importance_threshold: å¯é€‰ï¼Œé‡è¦æ€§é˜ˆå€¼ï¼Œè¿‡æ»¤ä½äºæ­¤é˜ˆå€¼çš„è®°å¿†
            
        Returns:
            List[str]: æœç´¢ç»“æœåˆ—è¡¨
        """
        import asyncio
        from app.utils.cache import cached_context_get, cached_context_set
        
        # æ„å»ºç¼“å­˜é”®ï¼Œè€ƒè™‘æ‰€æœ‰å‚æ•°
        cache_params = [f"{k}"]
        if categories:
            cache_params.extend(sorted(categories))
        if source_boosts:
            cache_params.extend([f"{k}:{v}" for k, v in sorted(source_boosts.items())])
        cache_params.append(f"{importance_threshold}")
        cache_key = f"vector_search:{hash(query)}:{':'.join(map(str, cache_params))}"
        
        # å…ˆæ£€æŸ¥ä¸Šä¸‹æ–‡ç¼“å­˜
        cached_results = await cached_context_get(cache_key)
        if cached_results:
            return cached_results
        
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥å‘é‡ï¼ˆåœ¨é”å¤–è¿›è¡Œï¼Œæé«˜å¹¶å‘æ€§èƒ½ï¼‰
        query_embedding = await self._generate_embeddings([query])
        query_embedding = query_embedding[0] if query_embedding else []
        
        with self._lock:
            try:
                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=k * 5,  # å¢åŠ å€™é€‰æ•°é‡ï¼Œæé«˜é€‰æ‹©è´¨é‡
                    include=["documents", "metadatas", "distances"]
                )
            except Exception as e:
                logger.error(f"[VectorStore Error] Search failed: {e}")
                return []

        if not results["documents"]:
            return []

        docs = results["documents"][0]
        metas = results["metadatas"][0]
        dists = results["distances"][0]

        scored_candidates = []
        seen_docs = set()  # ç”¨äºå»é‡
        
        # é»˜è®¤æ¥æºæƒé‡
        default_source_boosts = {
            "user_profile": 1.8,  # æé«˜ç”¨æˆ·èµ„æ–™çš„æƒé‡
            "chat_history": 1.3,  # æé«˜èŠå¤©å†å²çš„æƒé‡
            "interaction": 1.0,
            "system": 0.9
        }
        
        # åˆå¹¶è‡ªå®šä¹‰æ¥æºæƒé‡
        final_source_boosts = default_source_boosts.copy()
        if source_boosts:
            final_source_boosts.update(source_boosts)

        for doc, meta, dist in zip(docs, metas, dists):
            # å»é‡
            if doc in seen_docs:
                continue
            seen_docs.add(doc)
            
            # åˆ†ç±»ç­›é€‰
            if categories:
                doc_category = meta.get("category", "")
                if doc_category not in categories:
                    continue
            
            # é‡è¦æ€§é˜ˆå€¼è¿‡æ»¤
            importance = float(meta.get("importance", 1))
            if importance < importance_threshold:
                continue
            
            # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆè·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼‰
            semantic_score = 1.0 / (1.0 + dist)
            
            # è®¡ç®—æ—¶é—´è¡°å‡å› å­
            created_at = meta.get("created_at", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
            time_score = self._calculate_time_decay(created_at)
            
            # è®¡ç®—é‡è¦æ€§æƒé‡
            imp_boost = 1.0 + (importance * 0.3)  # å¢åŠ é‡è¦æ€§çš„å½±å“
            
            # è®¡ç®—æ¥æºæƒé‡
            source = meta.get("source", "interaction")
            source_boost = final_source_boosts.get(source, 1.0)
            
            # è®¡ç®—æœ€ç»ˆå¾—åˆ†
            final_score = semantic_score * time_score * imp_boost * source_boost
            
            # å¦‚æœæ–‡æ¡£åŒ…å«å…³é”®è¯ï¼Œç»™äºˆé¢å¤–å¥–åŠ±
            if query.lower() in doc.lower():
                final_score *= 1.1  # å…³é”®è¯åŒ¹é…å¥–åŠ±
            
            scored_candidates.append((final_score, doc))

        # æŒ‰å¾—åˆ†æ’åºï¼Œå–å‰kä¸ª
        scored_candidates.sort(key=lambda x: x[0], reverse=True)
        top_docs = [item[1] for item in scored_candidates[:k]]
        
        # å°†ç»“æœå­˜å…¥ä¸Šä¸‹æ–‡ç¼“å­˜
        await cached_context_set(cache_key, top_docs, ttl=3600)  # å¢åŠ ç¼“å­˜æ—¶é—´åˆ°1å°æ—¶

        return top_docs
        
    async def search_by_category(self, category: str, query: str = None, k: int = 5) -> List[str]:
        """
        æŒ‰åˆ†ç±»æœç´¢è®°å¿†
        
        Args:
            category: è¦æœç´¢çš„åˆ†ç±»
            query: å¯é€‰ï¼Œæœç´¢æŸ¥è¯¢
            k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[str]: æœç´¢ç»“æœåˆ—è¡¨
        """
        if not query:
            # å¦‚æœæ²¡æœ‰æŸ¥è¯¢è¯ï¼Œç›´æ¥ä½¿ç”¨åˆ†ç±»åä½œä¸ºæŸ¥è¯¢
            query = category
        
        return await self.search(query, k=k, categories=[category])

    async def delete_by_semantic(self, query: str, threshold: float = 0.3):
        """é€šè¿‡è¯­ä¹‰åˆ é™¤ç›¸ä¼¼é¡¹"""
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with self._lock:
            try:
                # æ‰‹åŠ¨ç”ŸæˆæŸ¥è¯¢åµŒå…¥å‘é‡ï¼ˆå¼‚æ­¥ï¼‰
                query_embedding = await self._generate_embeddings([query])[0]
                
                # ä½¿ç”¨æŸ¥è¯¢è·å–ç›¸ä¼¼çš„æ–‡æ¡£
                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=10,
                    include=["documents", "metadatas", "distances", "embeddings"]
                )
                
                if not results["documents"] or not results["documents"][0]:
                    return 0
                
                # è®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦
                import numpy as np
                query_emb = np.array(query_embedding)
                ids_to_delete = []
                
                # å…ˆè·å–æ‰€æœ‰æ–‡æ¡£IDå’Œå†…å®¹çš„æ˜ å°„ï¼Œä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
                all_docs = self.collection.get()
                doc_id_map = {}
                for doc_id, doc_content in zip(all_docs["ids"], all_docs["documents"]):
                    doc_id_map[doc_content] = doc_id
                
                for i in range(len(results["documents"][0])):
                    doc = results["documents"][0][i]
                    embedding = results["embeddings"][0][i]
                    distance = results["distances"][0][i]
                    
                    doc_emb = np.array(embedding)
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = np.dot(query_emb, doc_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(doc_emb))
                    
                    logger.debug(f"Doc {i}: {doc[:50]}..., Similarity: {similarity:.4f}, Distance: {distance:.4f}")
                    
                    # ä½™å¼¦ç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼æ—¶åˆ é™¤
                    if similarity > threshold:
                        # ä½¿ç”¨æ–‡æ¡£å†…å®¹æŸ¥æ‰¾å¯¹åº”çš„ID
                        if doc in doc_id_map:
                            ids_to_delete.append(doc_id_map[doc])
                            logger.debug(f"Found ID {doc_id_map[doc]} for document")
                
                logger.debug(f"Total ids to delete: {len(ids_to_delete)}")
                logger.debug(f"Ids to delete: {ids_to_delete}")
                
                if ids_to_delete:
                    self.collection.delete(ids=ids_to_delete)
                    logger.info(f"[{ts}] ğŸ§¹ [Memory] Deleted {len(ids_to_delete)} items.")
                    return len(ids_to_delete)
                return 0
            except Exception as e:
                logger.error(f"[{ts}] [VectorStore] Delete error: {e}")
                return 0

    # å®ç°LangChain VectorStoreæ¥å£çš„å…¶ä»–å¿…è¦æ–¹æ³•
    def as_retriever(self, **kwargs):
        """è½¬æ¢ä¸ºæ£€ç´¢å™¨"""
        from langchain_classic.memory.vectorstore import VectorStoreRetriever
        return VectorStoreRetriever(vectorstore=self, **kwargs)

    @classmethod
    async def from_documents(cls, documents: List[Document], embedding, **kwargs):
        """ä»æ–‡æ¡£åˆ›å»º"""
        instance = cls()
        texts = [doc.page_content for doc in documents]
        metas = [doc.metadata for doc in documents]
        await instance.add_texts(texts, metas)
        return instance

    async def delete(self, ids: List[str], **kwargs):
        """åˆ é™¤æŒ‡å®šIDçš„æ–‡æ¡£"""
        with self._lock:
            try:
                self.collection.delete(ids=ids)
                return True
            except Exception as e:
                logger.error(f"âŒ [VectorStore] Delete error: {e}")
                return False
                
    async def search_by_keyword(self, keyword: str, k: int = 10) -> List[Dict[str, Any]]:
        """
        é€šè¿‡å…³é”®è¯æœç´¢è®°å¿†
        
        Args:
            keyword: è¦æœç´¢çš„å…³é”®è¯
            k: è¿”å›ç»“æœçš„æ•°é‡
            
        Returns:
            åŒ…å«documentã€metadataå’Œdistanceçš„å­—å…¸åˆ—è¡¨
        """
        with self._lock:
            try:
                # è·å–æ‰€æœ‰æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®
                all_docs = self.collection.get()
                
                if not all_docs["documents"]:
                    return []
                
                # æ‰‹åŠ¨ç”Ÿæˆå…³é”®è¯åµŒå…¥å‘é‡ï¼ˆç”¨äºè®¡ç®—ç›¸ä¼¼åº¦ï¼‰
                keyword_embedding = await self._generate_embeddings([keyword])[0]
                
                # è¿‡æ»¤åŒ…å«å…³é”®è¯çš„æ–‡æ¡£
                import numpy as np
                keyword_emb = np.array(keyword_embedding)
                matching_docs = []
                
                # éå†æ‰€æœ‰æ–‡æ¡£ï¼ŒæŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„æ–‡æ¡£
                for i in range(len(all_docs["documents"])):
                    doc = all_docs["documents"][i]
                    doc_id = all_docs["ids"][i]
                    meta = all_docs["metadatas"][i] if "metadatas" in all_docs and all_docs["metadatas"] else {}
                    
                    # æ£€æŸ¥å…³é”®è¯æ˜¯å¦åœ¨æ–‡æ¡£ä¸­
                    if keyword.lower() in doc.lower():
                        # å¦‚æœæœ‰embeddingsï¼Œè®¡ç®—ç›¸ä¼¼åº¦ç”¨äºæ’åº
                        if "embeddings" in all_docs and all_docs["embeddings"]:
                            embedding = all_docs["embeddings"][i]
                            doc_emb = np.array(embedding)
                            similarity = np.dot(keyword_emb, doc_emb) / (np.linalg.norm(keyword_emb) * np.linalg.norm(doc_emb))
                            distance = 1 - similarity
                        else:
                            # å¦‚æœæ²¡æœ‰embeddingsï¼Œé»˜è®¤è·ç¦»ä¸º0.5
                            distance = 0.5
                        
                        matching_docs.append({
                            "id": doc_id,  # ä½¿ç”¨å®é™…çš„æ–‡æ¡£ID
                            "document": doc,
                            "metadata": meta,
                            "distance": distance
                        })
                
                # æŒ‰è·ç¦»æ’åºå¹¶å–å‰kä¸ª
                matching_docs.sort(key=lambda x: x["distance"])
                return matching_docs[:k]
                
            except Exception as e:
                logger.error(f"âŒ [VectorStore] Keyword search error: {e}")
                return []
                
    async def clear_all(self) -> bool:
        """
        æ¸…é™¤å‘é‡å­˜å‚¨ä¸­çš„æ‰€æœ‰è®°å¿†
        
        Returns:
            æ˜¯å¦æˆåŠŸæ¸…é™¤æ‰€æœ‰è®°å¿†
        """
        with self._lock:
            try:
                # ç›´æ¥åˆ é™¤æ•´ä¸ªé›†åˆå¹¶é‡æ–°åˆ›å»º
                self.client.delete_collection(name=self.collection.name)
                self.collection = self.client.get_or_create_collection(name=config.COLLECTION_NAME)
                return True
            except Exception as e:
                logger.error(f"âŒ [VectorStore] Clear all error: {e}")
                return False

    @classmethod
    async def from_texts(cls, texts: List[str], embedding, metadatas: Optional[List[dict]] = None, **kwargs):
        """ä»æ–‡æœ¬åˆ›å»ºå‘é‡å­˜å‚¨"""
        instance = cls()
        await instance.add_texts(texts, metadatas)
        return instance
    
    async def _start_cleanup_task(self):
        """å¯åŠ¨å®šæ—¶æ¸…ç†ä»»åŠ¡"""
        import time
        import asyncio
        from app.core.config import config
        
        # è·å–æ¸…ç†é—´éš”ï¼ˆé»˜è®¤6å°æ—¶ï¼‰
        cleanup_interval = getattr(config, 'VECTOR_DB_CLEANUP_INTERVAL', 6 * 3600)
        
        while True:
            try:
                # ç­‰å¾…æŒ‡å®šæ—¶é—´
                await asyncio.sleep(cleanup_interval)
                
                # æ‰§è¡Œæ¸…ç†
                await self._perform_cleanup()
            except Exception as e:
                logger.error(f"[VectorStore] å®šæ—¶æ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")
                # å‘ç”Ÿé”™è¯¯åï¼Œç­‰å¾…è¾ƒçŸ­æ—¶é—´åé‡è¯•
                await asyncio.sleep(3600)  # 1å°æ—¶åé‡è¯•
    
    async def _perform_cleanup(self):
        """æ‰§è¡Œå®é™…çš„æ¸…ç†æ“ä½œ"""
        logger.info("[VectorStore] å¼€å§‹æ‰§è¡Œå®šæ—¶æ¸…ç†ä»»åŠ¡")
        
        # 1. æ¸…ç†è¿‡æ—¶çš„è®°å¿†ï¼ˆè¶…è¿‡30å¤©çš„è®°å¿†ï¼‰
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£
            all_docs = self.collection.get(include=["documents", "metadatas"])
            
            if not all_docs["documents"]:
                logger.info("[VectorStore] æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ–‡æ¡£")
                return
            
            from datetime import datetime, timedelta
            current_time = datetime.now()
            old_doc_ids = []
            
            # æ‰¾å‡ºè¶…è¿‡30å¤©çš„æ–‡æ¡£
            for i, metadata in enumerate(all_docs["metadatas"]):
                created_at = metadata.get("created_at")
                if created_at:
                    try:
                        doc_time = datetime.strptime(created_at, "%Y-%m-%d %H:%M:%S")
                        if current_time - doc_time > timedelta(days=30):
                            old_doc_ids.append(all_docs["ids"][i])
                    except Exception:
                        # å¦‚æœæ—¥æœŸæ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡
                        continue
            
            # åˆ é™¤è¿‡æ—¶çš„æ–‡æ¡£
            if old_doc_ids:
                self.collection.delete(ids=old_doc_ids)
                logger.info(f"[VectorStore] åˆ é™¤äº† {len(old_doc_ids)} ä¸ªè¿‡æ—¶çš„æ–‡æ¡£")
        except Exception as e:
            logger.error(f"[VectorStore] æ¸…ç†è¿‡æ—¶æ–‡æ¡£å¤±è´¥: {e}")
        
        # 2. ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ¸…ç†é‡å¤å†…å®¹
        try:
            # è·å–ä¸€äº›ç¤ºä¾‹æ–‡æ¡£ä½œä¸ºæŸ¥è¯¢ï¼Œç”¨äºæ‰¾å‡ºç›¸ä¼¼çš„æ–‡æ¡£
            sample_docs = self.collection.get(n_results=10, include=["documents"])
            
            for doc in sample_docs["documents"]:
                if doc:
                    # åˆ é™¤ä¸ç¤ºä¾‹æ–‡æ¡£ç›¸ä¼¼åº¦è¿‡é«˜çš„æ–‡æ¡£ï¼ˆè¶…è¿‡0.9ï¼‰
                    deleted_count = await self.delete_by_semantic(doc, threshold=0.9)
                    if deleted_count > 0:
                        logger.info(f"[VectorStore] é€šè¿‡è¯­ä¹‰åˆ é™¤äº† {deleted_count} ä¸ªé‡å¤æ–‡æ¡£")
        except Exception as e:
            logger.error(f"[VectorStore] è¯­ä¹‰æ¸…ç†å¤±è´¥: {e}")
        
        logger.info("[VectorStore] å®šæ—¶æ¸…ç†ä»»åŠ¡å®Œæˆ")


vector_db = VectorMemory()
