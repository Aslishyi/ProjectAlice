import chromadb
import hashlib
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import math
import threading
from openai import OpenAI
from langchain_core.vectorstores import VectorStore
from langchain_core.documents import Document
from app.core.config import config
from app.utils.cache import cached_embedding_get, cached_embedding_set

# é…ç½®æ—¥å¿—
logger = logging.getLogger("VectorStore")


class VectorMemory(VectorStore):
    """
    å‘é‡å­˜å‚¨å™¨ï¼Œå®ç°äº†LangChainçš„VectorStoreæ¥å£
    """
    
    def __init__(self):
        # 1. åˆå§‹åŒ– ChromaDB
        self.client = chromadb.PersistentClient(path=config.VECTOR_DB_PATH)
        self._lock = threading.Lock()  # åˆå§‹åŒ–äº’æ–¥é”

        # 2. åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.openai_client = OpenAI(
            api_key=config.SILICONFLOW_API_KEY,
            base_url=config.SILICONFLOW_BASE_URL
        )

        # 3. åˆå§‹åŒ–åµŒå…¥å‡½æ•°
        self.embedding_model = config.EMBEDDING_MODEL_NAME

        self.collection = self.client.get_or_create_collection(
            name=config.COLLECTION_NAME
        )

    async def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """æ‰‹åŠ¨ç”ŸæˆåµŒå…¥å‘é‡ï¼Œå¸¦ç¼“å­˜"""
        import asyncio
        from app.utils.cache import cached_embedding_get, cached_embedding_set
        
        texts = [t.replace("\n", " ") for t in texts]
        embeddings = []
        uncached_texts = []
        uncached_indices = []
        
        # å…ˆæ£€æŸ¥ç¼“å­˜
        for i, text in enumerate(texts):
            cached_emb = await cached_embedding_get(text, self.embedding_model)
            if cached_emb:
                embeddings.append(cached_emb)
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
        
        # å¦‚æœæœ‰æœªç¼“å­˜çš„æ–‡æœ¬ï¼Œè°ƒç”¨APIè·å–åµŒå…¥å‘é‡
        if uncached_texts:
            response = self.openai_client.embeddings.create(
                input=uncached_texts,
                model=self.embedding_model
            )
            uncached_embeddings = [data.embedding for data in response.data]
            
            # å°†æ–°è·å–çš„åµŒå…¥å‘é‡åŠ å…¥ç»“æœå¹¶ç¼“å­˜
            for idx, text, emb in zip(uncached_indices, uncached_texts, uncached_embeddings):
                embeddings.insert(idx, emb)
                await cached_embedding_set(text, self.embedding_model, emb)
        
        return embeddings

    async def add_texts(self, texts: List[str], metadatas: Optional[List[dict]] = None,
                  ids: Optional[List[str]] = None, **kwargs) -> List[str]:
        """æ·»åŠ æ–‡æœ¬åˆ°å‘é‡å­˜å‚¨"""
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        if not texts: return []

        # å¦‚æœæ²¡æœ‰æä¾›IDï¼Œç”Ÿæˆå”¯ä¸€ID
        if not ids:
            ids = [f"mem_{hash(t)}" for t in texts]

        final_metadatas = []
        if metadatas:
            for m in metadatas:
                if "importance" not in m: m["importance"] = 1
                if "created_at" not in m: m["created_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                final_metadatas.append(m)
        else:
            now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            final_metadatas = [{"source": "interaction", "importance": 1, "created_at": now_str}] * len(texts)

        # æ‰‹åŠ¨ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆå¼‚æ­¥ï¼‰
        embeddings = await self._generate_embeddings(texts)

        # åŠ é”å†™å…¥
        with self._lock:
            try:
                self.collection.upsert(
                    documents=texts,
                    embeddings=embeddings,
                    metadatas=final_metadatas,
                    ids=ids
                )
                return ids
            except Exception as e:
                logger.error(f"[{ts}] âŒ [VectorStore Write Error] {e}")
                return []

    async def similarity_search(self, query: str, k: int = 4, **kwargs) -> List[Document]:
        """ç›¸ä¼¼æ€§æœç´¢"""
        with self._lock:
            try:
                # æ‰‹åŠ¨ç”ŸæˆæŸ¥è¯¢åµŒå…¥å‘é‡ï¼ˆå¼‚æ­¥ï¼‰
                query_embedding = await self._generate_embeddings([query])[0]
                
                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=k * 3,
                    include=["documents", "metadatas", "distances"]
                )
            except Exception as e:
                logger.error(f"[VectorStore Error] Search failed: {e}")
                return []

        if not results["documents"]:
            return []

        docs = results["documents"][0]
        metas = results["metadatas"][0]
        dists = results["distances"][0]

        scored_candidates = []

        for doc, meta, dist in zip(docs, metas, dists):
            # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆè·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼‰
            semantic_score = 1.0 / (1.0 + dist)
            document = Document(page_content=doc, metadata=meta)
            scored_candidates.append((document, semantic_score))

        # æŒ‰å¾—åˆ†æ’åºï¼Œå–å‰kä¸ª
        scored_candidates.sort(key=lambda x: x[1], reverse=True)
        return [doc_score[0] for doc_score in scored_candidates[:k]]

    async def similarity_search_with_score(self, query: str, k: int = 4, **kwargs) -> List[tuple[Document, float]]:
        """å¸¦åˆ†æ•°çš„ç›¸ä¼¼æ€§æœç´¢"""
        with self._lock:
            try:
                # æ‰‹åŠ¨ç”ŸæˆæŸ¥è¯¢åµŒå…¥å‘é‡ï¼ˆå¼‚æ­¥ï¼‰
                query_embedding = await self._generate_embeddings([query])[0]
                
                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=k * 3,
                    include=["documents", "metadatas", "distances"]
                )
            except Exception as e:
                logger.error(f"[VectorStore Error] Search failed: {e}")
                return []

        if not results["documents"]:
            return []

        docs = results["documents"][0]
        metas = results["metadatas"][0]
        dists = results["distances"][0]

        scored_candidates = []

        for doc, meta, dist in zip(docs, metas, dists):
            # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆè·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼‰
            semantic_score = 1.0 / (1.0 + dist)
            document = Document(page_content=doc, metadata=meta)
            scored_candidates.append((document, semantic_score))

        # æŒ‰å¾—åˆ†æ’åºï¼Œå–å‰kä¸ª
        scored_candidates.sort(key=lambda x: x[1], reverse=True)
        return scored_candidates[:k]

    def _calculate_time_decay(self, created_at_str: str, half_life_hours: float = 48.0) -> float:
        """è®¡ç®—æ—¶é—´è¡°å‡å› å­"""
        try:
            mem_time = datetime.strptime(created_at_str, "%Y-%m-%d %H:%M:%S")
            delta_hours = (datetime.now() - mem_time).total_seconds() / 3600.0
            decay = max(0.3, math.pow(0.5, delta_hours / half_life_hours))
            return decay
        except:
            return 1.0

    async def search(self, query: str, k: int = 3) -> List[str]:
        """è‡ªå®šä¹‰æœç´¢ï¼Œè€ƒè™‘æ—¶é—´è¡°å‡å’Œé‡è¦æ€§ï¼Œå¸¦ç¼“å­˜"""
        import asyncio
        from app.utils.cache import cached_context_get, cached_context_set
        
        # å…ˆæ£€æŸ¥ä¸Šä¸‹æ–‡ç¼“å­˜
        cache_key = f"vector_search:{hash(query)}:{k}"
        cached_results = await cached_context_get(cache_key)
        if cached_results:
            return cached_results
        
        with self._lock:
            try:
                # æ‰‹åŠ¨ç”ŸæˆæŸ¥è¯¢åµŒå…¥å‘é‡ï¼ˆä½¿ç”¨å¼‚æ­¥æ–¹æ³•ï¼‰
                query_embedding = await self._generate_embeddings([query])
                query_embedding = query_embedding[0] if query_embedding else []
                
                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=k * 3,
                    include=["documents", "metadatas", "distances"]
                )
            except Exception as e:
                logger.error(f"[VectorStore Error] Search failed: {e}")
                return []

        if not results["documents"]:
            return []

        docs = results["documents"][0]
        metas = results["metadatas"][0]
        dists = results["distances"][0]

        scored_candidates = []

        for doc, meta, dist in zip(docs, metas, dists):
            semantic_score = 1.0 / (1.0 + dist)
            created_at = meta.get("created_at", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
            time_score = self._calculate_time_decay(created_at)
            importance = float(meta.get("importance", 1))
            imp_boost = 1.0 + (importance * 0.15)

            final_score = semantic_score * time_score * imp_boost
            scored_candidates.append((final_score, doc))

        scored_candidates.sort(key=lambda x: x[0], reverse=True)
        top_docs = [item[1] for item in scored_candidates[:k]]
        
        # å°†ç»“æœå­˜å…¥ä¸Šä¸‹æ–‡ç¼“å­˜
        await cached_context_set(cache_key, top_docs, ttl=1800)  # ç¼“å­˜30åˆ†é’Ÿ

        return top_docs

    async def delete_by_semantic(self, query: str, threshold: float = 0.3):
        """é€šè¿‡è¯­ä¹‰åˆ é™¤ç›¸ä¼¼é¡¹"""
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with self._lock:
            try:
                # æ‰‹åŠ¨ç”ŸæˆæŸ¥è¯¢åµŒå…¥å‘é‡ï¼ˆå¼‚æ­¥ï¼‰
                query_embedding = await self._generate_embeddings([query])[0]
                
                # ä½¿ç”¨æŸ¥è¯¢è·å–ç›¸ä¼¼çš„æ–‡æ¡£
                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=10,
                    include=["documents", "metadatas", "distances", "embeddings"]
                )
                
                if not results["documents"] or not results["documents"][0]:
                    return 0
                
                # è®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦
                import numpy as np
                query_emb = np.array(query_embedding)
                ids_to_delete = []
                
                # å…ˆè·å–æ‰€æœ‰æ–‡æ¡£IDå’Œå†…å®¹çš„æ˜ å°„ï¼Œä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
                all_docs = self.collection.get()
                doc_id_map = {}
                for doc_id, doc_content in zip(all_docs["ids"], all_docs["documents"]):
                    doc_id_map[doc_content] = doc_id
                
                for i in range(len(results["documents"][0])):
                    doc = results["documents"][0][i]
                    embedding = results["embeddings"][0][i]
                    distance = results["distances"][0][i]
                    
                    doc_emb = np.array(embedding)
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = np.dot(query_emb, doc_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(doc_emb))
                    
                    logger.debug(f"Doc {i}: {doc[:50]}..., Similarity: {similarity:.4f}, Distance: {distance:.4f}")
                    
                    # ä½™å¼¦ç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼æ—¶åˆ é™¤
                    if similarity > threshold:
                        # ä½¿ç”¨æ–‡æ¡£å†…å®¹æŸ¥æ‰¾å¯¹åº”çš„ID
                        if doc in doc_id_map:
                            ids_to_delete.append(doc_id_map[doc])
                            logger.debug(f"Found ID {doc_id_map[doc]} for document")
                
                logger.debug(f"Total ids to delete: {len(ids_to_delete)}")
                logger.debug(f"Ids to delete: {ids_to_delete}")
                
                if ids_to_delete:
                    self.collection.delete(ids=ids_to_delete)
                    logger.info(f"[{ts}] ğŸ§¹ [Memory] Deleted {len(ids_to_delete)} items.")
                    return len(ids_to_delete)
                return 0
            except Exception as e:
                logger.error(f"[{ts}] [VectorStore] Delete error: {e}")
                return 0

    # å®ç°LangChain VectorStoreæ¥å£çš„å…¶ä»–å¿…è¦æ–¹æ³•
    def as_retriever(self, **kwargs):
        """è½¬æ¢ä¸ºæ£€ç´¢å™¨"""
        from langchain_classic.memory.vectorstore import VectorStoreRetriever
        return VectorStoreRetriever(vectorstore=self, **kwargs)

    @classmethod
    async def from_documents(cls, documents: List[Document], embedding, **kwargs):
        """ä»æ–‡æ¡£åˆ›å»º"""
        instance = cls()
        texts = [doc.page_content for doc in documents]
        metas = [doc.metadata for doc in documents]
        await instance.add_texts(texts, metas)
        return instance

    async def delete(self, ids: List[str], **kwargs):
        """åˆ é™¤æŒ‡å®šIDçš„æ–‡æ¡£"""
        with self._lock:
            try:
                self.collection.delete(ids=ids)
                return True
            except Exception as e:
                logger.error(f"âŒ [VectorStore] Delete error: {e}")
                return False
                
    async def search_by_keyword(self, keyword: str, k: int = 10) -> List[Dict[str, Any]]:
        """
        é€šè¿‡å…³é”®è¯æœç´¢è®°å¿†
        
        Args:
            keyword: è¦æœç´¢çš„å…³é”®è¯
            k: è¿”å›ç»“æœçš„æ•°é‡
            
        Returns:
            åŒ…å«documentã€metadataå’Œdistanceçš„å­—å…¸åˆ—è¡¨
        """
        with self._lock:
            try:
                # è·å–æ‰€æœ‰æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®
                all_docs = self.collection.get()
                
                if not all_docs["documents"]:
                    return []
                
                # æ‰‹åŠ¨ç”Ÿæˆå…³é”®è¯åµŒå…¥å‘é‡ï¼ˆç”¨äºè®¡ç®—ç›¸ä¼¼åº¦ï¼‰
                keyword_embedding = await self._generate_embeddings([keyword])[0]
                
                # è¿‡æ»¤åŒ…å«å…³é”®è¯çš„æ–‡æ¡£
                import numpy as np
                keyword_emb = np.array(keyword_embedding)
                matching_docs = []
                
                # éå†æ‰€æœ‰æ–‡æ¡£ï¼ŒæŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„æ–‡æ¡£
                for i in range(len(all_docs["documents"])):
                    doc = all_docs["documents"][i]
                    doc_id = all_docs["ids"][i]
                    meta = all_docs["metadatas"][i] if "metadatas" in all_docs and all_docs["metadatas"] else {}
                    
                    # æ£€æŸ¥å…³é”®è¯æ˜¯å¦åœ¨æ–‡æ¡£ä¸­
                    if keyword.lower() in doc.lower():
                        # å¦‚æœæœ‰embeddingsï¼Œè®¡ç®—ç›¸ä¼¼åº¦ç”¨äºæ’åº
                        if "embeddings" in all_docs and all_docs["embeddings"]:
                            embedding = all_docs["embeddings"][i]
                            doc_emb = np.array(embedding)
                            similarity = np.dot(keyword_emb, doc_emb) / (np.linalg.norm(keyword_emb) * np.linalg.norm(doc_emb))
                            distance = 1 - similarity
                        else:
                            # å¦‚æœæ²¡æœ‰embeddingsï¼Œé»˜è®¤è·ç¦»ä¸º0.5
                            distance = 0.5
                        
                        matching_docs.append({
                            "id": doc_id,  # ä½¿ç”¨å®é™…çš„æ–‡æ¡£ID
                            "document": doc,
                            "metadata": meta,
                            "distance": distance
                        })
                
                # æŒ‰è·ç¦»æ’åºå¹¶å–å‰kä¸ª
                matching_docs.sort(key=lambda x: x["distance"])
                return matching_docs[:k]
                
            except Exception as e:
                logger.error(f"âŒ [VectorStore] Keyword search error: {e}")
                return []
                
    async def clear_all(self) -> bool:
        """
        æ¸…é™¤å‘é‡å­˜å‚¨ä¸­çš„æ‰€æœ‰è®°å¿†
        
        Returns:
            æ˜¯å¦æˆåŠŸæ¸…é™¤æ‰€æœ‰è®°å¿†
        """
        with self._lock:
            try:
                # ç›´æ¥åˆ é™¤æ•´ä¸ªé›†åˆå¹¶é‡æ–°åˆ›å»º
                self.client.delete_collection(name=self.collection.name)
                self.collection = self.client.get_or_create_collection(name=config.COLLECTION_NAME)
                return True
            except Exception as e:
                logger.error(f"âŒ [VectorStore] Clear all error: {e}")
                return False

    @classmethod
    async def from_texts(cls, texts: List[str], embedding, metadatas: Optional[List[dict]] = None, **kwargs):
        """ä»æ–‡æœ¬åˆ›å»ºå‘é‡å­˜å‚¨"""
        instance = cls()
        await instance.add_texts(texts, metadatas)
        return instance


vector_db = VectorMemory()
