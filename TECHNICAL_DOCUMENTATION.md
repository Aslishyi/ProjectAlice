# ProjectAlice æŠ€æœ¯æ–‡æ¡£

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 é¡¹ç›®ç®€ä»‹
ProjectAlice æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½èŠå¤©æœºå™¨äººé¡¹ç›®ï¼Œå…·å¤‡å¤šæ¨¡æ€æ„ŸçŸ¥ã€ä¸Šä¸‹æ–‡ç†è§£ã€ä¸»åŠ¨å¯¹è¯å’Œé•¿æœŸè®°å¿†ç­‰èƒ½åŠ›ã€‚é¡¹ç›®é€šè¿‡ LangGraph æ„å»ºäº†å®Œæ•´çš„è®¤çŸ¥æµç¨‹ï¼Œæ”¯æŒ QQ å¹³å°çš„æ¶ˆæ¯äº¤äº’ã€‚

### 1.2 æŠ€æœ¯æ ˆ
- **æ ¸å¿ƒæ¡†æ¶**: Python
- **å·¥ä½œæµå¼•æ“**: LangGraph
- **å¤§è¯­è¨€æ¨¡å‹**: OpenAI API / ç¡…åŸºæµåŠ¨ (SiliconFlow) / Qwen ç³»åˆ—æ¨¡å‹
- **è®°å¿†ç³»ç»Ÿ**: ChromaDB (å‘é‡æ•°æ®åº“)
- **APIæ¥å£**: FastAPI
- **WebSocket**: ç”¨äº QQ æ¶ˆæ¯é€šä¿¡
- **å›¾åƒå¤„ç†**: Pillow
- **é…ç½®ç®¡ç†**: python-dotenv
- **æ•°æ®éªŒè¯**: Pydantic

### 1.3 ä¸»è¦åŠŸèƒ½
- å¤šæ¨¡æ€æ¶ˆæ¯å¤„ç†ï¼ˆæ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æƒ…ï¼‰
- ä¸Šä¸‹æ–‡ç†è§£ä¸æ™ºèƒ½å›å¤
- ç¾¤èŠä¸ç§èŠåœºæ™¯é€‚é…
- ä¸»åŠ¨å¯¹è¯å‘èµ·æœºåˆ¶
- é•¿æœŸè®°å¿†ä¸è¯­ä¹‰æœç´¢
- æƒ…ç»ªä¸å¿ƒç†çŠ¶æ€æ¨¡æ‹Ÿ
- å·¥å…·é›†æˆï¼ˆç½‘ç»œæœç´¢ã€å›¾ç‰‡ç”Ÿæˆç­‰ï¼‰

## 2. é¡¹ç›®ç»“æ„

### 2.1 ç›®å½•ç»“æ„
```
ProjectAlice/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ background/          # åå°ä»»åŠ¡æ¨¡å—
â”‚   â”œâ”€â”€ core/                # æ ¸å¿ƒé…ç½®ä¸çŠ¶æ€ç®¡ç†
â”‚   â”œâ”€â”€ graph/               # LangGraph å·¥ä½œæµå®šä¹‰
â”‚   â”‚   â””â”€â”€ nodes/           # å·¥ä½œæµèŠ‚ç‚¹å®ç°
â”‚   â”œâ”€â”€ memory/              # è®°å¿†ç³»ç»Ÿå®ç°
â”‚   â”œâ”€â”€ monitor/             # ç›‘æ§æ¨¡å—
â”‚   â”œâ”€â”€ tools/               # å·¥å…·é›†æˆæ¨¡å—
â”‚   â””â”€â”€ utils/               # å·¥å…·å‡½æ•°é›†åˆ
â”œâ”€â”€ tests/                   # æµ‹è¯•ä»£ç 
â”œâ”€â”€ web/                     # Web ç•Œé¢
â”œâ”€â”€ .env.example             # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ main.py                  # ä¸»å…¥å£
â”œâ”€â”€ qq_server.py             # QQ æœåŠ¡å™¨å®ç°
â”œâ”€â”€ requirements.txt         # ä¾èµ–åˆ—è¡¨
â””â”€â”€ server.py                # Web æœåŠ¡å™¨
```

### 2.2 æ ¸å¿ƒæ–‡ä»¶è¯´æ˜
- **main.py**: é¡¹ç›®ä¸»å…¥å£ï¼Œæä¾›å‘½ä»¤è¡Œäº¤äº’
- **qq_server.py**: QQ æ¶ˆæ¯æœåŠ¡å™¨ï¼Œå¤„ç†æ¶ˆæ¯æ¥æ”¶ä¸å‘é€
- **app/core/config.py**: é¡¹ç›®é…ç½®ç®¡ç†
- **app/core/state.py**: æ™ºèƒ½ä½“çŠ¶æ€å®šä¹‰
- **app/graph/graph_builder.py**: å·¥ä½œæµæ„å»º
- **app/memory/vector_store.py**: å‘é‡è®°å¿†å­˜å‚¨
- **app/utils/safety.py**: å®‰å…¨è¿‡æ»¤æ¨¡å—

## 3. ç³»ç»Ÿæ¶æ„

### 3.1 æ•´ä½“æ¶æ„è®¾è®¡
ProjectAlice é‡‡ç”¨æ¨¡å—åŒ–æ¶æ„è®¾è®¡ï¼Œä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ å±‚ï¼š

1. **æ¥å…¥å±‚**: è´Ÿè´£ä¸å¤–éƒ¨å¹³å°ï¼ˆå¦‚ QQï¼‰çš„æ¶ˆæ¯äº¤äº’
2. **å·¥ä½œæµå±‚**: ä½¿ç”¨ LangGraph æ„å»ºçš„æ™ºèƒ½ä½“è®¤çŸ¥æµç¨‹
3. **æ ¸å¿ƒå±‚**: åŒ…å«çŠ¶æ€ç®¡ç†ã€é…ç½®ç®¡ç†å’Œå·¥å…·é›†æˆ
4. **è®°å¿†å±‚**: æä¾›é•¿æœŸè®°å¿†å’ŒçŸ­æœŸä¸Šä¸‹æ–‡ç®¡ç†
5. **å·¥å…·å±‚**: é›†æˆå„ç§å¤–éƒ¨å·¥å…·å’ŒæœåŠ¡

### 3.2 æ¨¡å—é—´å…³ç³»å›¾
```mermaid
graph TD
    A[QQ Server] --> B[LangGraph Workflow]
    B --> C[Context Filter]
    B --> D[Agent Node]
    B --> E[Tool Handler]
    B --> F[Memory Saver]
    D --> G[LLM Interface]
    E --> H[Web Search]
    E --> I[Image Generation]
    F --> J[Vector Store]
    F --> K[Local History]
    C --> L[Safety Filter]
```

## 4. æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

### 4.1 çŠ¶æ€ç®¡ç† (app/core/state.py)
AgentState æ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œç”¨äºåœ¨ LangGraph å·¥ä½œæµä¸­ä¼ é€’çŠ¶æ€ä¿¡æ¯ã€‚å®ƒè®¾è®¡ä¸º TypedDict ç±»å‹ï¼Œç¡®ä¿äº†ç±»å‹å®‰å…¨å’Œæ¸…æ™°çš„çŠ¶æ€ç»“æ„ã€‚

```python
from typing import TypedDict, List, Optional, Dict, Any
from langchain_core.messages import BaseMessage

class AgentState(TypedDict):
    # --- åŸºç¡€æ¶ˆæ¯æµ ---
    messages: List[BaseMessage]       # å®Œæ•´çš„å¯¹è¯å†å²
    conversation_summary: str          # å¯¹è¯æ‘˜è¦ï¼Œç”¨äºé•¿æœŸè®°å¿†

    # --- æ ¸å¿ƒèº«ä»½ä¸ç¯å¢ƒ ---
    session_id: str                    # ä¼šè¯å”¯ä¸€æ ‡è¯†
    sender_qq: str                     # å‘é€è€…QQå·ç 
    sender_name: str                   # å‘é€è€…åç§°
    is_group: bool                     # æ˜¯å¦ä¸ºç¾¤èŠ
    is_mentioned: bool                 # æ˜¯å¦è¢«@æåŠ

    # --- æµç¨‹æ§åˆ¶ ---
    should_reply: bool                 # æ˜¯å¦éœ€è¦å›å¤
    filter_reason: str                 # è¿‡æ»¤åŸå› ï¼ˆå¦‚æœä¸éœ€è¦å›å¤ï¼‰
    is_proactive_mode: bool            # æ˜¯å¦ä¸ºä¸»åŠ¨å¯¹è¯æ¨¡å¼

    # --- è§†è§‰ä¼˜åŒ– ---
    image_urls: List[str]              # æ¶ˆæ¯ä¸­çš„å›¾ç‰‡URLåˆ—è¡¨

    # --- ä¸Šä¸‹æ–‡ä¸çŠ¶æ€ ---
    psychological_context: Dict[str, Any]      # å¿ƒç†çŠ¶æ€ä¸Šä¸‹æ–‡
    global_emotion_snapshot: Dict[str, Any]    # å…¨å±€æƒ…ç»ªå¿«ç…§
    internal_monologue: str                    # å†…éƒ¨ç‹¬ç™½
    emotion: Any                               # å½“å‰æƒ…ç»ªçŠ¶æ€

    # --- è§†è§‰ç›¸å…³å­—æ®µ ---
    current_image_artifact: Optional[str]  # æœ‰æ„ä¹‰çš„å›¾ç‰‡ï¼ˆBase64ç¼–ç ï¼‰
    visual_input: Optional[str]             # è§†è§‰è¾“å…¥å†…å®¹
    visual_type: Optional[str]              # è§†è§‰ç±»å‹ï¼š'photo', 'sticker', 'icon', 'none'

    # --- å…¶ä»–çŠ¶æ€ä¿¡æ¯ ---
    current_activity: str                  # å½“å‰æ´»åŠ¨
    last_interaction_ts: float             # æœ€åäº¤äº’æ—¶é—´æˆ³
    next_step: str                         # ä¸‹ä¸€æ­¥æ“ä½œ
    user_profile: Dict                     # ç”¨æˆ·èµ„æ–™
    tool_call: Dict[str, Any]              # å·¥å…·è°ƒç”¨ä¿¡æ¯
```

**çŠ¶æ€ç®¡ç†è®¾è®¡äº®ç‚¹**ï¼š
1. **åˆ†å±‚è®¾è®¡**ï¼šå°†çŠ¶æ€æŒ‰åŠŸèƒ½æ¨¡å—æ¸…æ™°åˆ’åˆ†ï¼Œä¾¿äºç»´æŠ¤
2. **ç±»å‹å®‰å…¨**ï¼šä½¿ç”¨ TypedDict ç¡®ä¿çŠ¶æ€å­—æ®µç±»å‹æ­£ç¡®
3. **æ‰©å±•æ€§**ï¼šé¢„ç•™äº†è¶³å¤Ÿçš„å­—æ®µä»¥æ”¯æŒæœªæ¥åŠŸèƒ½æ‰©å±•
4. **å®Œæ•´æ€§**ï¼šåŒ…å«äº†ä»æ¶ˆæ¯æµåˆ°ä¸Šä¸‹æ–‡çŠ¶æ€çš„å®Œæ•´ä¿¡æ¯é“¾

### 4.2 é…ç½®ç®¡ç† (app/core/config.py)
é…ç½®æ¨¡å—ä½¿ç”¨å•ä¾‹æ¨¡å¼ç®¡ç†é¡¹ç›®çš„æ‰€æœ‰é…ç½®å‚æ•°ï¼Œæ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è¿›è¡Œçµæ´»é…ç½®ï¼š

```python
import os
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

class Config:
    # --- LLM Settings ---
    
    # ç¡…åŸºæµåŠ¨ API é…ç½®
    SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
    SILICONFLOW_BASE_URL = os.getenv("SILICONFLOW_BASE_URL", "https://api.siliconflow.cn/v1")
    
    # MIMO API é…ç½®
    MIMO_API_KEY = os.getenv("MIMO_API_KEY")
    MIMO_BASE_URL = os.getenv("MIMO_BASE_URL", "https://api.xiaomimimo.com/v1")
    MIMO_MODEL = os.getenv("MIMO_MODEL")
    
    # æ¨¡å‹é€‰æ‹©
    MODEL_NAME = os.getenv("LLM_MODEL_NAME", "Qwen/Qwen3-VL-30B-A3B-Instruct")  # ä¸»æ¨¡å‹
    SMALL_LLM_MODEL_NAME = os.getenv("SMALL_LLM_MODEL_NAME", "Qwen/Qwen3-VL-8B-Instruct")  # è½»é‡çº§æ¨¡å‹
    EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME", "Qwen/Qwen3-Embedding-8B")  # åµŒå…¥æ¨¡å‹
    TEMPERATURE = 0.7  # ç”Ÿæˆæ¸©åº¦
    
    # --- Vector DB Settings ---
    VECTOR_DB_PATH = os.getenv("VECTOR_DB_PATH", "./chroma_db")  # å‘é‡æ•°æ®åº“è·¯å¾„
    COLLECTION_NAME = "anima_memories"  # è®°å¿†é›†åˆåç§°
    
    # --- Tool Settings ---
    MAX_SEARCH_RESULTS = 3  # æœ€å¤§æœç´¢ç»“æœæ•°
    
    # --- Emotion & Personality Settings ---
    DEFAULT_VALENCE = 0.1  # åˆå§‹æƒ…ç»ªç§¯æåº¦ (ç•¥å¾®ç§¯æ)
    DEFAULT_AROUSAL = 0.5  # åˆå§‹æƒ…ç»ªå”¤é†’åº¦ (å¹³é™ä¸”ä¸“æ³¨)
    
    # --- System Paths ---
    LOG_DIR = "./logs"  # æ—¥å¿—ç›®å½•
    
    # --- Safety Thresholds ---
    SENSITIVE_THRESHOLD = 0.8  # æ•æ„Ÿè¯æ£€æµ‹é˜ˆå€¼


config = Config()
```

**é…ç½®ç®¡ç†è®¾è®¡äº®ç‚¹**ï¼š
1. **ç¯å¢ƒå˜é‡æ”¯æŒ**ï¼šé€šè¿‡ `.env` æ–‡ä»¶å’Œç¯å¢ƒå˜é‡çµæ´»é…ç½®
2. **é»˜è®¤å€¼è®¾ç½®**ï¼šä¸ºå…³é”®é…ç½®é¡¹æä¾›åˆç†é»˜è®¤å€¼ï¼Œé™ä½éƒ¨ç½²éš¾åº¦
3. **æ¨¡å—åŒ–ç»„ç»‡**ï¼šæŒ‰åŠŸèƒ½æ¨¡å—ç»„ç»‡é…ç½®é¡¹ï¼Œä¾¿äºç®¡ç†
4. **å•ä¾‹æ¨¡å¼**ï¼šç¡®ä¿é…ç½®çš„å…¨å±€ä¸€è‡´æ€§
5. **æ‰©å±•æ€§**ï¼šé¢„ç•™äº†å¤šä¸ªAPIæœåŠ¡çš„é…ç½®æ¥å£ï¼Œæ”¯æŒå¤šæ¨¡å‹åˆ‡æ¢

### 4.2.1 æ•°æ®åº“é…ç½® (app/core/database.py)
è´Ÿè´£æ•°æ®åº“è¿æ¥ç®¡ç†ã€ä¼šè¯å·¥å‚åˆ›å»ºå’Œæ•°æ®æ¨¡å‹å®šä¹‰ï¼š

```python
# === æ•°æ®åº“é…ç½®æ–‡ä»¶ ===

from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
import os
from typing import Generator

# æ•°æ®åº“è·¯å¾„
DB_PATH = "data/project_alice.db"

# ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
os.makedirs("data", exist_ok=True)

# åˆ›å»ºæ•°æ®åº“å¼•æ“
engine = create_engine(
    f"sqlite:///{DB_PATH}",
    connect_args={"check_same_thread": False}  # SQLite å¤šçº¿ç¨‹æ”¯æŒ
)

# åˆ›å»ºä¼šè¯å·¥å‚
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# åˆ›å»ºåŸºç±»
Base = declarative_base()


# æ•°æ®åº“ä¾èµ–
def get_db() -> Generator[Session, None, None]:
    """è·å–æ•°æ®åº“ä¼šè¯"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# å¯¼å…¥æ•°æ®åº“æ¨¡å‹
from sqlalchemy import Column, String, Text, DateTime, func

# ä¼šè¯å†å²æ¨¡å‹
class SessionHistoryModel(Base):
    __tablename__ = "session_history"
    
    session_id = Column(String(100), primary_key=True, index=True)
    summary = Column(Text, nullable=False, default="")
    messages = Column(Text, nullable=False, default="[]")  # JSONæ ¼å¼å­˜å‚¨
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())


# åˆå§‹åŒ–æ•°æ®åº“
def init_db():
    """åˆ›å»ºæ‰€æœ‰æ•°æ®åº“è¡¨"""
    from app.memory.relation_db import UserProfileModel  # é¿å…å¾ªç¯å¯¼å…¥
    Base.metadata.create_all(bind=engine)
```

**æ•°æ®åº“é…ç½®è®¾è®¡ç‰¹ç‚¹**ï¼š
1. **SQLiteæ•°æ®åº“**ï¼šä½¿ç”¨è½»é‡çº§çš„SQLiteæ•°æ®åº“ï¼Œä¾¿äºéƒ¨ç½²å’Œç»´æŠ¤
2. **å¤šçº¿ç¨‹æ”¯æŒ**ï¼šé€šè¿‡ `check_same_thread=False` æ”¯æŒå¤šçº¿ç¨‹ç¯å¢ƒ
3. **ä¾èµ–æ³¨å…¥**ï¼šæä¾› `get_db` å‡½æ•°æ”¯æŒä¾èµ–æ³¨å…¥æ¨¡å¼
4. **è‡ªåŠ¨åˆå§‹åŒ–**ï¼šé€šè¿‡ `init_db` å‡½æ•°è‡ªåŠ¨åˆ›å»ºæ‰€æœ‰æ•°æ®åº“è¡¨
5. **æ¨¡å‹å®šä¹‰**ï¼šåŒ…å« `SessionHistoryModel` ç”¨äºå­˜å‚¨ä¼šè¯å†å²æ•°æ®
6. **JSONå­˜å‚¨**ï¼šä¼šè¯æ¶ˆæ¯ä»¥JSONæ ¼å¼å­˜å‚¨åœ¨æ•°æ®åº“ä¸­ï¼Œä¾¿äºåºåˆ—åŒ–å’Œååºåˆ—åŒ–

### 4.3 å·¥ä½œæµå¼•æ“ (app/graph/)

#### 4.3.1 å·¥ä½œæµæ„å»º (graph_builder.py)
ä½¿ç”¨ LangGraph æ„å»ºå®Œæ•´çš„èŠå¤©æœºå™¨äººå·¥ä½œæµç¨‹ï¼š

```python
def build_graph():
    workflow = StateGraph(AgentState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("filter", context_filter_node)
    workflow.add_node("parallel_processor", parallel_processing_node)
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", tool_node)
    workflow.add_node("saver", memory_saver_node)
    workflow.add_node("summarizer", summarizer_node)
    workflow.add_node("perception", perception_node)
    workflow.add_node("proactive", proactive_node)
    
    # å…¥å£è·¯ç”±
    workflow.set_conditional_entry_point(
        route_root,
        {"filter": "filter", "proactive": "proactive"}
    )
    
    # ä¸Šä¸‹æ–‡è¿‡æ»¤å™¨è·¯ç”±
    workflow.add_conditional_edges(
        "filter",
        route_filter,
        {"parallel_processor": "parallel_processor", "summarizer": "summarizer"}
    )
    
    # å“åº”å¼æµç¨‹ä¸»çº¿
    workflow.add_edge("parallel_processor", "agent")
    workflow.add_conditional_edges(
        "agent",
        route_agent_output,
        {"tools": "tools", "saver": "saver"}
    )
    workflow.add_edge("tools", "agent")
    
    # è®°å¿†å¤„ç†æµç¨‹
    workflow.add_edge("saver", "summarizer")
    workflow.add_edge("summarizer", END)
    
    # ä¸»åŠ¨å¼æµç¨‹
    workflow.add_edge("proactive", "summarizer")
    
    return workflow.compile()
```

#### 4.3.2 ä¸Šä¸‹æ–‡è¿‡æ»¤ (context_filter.py)
å†³å®šæ™ºèƒ½ä½“æ˜¯å¦åº”è¯¥å›å¤æ¶ˆæ¯çš„æ ¸å¿ƒé€»è¾‘ï¼š

```python
async def context_filter_node(state: AgentState):
    # å¼ºè§„åˆ™ï¼šè¢«è‰¾ç‰¹å¿…é¡»å›å¤
    if state.get("is_mentioned", False):
        return {"should_reply": True, "filter_reason": "Directly mentioned (Hard Rule)"}
    
    # æå–æœ€åä¸€æ¡æ¶ˆæ¯å†…å®¹
    last_content = _extract_last_message_content(state.get("messages", []))
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡
    has_img = _check_has_image(state, last_content)
    
    # åº”ç”¨å¯å‘å¼é¢„è¿‡æ»¤
    pre_filter_result = _apply_heuristic_pre_filter(state, last_content, has_img)
    if pre_filter_result:
        return pre_filter_result
    
    # è°ƒç”¨ LLM è¿›è¡Œä¸Šä¸‹æ–‡åˆ†æ
    # ...ï¼ˆçœç•¥å…·ä½“å®ç°ï¼‰
```

#### 4.3.3 æ ¸å¿ƒæ™ºèƒ½ä½“èŠ‚ç‚¹ (unified_agent.py)
å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆå›å¤çš„æ ¸å¿ƒé€»è¾‘ï¼ŒåŒ…å«å¤šæ¨¡æ€è¾“å…¥å¤„ç†ã€RAGæ£€ç´¢å’Œå®‰å…¨é˜²æŠ¤ï¼š

```python
async def agent_node(state: AgentState):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    msgs = state.get("messages", [])
    image_data = state.get("current_image_artifact")
    visual_type = state.get("visual_type", "none")

    # æå–æœ€è¿‘ä¸€æ¡æ¶ˆæ¯æ–‡æœ¬
    last_human_content = ""
    if msgs:
        for m in reversed(msgs):
            if isinstance(m, HumanMessage):
                content = m.content
                if isinstance(content, list):
                    content = next((x['text'] for x in content if x['type'] == 'text'), "")
                last_human_content = str(content).strip()
                break

    # ğŸ›¡ï¸ ç¬¬ä¸€é“é˜²çº¿ï¼šçŸ­è·¯æ‹¦æˆª (Short-Circuit)
    if visual_type == "sticker":
        # æ¸…æ´—æ–‡æœ¬ï¼Œç§»é™¤ç”¨æˆ·åå‰ç¼€å’Œå ä½ç¬¦
        temp_text = re.sub(r"^\[.*?\]:\s*", "", last_human_content)
        clean_text = temp_text.replace("[å›¾ç‰‡]", "").replace("[è¡¨æƒ…]", "").replace(" ", "").strip()
        
        # çº¯è¡¨æƒ…ç›´æ¥å›å¤æˆ–å¿½ç•¥
        if len(clean_text) < 2:
            # 50% æ¦‚ç‡å›å¤è¡¨æƒ…ï¼Œ50% æ¦‚ç‡æ²‰é»˜
            if random.random() < 0.6:
                replies = ["ğŸ¶", "ğŸ±", "ğŸ’–", "ğŸ’•", "ğŸ’", "ğŸ¤—", "ğŸ‘»", "ğŸ‘½"]
                reply = random.choice(replies)
                return {
                    "internal_monologue": "Sticker acknowledged.",
                    "messages": msgs + [AIMessage(content=reply)],
                    "last_interaction_ts": time.time(),
                    "next_step": "save"
                }
            else:
                return {
                    "internal_monologue": "Sticker ignored.",
                    "messages": msgs,
                    "last_interaction_ts": time.time(),
                    "next_step": "save"
                }

    # ğŸ§  LLM å¤„ç† (Photo æˆ– å¸¦æœ‰æ–‡å­—çš„ Sticker)
    # RAG æ£€ç´¢ç›¸å…³è®°å¿†
    memory_context = ""
    try:
        query_text = re.sub(r"^\[.*?\]:\s*", "", last_human_content).replace("[å›¾ç‰‡]", "").strip()
        if len(query_text) > 4:
            docs = vector_db.search(query_text, k=3)
            if docs:
                memory_context = f"ã€ç›¸å…³å›å¿†ã€‘\n" + "\n".join(docs)
    except Exception:
        pass

    # æ„é€ å®Œæ•´ Prompt
    final_system_prompt = AGENT_SYSTEM_PROMPT.format(
        core_persona=ALICE_CORE_PERSONA,
        time=now_str,
        current_user=f"{user_display_name} (ID: {real_user_id})",
        vision_summary=vision_summary_text,
        mood_label=psych_ctx.get("primary_emotion", "å¹³æ·¡"),
        internal_thought=psych_ctx.get("internal_thought", "æ€è€ƒä¸­..."),
        style_instruction=psych_ctx.get("style_instruction", "ä¿æŒæ—¥å¸¸è¯­æ°”"),
        intimacy=psych_ctx.get("current_intimacy", 30),
        memories=memory_context
    ) + "\n\n" + format_instruction

    # è°ƒç”¨ LLM å¹¶è§£æç»“æœ
    response = await cached_llm_invoke(llm, input_messages, temperature=llm.temperature)
    parsed_result = robust_json_parse(response.content.strip())
    
    # è¿”å›ç»“æœ
    ai_msg = AIMessage(content=parsed.get("response", "..."))
    return {
        "messages": msgs + [ai_msg],
        "next_step": "save",
        "tool_call": {} if parsed.get("action") == "reply" else {"name": parsed.get("action"), "args": parsed.get("args")}
    }
```

#### 4.3.4 è®°å¿†ä¿å­˜èŠ‚ç‚¹ (memory_saver.py)
è´Ÿè´£ä»å¯¹è¯ä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯å¹¶ä¿å­˜åˆ°å‘é‡æ•°æ®åº“ï¼š

```python
async def memory_saver_node(state: AgentState):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    msgs = state.get("messages", [])
    if not msgs: return {}

    # è·å–ç”¨æˆ·ä¿¡æ¯
    real_user_id = state.get("sender_qq", "unknown")
    user_nickname = state.get("sender_name", "User")

    # ç¡®å®šæ¨¡å¼ï¼ˆäº¤äº’æˆ–è§‚å¯Ÿï¼‰
    last_msg = msgs[-1]
    ai_output = "N/A (AI remained silent)"
    mode = "OBSERVATION"
    user_text = ""

    if last_msg.type == 'ai':
        mode = "INTERACTIVE"
        ai_output = last_msg.content
        if len(msgs) >= 2:
            user_text = msgs[-2].content
        else:
            return {}
    else:
        mode = "OBSERVATION"
        user_text = last_msg.content

    # æå–å…³é”®ä¿¡æ¯
    prompt = ChatPromptTemplate.from_template(MEMORY_SYSTEM_PROMPT)
    formatted_prompt = prompt.format(
        mode=mode,
        user_id=real_user_id,
        user_name=user_nickname,
        user_input=user_text,
        ai_output=ai_output
    )
    
    # è°ƒç”¨ LLM æå–è®°å¿†
    resp = await cached_llm_invoke(llm, [SystemMessage(content=formatted_prompt)], temperature=llm.temperature)
    raw_content = resp.content.strip().replace("```json", "").replace("```", "").strip()
    data = json.loads(raw_content)

    # ä¿å­˜æå–çš„è®°å¿†
    operations = data.get("operations", [])
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    facts_to_add = []
    metadatas_to_add = []

    for op in operations:
        action = op.get("action")
        content = op.get("content", "")
        category = op.get("category", "event")
        importance = op.get("importance", 1)

        if mode == "OBSERVATION" and importance < 3:
            continue

        if action == "add":
            final_content = f"User {user_nickname} (ID:{real_user_id}): {content}"
            full_text = f"[{current_time}] ({category.upper()}) {final_content}"

            facts_to_add.append(full_text)
            metadatas_to_add.append({
                "source": "chat" if mode == "INTERACTIVE" else "observation",
                "user_id": real_user_id,
                "created_at": current_time,
                "importance": importance,
                "category": category
            })

    if facts_to_add:
        vector_db.add_texts(facts_to_add, metadatas_to_add)

    return {}
```

#### 4.3.5 å·¥å…·å¤„ç†èŠ‚ç‚¹ (tool_handler.py)
æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶å°†ç»“æœä½œä¸º ToolMessage æ³¨å…¥å¯¹è¯å†å²ï¼š

```python
async def tool_node(state: AgentState):
    """
    æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼Œå¹¶å°†ç»“æœä½œä¸º ToolMessage æ³¨å…¥å†å²
    """
    current_messages = state.get("messages", [])
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    tool_data = state.get("tool_call", {})
    tool_name = tool_data.get("name")
    tool_args = tool_data.get("args")

    # ç”Ÿæˆä¸€ä¸ªéšæœºçš„ tool_call_id
    tool_call_id = str(uuid.uuid4())

    print(f"[{ts}] --- [Tools] Executing: {tool_name} with {tool_args} ---")

    result = "Tool execution failed."

    try:
        # å‚æ•°æ¸…æ´—
        final_arg = tool_args
        if isinstance(tool_args, dict):
            if "query" in tool_args:
                final_arg = tool_args["query"]
            elif "prompt" in tool_args:
                final_arg = tool_args["prompt"]
            elif "code" in tool_args:
                final_arg = tool_args["code"]
            else:
                final_arg = list(tool_args.values())[0]

        final_arg = str(final_arg)

        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        if tool_name == "web_search":
            result = perform_web_search.invoke(final_arg)
        elif tool_name == "generate_image":
            url = generate_image.invoke(final_arg)
            result = f"IMAGE_GENERATED: {url}"
        elif tool_name == "run_python_analysis":
            result = run_python_analysis.invoke(final_arg)
        else:
            result = f"Unknown tool: {tool_name}"

    except Exception as e:
        print(f"[{ts}] [Tool Error] {e}")
        result = f"Tool Error: {str(e)}"

    # åˆ›å»º ToolMessage
    tool_msg = ToolMessage(
        content=f"[System: Tool '{tool_name}' Result]\n{str(result)}",
        tool_call_id=tool_call_id,
        name=tool_name
    )

    return {
        "messages": current_messages + [tool_msg],
        "tool_call": {}
    }
```

### 4.4 è®°å¿†ç³»ç»Ÿ (app/memory/)

#### 4.4.1 å‘é‡å­˜å‚¨ (vector_store.py)
ä½¿ç”¨ ChromaDB å®ç°çš„é•¿æœŸè®°å¿†ç³»ç»Ÿï¼Œå…·å¤‡è¯­ä¹‰æœç´¢ã€æ—¶é—´è¡°å‡å’Œé‡è¦æ€§è¯„åˆ†ç­‰åŠŸèƒ½ï¼š

#### 4.4.2 è®°å¿†å›ºåŒ–ä¸æ¸…ç† (app/background/dream.py)
DreamCycle æ˜¯ä¸€ä¸ªåå°ä»»åŠ¡æ¨¡å—ï¼Œè´Ÿè´£è®°å¿†çš„ä¼˜åŒ–å’Œç®¡ç†ï¼ŒåŒ…æ‹¬æ¸…ç†è¿‡æ—¶è®°å¿†å’Œå›ºåŒ–é‡è¦è®°å¿†ç¢ç‰‡ï¼š

```python
import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

from app.core.config import config
from app.memory.vector_store import vector_db
from app.core.global_store import global_store
from app.utils.cache import cached_llm_invoke

logger = logging.getLogger("DreamCycle")

# --- è®°å¿†å›ºåŒ– Prompt ---
CONSOLIDATION_PROMPT = """
ä½ æ˜¯ Alice çš„æ½œæ„è¯†æ•´ç†è€…ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç¢ç‰‡åŒ–çš„çŸ­æœŸè®°å¿†åˆå¹¶ä¸ºæœ‰ä»·å€¼çš„é•¿æœŸè®°å¿†ã€‚

ã€å¾…æ•´ç†çš„è®°å¿†ç¢ç‰‡ã€‘
{fragments}

ã€ä»»åŠ¡è¦æ±‚ã€‘
1. åˆ†æè¿™äº›ç¢ç‰‡ä¹‹é—´æ˜¯å¦å­˜åœ¨å…³è”ï¼ˆä¾‹å¦‚ï¼šéƒ½æ˜¯å…³äºé¥®é£Ÿåå¥½ã€éƒ½æ˜¯å…³äºæŸä¸ªç‰¹å®šé¡¹ç›®ã€æˆ–è€…æ˜¯è¿ç»­çš„äº‹ä»¶ï¼‰ã€‚
2. å¦‚æœå­˜åœ¨å…³è”ï¼Œè¯·å°†å®ƒä»¬**æ¦‚æ‹¬**ä¸ºä¸€æ¡ç®€æ´çš„ã€åŒ…å«æ ¸å¿ƒä¿¡æ¯çš„é™ˆè¿°å¥ã€‚
3. æ¦‚æ‹¬åçš„è®°å¿†åº”å½“å»é™¤æ—¶é—´çŠ¶è¯­ï¼ˆå¦‚â€œåˆšæ‰â€ã€â€œä»Šå¤©â€ï¼‰ï¼Œè½¬å˜ä¸ºæŒä¹…çš„äº‹å®æè¿°ã€‚
4. å¦‚æœç¢ç‰‡ä¹‹é—´æ²¡æœ‰æ˜æ˜¾å…³è”ï¼Œæˆ–è€…ä¿¡æ¯å¤ªæ‚ä¹±æ— æ³•åˆå¹¶ï¼Œè¯·è¾“å‡º "SKIP"ã€‚

ã€è¾“å‡ºç¤ºä¾‹ã€‘
è¾“å…¥ç¢ç‰‡: ["ç”¨æˆ·è¯´ä»Šå¤©æƒ³åƒè¾£", "ä¸­åˆç‚¹äº†éº»è¾£çƒ«", "æ™šä¸Šè¿˜åœ¨æ‰¾ç«é”…åº—"]
è¾“å‡º: ç”¨æˆ·éå¸¸å–œæ¬¢åƒè¾£çš„é£Ÿç‰©ï¼Œå°¤å…¶æ˜¯éº»è¾£çƒ«å’Œç«é”…ã€‚

è¯·è¾“å‡ºç»“æœ (çº¯æ–‡æœ¬):
"""


class DreamCycle:
    def __init__(self, interval_seconds=1800):
        """
        :param interval_seconds: åšæ¢¦å¾ªç¯çš„é—´éš”ï¼Œé»˜è®¤ 30 åˆ†é’Ÿ (1800ç§’)
        """
        self.interval = interval_seconds
        self.running = False
        self._task = None

        # ä¸“é—¨ç”¨äºæ•´ç†è®°å¿†çš„ LLMï¼Œå¯ä»¥ä½¿ç”¨ä¾¿å®œçš„æ¨¡å‹ (å¦‚ gpt-3.5-turbo æˆ– qwen-turbo) ä»¥ä¿è¯é€Ÿåº¦
        self.llm = ChatOpenAI(
            model=config.LLM_MODEL_NAME,
            temperature=0.1,
            api_key=config.SILICONFLOW_API_KEY,
            base_url=config.SILICONFLOW_BASE_URL
        )

    async def start(self):
        self.running = True
        self._task = asyncio.create_task(self._dream_loop())
        logger.info("ğŸ’¤ [Dream] Background memory consolidation module started.")

    async def stop(self):
        self.running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass

    async def _dream_loop(self):
        while self.running:
            try:
                # ç­‰å¾…ä¸‹ä¸€ä¸ªå‘¨æœŸ
                await asyncio.sleep(self.interval)

                # 1. æ£€æŸ¥æ´»è·ƒåº¦ï¼šå¦‚æœç”¨æˆ·æœ€è¿‘ 5 åˆ†é’Ÿè¿˜åœ¨è¯´è¯ï¼Œä¸è¦åšæ¢¦ï¼Œé¿å…æ•°æ®åº“é”å†²çª
                last_active_str = global_store.get_emotion_snapshot().last_updated
                last_active = datetime.strptime(last_active_str, "%Y-%m-%d %H:%M:%S")
                if (datetime.now() - last_active).total_seconds() < 300:
                    logger.info("ğŸ’¤ [Dream] User is active. Postponing dream cycle.")
                    continue

                logger.info("ğŸ’¤ [Dream] Entering REM sleep (Memory Optimization)...")

                # 2. æ‰§è¡Œæ¸…ç†é€»è¾‘
                deleted_count = self._prune_garbage_memories(days_threshold=3)

                # 3. æ‰§è¡Œå›ºåŒ–é€»è¾‘
                consolidated_count = await self._consolidate_memories()

                # 4. æ¢å¤ä½“åŠ› (ä½œä¸ºå¥–åŠ±)
                if deleted_count > 0 or consolidated_count > 0:
                    global_store.update_emotion(0, 0, stamina_delta=30.0)
                    logger.info(
                        f"ğŸ’¤ [Dream] Cycle Done. Pruned: {deleted_count}, Consolidated: {consolidated_count}. Stamina Recovered.")
                else:
                    logger.info("ğŸ’¤ [Dream] Deep sleep. No memories needed processing.")

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ [Dream Error] {e}", exc_info=True)

    def _prune_garbage_memories(self, days_threshold: int = 3) -> int:
        """
        æ¸…ç†é€»è¾‘ï¼šåˆ é™¤ [importance=1] ä¸” [åˆ›å»ºæ—¶é—´ > 3å¤©] çš„è®°å¿†
        """
        try:
            # Chroma API è·å–æ‰€æœ‰ metadata (limit è®¾å¤§ä¸€ç‚¹ä»¥è¦†ç›–)
            # æ³¨æ„ï¼šå¦‚æœæ•°æ®é‡å·¨å¤§ï¼Œè¿™é‡Œéœ€è¦åˆ†é¡µå¤„ç†ï¼ŒDemo ä¸­æš‚ä¸”ä¸€æ¬¡æ€§è·å–
            result = vector_db.collection.get(include=["metadatas"])

            ids = result["ids"]
            metadatas = result["metadatas"]

            ids_to_delete = []
            now = datetime.now()
            cutoff_date = now - timedelta(days=days_threshold)

            for i, meta in enumerate(metadatas):
                # æ£€æŸ¥ Importance (å¦‚æœæ²¡æœ‰å­—æ®µï¼Œé»˜è®¤ä¸º 1)
                importance = meta.get("importance", 1)

                # åªæ¸…ç†ä½æƒé‡è®°å¿†
                if importance > 1:
                    continue

                # æ£€æŸ¥æ—¶é—´
                created_at_str = meta.get("created_at")
                if created_at_str:
                    try:
                        mem_time = datetime.strptime(created_at_str, "%Y-%m-%d %H:%M:%S")
                        if mem_time < cutoff_date:
                            ids_to_delete.append(ids[i])
                    except ValueError:
                        continue  # æ—¶é—´æ ¼å¼ä¸å¯¹åˆ™è·³è¿‡

            if ids_to_delete:
                logger.info(f"ğŸ§¹ [Dream] Pruning {len(ids_to_delete)} garbage memories...")
                vector_db.collection.delete(ids=ids_to_delete)
                return len(ids_to_delete)

            return 0

        except Exception as e:
            logger.error(f"Error in pruning: {e}")
            return 0

    async def _consolidate_memories(self) -> int:
        """
        å›ºåŒ–é€»è¾‘ï¼š
        1. æ‰¾å‡ºæœ€è¿‘ 24 å°æ—¶äº§ç”Ÿçš„ã€importance=2 (Context) æˆ– 3 (Preference) çš„è®°å¿†ã€‚
        2. å¦‚æœç¢ç‰‡æ•°é‡ > 3ï¼Œå°è¯•è®© LLM æ€»ç»“ã€‚
        3. å¦‚æœæ€»ç»“æˆåŠŸï¼Œå†™å…¥ä¸€æ¡ importance=4 çš„æ–°è®°å¿†ï¼Œå¹¶åˆ é™¤æ—§ç¢ç‰‡ã€‚
        """
        try:
            # 1. è·å–æœ€è¿‘è®°å¿†
            result = vector_db.collection.get(include=["documents", "metadatas"])
            ids = result["ids"]
            docs = result["documents"]
            metadatas = result["metadatas"]

            candidates = []  # list of (id, doc, meta)
            now = datetime.now()

            # ç­›é€‰ï¼šæœ€è¿‘ 24 å°æ—¶ ä¸” é‡è¦æ€§ä¸º 2 æˆ– 3
            for i, meta in enumerate(metadatas):
                imp = meta.get("importance", 1)
                if imp not in [2, 3]:
                    continue

                c_time_str = meta.get("created_at")
                if not c_time_str: continue

                try:
                    mem_time = datetime.strptime(c_time_str, "%Y-%m-%d %H:%M:%S")
                    # åªçœ‹æœ€è¿‘ 24 å°æ—¶
                    if (now - mem_time).total_seconds() < 86400:
                        candidates.append((ids[i], docs[i]))
                except:
                    continue

            # å¦‚æœç¢ç‰‡å¤ªå°‘ï¼Œæ²¡å¿…è¦æ€»ç»“
            if len(candidates) < 4:
                return 0

            # 2. å‡†å¤‡ Prompt æ•°æ® (å–å‰ 10 æ¡å¤„ç†ï¼Œé¿å… token çˆ†ç‚¸)
            batch = candidates[:10]
            batch_texts = [item[1] for item in batch]
            batch_ids = [item[0] for item in batch]

            fragments_text = json.dumps(batch_texts, ensure_ascii=False, indent=2)

            # 3. LLM æ€è€ƒ
            logger.info(f"ğŸ§  [Dream] Attempting to consolidate {len(batch)} fragments...")

            prompt = CONSOLIDATION_PROMPT.format(fragments=fragments_text)
            response = await cached_llm_invoke(self.llm, [SystemMessage(content=prompt)],
                                               temperature=self.llm.temperature)
            result_text = response.content.strip()

            # 4. å¤„ç†ç»“æœ
            if "SKIP" in result_text or len(result_text) < 5:
                # æ— æ³•åˆå¹¶ï¼Œä¿æŒåŸæ ·
                return 0

            # 5. æ‰§è¡Œâ€œæ–°é™ˆä»£è°¢â€
            logger.info(f"âœ¨ [Dream] Consolidation Success: '{result_text}'")

            # A. å†™å…¥æ–°è®°å¿† (Importance = 4, è¡¨ç¤ºè¿™æ˜¯ç»è¿‡æ·±æ€ç†Ÿè™‘çš„äº‹å®)
            new_metadata = {
                "source": "dream_consolidation",
                "importance": 4,
                "created_at": now.strftime("%Y-%m-%d %H:%M:%S"),
                "consolidated_from_count": len(batch)
            }
            vector_db.add_texts([result_text], [new_metadata])

            # B. åˆ é™¤æ—§ç¢ç‰‡ (ç‰©ç†åˆ é™¤ï¼Œé‡Šæ”¾ç©ºé—´)
            # vector_db.collection.delete(ids=batch_ids) # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œä¸ºäº†è°ƒè¯•å®‰å…¨ã€‚ç¡®è®¤ç¨³å®šåå–æ¶ˆæ³¨é‡Šã€‚
            # è¿™é‡Œæˆ‘ä»¬åšä¸€ä¸ªæŠ˜ä¸­ï¼šä¸åˆ é™¤ï¼Œè€Œæ˜¯å°†å…¶ importance é™çº§ä¸º 0ï¼Œç­‰å¾…ä¸‹æ¬¡ Pruning æ¸…ç†
            # ä½† Chroma update æ¯”è¾ƒéº»çƒ¦ï¼Œæ‰€ä»¥ç›´æ¥åˆ é™¤æ˜¯æ¯”è¾ƒå¹²å‡€çš„åšæ³•ã€‚
            # ç”Ÿäº§ç¯å¢ƒå»ºè®®å¼€å¯åˆ é™¤ï¼š
            vector_db.collection.delete(ids=batch_ids)

            return 1

        except Exception as e:
            logger.error(f"Error in consolidation: {e}")
            return 0


# å•ä¾‹å¯¼å‡º
dream_machine = DreamCycle(interval_seconds=1800)
```

**è®°å¿†å›ºåŒ–æœºåˆ¶**ï¼š
1. **ç­›é€‰é˜¶æ®µ**ï¼šæ‰¾å‡ºæœ€è¿‘24å°æ—¶äº§ç”Ÿçš„é‡è¦æ€§ä¸º2æˆ–3çš„è®°å¿†ç¢ç‰‡
2. **æ€»ç»“é˜¶æ®µ**ï¼šä½¿ç”¨LLMå°†ç›¸å…³ç¢ç‰‡åˆå¹¶ä¸ºè¿è´¯çš„äº‹å®æè¿°
3. **å›ºåŒ–é˜¶æ®µ**ï¼šå°†æ€»ç»“ç»“æœä½œä¸ºimportance=4çš„é•¿æœŸè®°å¿†ä¿å­˜
4. **æ¸…ç†é˜¶æ®µ**ï¼šåˆ é™¤åŸå§‹è®°å¿†ç¢ç‰‡

**è®°å¿†æ¸…ç†æœºåˆ¶**ï¼š
- åˆ é™¤é‡è¦æ€§ä¸º1ä¸”è¶…è¿‡3å¤©çš„è®°å¿†
- é¿å…å½±å“ç”¨æˆ·æ´»è·ƒä¼šè¯

#### 4.4.3 å‘é‡å­˜å‚¨å®ç° (vector_store.py)

```python
import chromadb
from typing import List
from datetime import datetime
import math
import threading
from openai import OpenAI
from app.core.config import config

class VectorMemory:
    def __init__(self):
        # åˆå§‹åŒ– ChromaDB æŒä¹…åŒ–å®¢æˆ·ç«¯
        self.client = chromadb.PersistentClient(path=config.VECTOR_DB_PATH)
        self._lock = threading.Lock()  # çº¿ç¨‹å®‰å…¨é”

        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.openai_client = OpenAI(
            api_key=config.SILICONFLOW_API_KEY,
            base_url=config.SILICONFLOW_BASE_URL
        )

        # ç¡…åŸºæµåŠ¨ (SiliconFlow) åµŒå…¥å‡½æ•°é€‚é…å™¨
        class SiliconFlowAdapter:
            def __init__(self, client, model_name):
                self.client = client
                self.model_name = model_name

            def _embed(self, texts: List[str]) -> List[List[float]]:
                texts = [t.replace("\n", " ") for t in texts]
                response = self.client.embeddings.create(
                    input=texts,
                    model=self.model_name
                )
                return [data.embedding for data in response.data]

            def __call__(self, input: List[str]) -> List[List[float]]:
                return self._embed(input)

            def embed_documents(self, input: List[str]) -> List[List[float]]:
                return self._embed(input)

            def embed_query(self, input: List[str]) -> List[List[float]]:
                return self._embed(input)

        self.embedding_fn = SiliconFlowAdapter(
            self.openai_client,
            config.EMBEDDING_MODEL_NAME
        )

        # è·å–æˆ–åˆ›å»ºé›†åˆ
        self.collection = self.client.get_or_create_collection(
            name=config.COLLECTION_NAME,
            embedding_function=self.embedding_fn
        )

    def add_texts(self, texts: List[str], metadatas: List[dict] = None):
        """æ·»åŠ æ–‡æœ¬åˆ°å‘é‡æ•°æ®åº“"""
        if not texts: return

        ids = [f"mem_{hash(t)}" for t in texts]

        final_metadatas = []
        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        if metadatas:
            for m in metadatas:
                if "importance" not in m: m["importance"] = 1
                if "created_at" not in m: m["created_at"] = now_str
                final_metadatas.append(m)
        else:
            final_metadatas = [{"source": "interaction", "importance": 1, "created_at": now_str}] * len(texts)

        # åŠ é”å†™å…¥ç¡®ä¿çº¿ç¨‹å®‰å…¨
        with self._lock:
            try:
                self.collection.upsert(
                    documents=texts,
                    metadatas=final_metadatas,
                    ids=ids
                )
            except Exception as e:
                print(f"[VectorStore Write Error] {e}")

    def _calculate_time_decay(self, created_at_str: str, half_life_hours: float = 48.0) -> float:
        """è®¡ç®—æ—¶é—´è¡°å‡å› å­"""
        try:
            mem_time = datetime.strptime(created_at_str, "%Y-%m-%d %H:%M:%S")
            delta_hours = (datetime.now() - mem_time).total_seconds() / 3600.0
            # ä½¿ç”¨åŠè¡°æœŸ48å°æ—¶çš„æŒ‡æ•°è¡°å‡
            decay = max(0.3, math.pow(0.5, delta_hours / half_life_hours))
            return decay
        except:
            return 1.0  # è§£æå¤±è´¥æ—¶ä¸è¡°å‡

    def search(self, query: str, k: int = 3) -> List[str]:
        """è¯­ä¹‰æœç´¢è®°å¿†ï¼Œç»“åˆæ—¶é—´è¡°å‡å’Œé‡è¦æ€§è¯„åˆ†"""
        with self._lock:
            try:
                results = self.collection.query(
                    query_texts=[query],
                    n_results=k * 3,  # è·å–æ›´å¤šç»“æœè¿›è¡Œç²¾æ’
                    include=["documents", "metadatas", "distances"]
                )
            except Exception as e:
                print(f"[VectorStore Search Error] {e}")
                return []

        if not results["documents"]:
            return []

        docs = results["documents"][0]
        metas = results["metadatas"][0]
        dists = results["distances"][0]

        # è®¡ç®—ç»¼åˆå¾—åˆ†
        scored_candidates = []
        for doc, meta, dist in zip(docs, metas, dists):
            semantic_score = 1.0 / (1.0 + dist)  # è¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ†
            created_at = meta.get("created_at", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
            time_score = self._calculate_time_decay(created_at)  # æ—¶é—´è¡°å‡å¾—åˆ†
            importance = float(meta.get("importance", 1))  # é‡è¦æ€§
            importance_boost = 1.0 + (importance * 0.15)  # é‡è¦æ€§æå‡

            final_score = semantic_score * time_score * importance_boost
            scored_candidates.append((final_score, doc))

        # æŒ‰å¾—åˆ†æ’åºå¹¶è¿”å›å‰kä¸ªç»“æœ
        scored_candidates.sort(key=lambda x: x[0], reverse=True)
        return [item[1] for item in scored_candidates[:k]]

    def delete_by_semantic(self, query: str, threshold: float = 0.25):
        """é€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦åˆ é™¤è®°å¿†"""
        with self._lock:
            try:
                results = self.collection.query(
                    query_texts=[query],
                    n_results=5
                )
                ids_to_delete = []
                if results["ids"]:
                    for id_val, dist in zip(results["ids"][0], results["distances"][0]):
                        if dist < threshold:  # è·ç¦»å°äºé˜ˆå€¼è¡¨ç¤ºç›¸ä¼¼åº¦é«˜
                            ids_to_delete.append(id_val)

                if ids_to_delete:
                    self.collection.delete(ids=ids_to_delete)
                    return len(ids_to_delete)
                return 0
            except Exception as e:
                print(f"[VectorStore Delete Error] {e}")
                return 0


# å•ä¾‹å¯¼å‡º
vector_db = VectorMemory()
```

**å‘é‡è®°å¿†ç³»ç»Ÿè®¾è®¡ç‰¹ç‚¹**ï¼š
1. **çº¿ç¨‹å®‰å…¨**ï¼šä½¿ç”¨äº’æ–¥é”ç¡®ä¿å¤šçº¿ç¨‹ç¯å¢ƒä¸‹çš„å®‰å…¨æ“ä½œ
2. **è‡ªé€‚åº”åµŒå…¥**ï¼šé€šè¿‡é€‚é…å™¨æ”¯æŒå¤šç§åµŒå…¥æ¨¡å‹å’ŒæœåŠ¡
3. **ç»¼åˆè¯„åˆ†æœºåˆ¶**ï¼šç»“åˆè¯­ä¹‰ç›¸ä¼¼åº¦ã€æ—¶é—´è¡°å‡å’Œé‡è¦æ€§è¿›è¡Œè®°å¿†æ£€ç´¢
4. **æŒä¹…åŒ–å­˜å‚¨**ï¼šä½¿ç”¨ ChromaDB çš„æŒä¹…åŒ–å®¢æˆ·ç«¯ç¡®ä¿æ•°æ®ä¸ä¼šä¸¢å¤±
5. **çµæ´»çš„å…ƒæ•°æ®ç®¡ç†**ï¼šæ”¯æŒè‡ªå®šä¹‰å…ƒæ•°æ®å’Œè‡ªåŠ¨æ·»åŠ æ—¶é—´æˆ³
6. **æ™ºèƒ½æ¸…ç†**ï¼šæ”¯æŒé€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦åˆ é™¤ç›¸å…³è®°å¿†

**è®°å¿†è¯„åˆ†æœºåˆ¶**ï¼š
- **è¯­ä¹‰ç›¸ä¼¼åº¦ (semantic_score)**ï¼šåŸºäºå‘é‡ç©ºé—´è·ç¦»è®¡ç®—ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºè¶Šç›¸ä¼¼
- **æ—¶é—´è¡°å‡ (time_score)**ï¼šä½¿ç”¨åŠè¡°æœŸ48å°æ—¶çš„æŒ‡æ•°è¡°å‡ï¼Œæ—§è®°å¿†æƒé‡é€æ¸é™ä½
- **é‡è¦æ€§æå‡ (importance_boost)**ï¼šæ ¹æ®è®°å¿†çš„é‡è¦æ€§ç­‰çº§ç»™äºˆé¢å¤–æƒé‡æå‡
- **ç»¼åˆå¾—åˆ†**ï¼šä¸‰è€…çš„ä¹˜ç§¯ï¼Œç»¼åˆè€ƒè™‘äº†è®°å¿†çš„ç›¸å…³æ€§ã€æ—¶æ•ˆæ€§å’Œé‡è¦æ€§

#### 4.4.4 æœ¬åœ°å†å²ç®¡ç† (local_history.py)
è´Ÿè´£ä¼šè¯å†å²çš„æŒä¹…åŒ–å­˜å‚¨ï¼Œä½¿ç”¨æ•°æ®åº“è¿›è¡Œæ•°æ®ç®¡ç†ï¼Œå¹¶æ”¯æŒä»æ—§çš„JSONæ ¼å¼è¿ç§»æ•°æ®ï¼š

```python
import os
import json
import aiofiles
from typing import List, Tuple, Dict, Any
from langchain_core.messages import BaseMessage, messages_to_dict, messages_from_dict
from sqlalchemy.orm import Session
from app.core.database import SessionLocal, SessionHistoryModel

# å®šä¹‰å­˜å‚¨è·¯å¾„ï¼ˆç”¨äºè¿ç§»æ—§æ•°æ®ï¼‰
HISTORY_DIR = "./data/history"


class LocalHistoryManager:
    """
    è´Ÿè´£ä¼šè¯å†å²å­˜å‚¨ï¼Œä½¿ç”¨æ•°æ®åº“ã€‚
    æ ¹æ® session_id è¿›è¡Œæ•°æ®éš”ç¦»ã€‚
    """

    @staticmethod
    def _get_db() -> Session:
        """è·å–æ•°æ®åº“ä¼šè¯"""
        return SessionLocal()

    @classmethod
    async def save_state(cls, messages: List[BaseMessage], summary: str, session_id: str):
        """
        å¼‚æ­¥ä¿å­˜å½“å‰å¯¹è¯çŠ¶æ€åˆ°æ•°æ®åº“ã€‚
        :param messages: LangChain æ¶ˆæ¯åˆ—è¡¨
        :param summary: å½“å‰çš„å¯¹è¯æ€»ç»“
        :param session_id: ä¼šè¯å”¯ä¸€æ ‡è¯† (private_xxx æˆ– group_xxx)
        """
        if not session_id:
            print("âš ï¸ [History] Cannot save: session_id is missing.")
            return

        # å°†æ¶ˆæ¯å¯¹è±¡åºåˆ—åŒ–ä¸ºJSONå­—ç¬¦ä¸²
        serialized_msgs = json.dumps(messages_to_dict(messages), ensure_ascii=False)

        try:
            db = cls._get_db()
            
            # æŸ¥æ‰¾ç°æœ‰è®°å½•
            history = db.query(SessionHistoryModel).filter_by(session_id=session_id).first()
            
            if history:
                # æ›´æ–°ç°æœ‰è®°å½•
                history.summary = summary
                history.messages = serialized_msgs
            else:
                # åˆ›å»ºæ–°è®°å½•
                history = SessionHistoryModel(
                    session_id=session_id,
                    summary=summary,
                    messages=serialized_msgs
                )
                db.add(history)
            
            db.commit()
            db.close()
        except Exception as e:
            print(f"âŒ [History] Save failed for {session_id}: {e}")
            try:
                db.rollback()
            except:
                pass
            finally:
                db.close()

    @classmethod
    async def load_state(cls, session_id: str) -> Tuple[List[BaseMessage], str]:
        """
        å¼‚æ­¥è¯»å–ä¼šè¯çŠ¶æ€ã€‚
        :param session_id: ä¼šè¯å”¯ä¸€æ ‡è¯†
        :return: (messages, summary)
        """
        if not session_id:
            return [], ""

        try:
            db = cls._get_db()
            
            # æŸ¥æ‰¾è®°å½•
            history = db.query(SessionHistoryModel).filter_by(session_id=session_id).first()
            
            if not history:
                # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æ—§çš„JSONæ–‡ä»¶ä¸­è¿ç§»
                await cls._migrate_from_json(session_id)
                # å†æ¬¡æŸ¥è¯¢
                history = db.query(SessionHistoryModel).filter_by(session_id=session_id).first()
                
            if not history:
                return [], ""
            
            # ååºåˆ—åŒ–æ¶ˆæ¯
            msgs_dict = json.loads(history.messages)
            messages = messages_from_dict(msgs_dict)
            
            db.close()
            return messages, history.summary

        except Exception as e:
            print(f"âŒ [History] Load failed for {session_id}: {e}")
            try:
                db.close()
            except:
                pass
            return [], ""
    
    @classmethod
    def get_existing_summary_sync(cls, session_id: str) -> str:
        """
        åŒæ­¥è¾…åŠ©æ–¹æ³•ï¼šä»…è·å– Summary (ç”¨äºåˆå§‹åŒ–æ—¶å¿«é€Ÿè¯»å–)
        """
        if not session_id: return ""

        try:
            db = cls._get_db()
            
            # æŸ¥æ‰¾è®°å½•
            history = db.query(SessionHistoryModel).filter_by(session_id=session_id).first()
            
            if not history:
                # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æ—§çš„JSONæ–‡ä»¶ä¸­è¿ç§»
                import asyncio
                asyncio.run(cls._migrate_from_json(session_id))
                # å†æ¬¡æŸ¥è¯¢
                history = db.query(SessionHistoryModel).filter_by(session_id=session_id).first()
                
            db.close()
            return history.summary if history else ""
        except Exception as e:
            print(f"âŒ [History] Get summary failed for {session_id}: {e}")
            try:
                db.close()
            except:
                pass
            return ""
    
    @classmethod
    async def _migrate_from_json(cls, session_id: str):
        """
        ä»æ—§çš„JSONæ–‡ä»¶è¿ç§»æ•°æ®åˆ°æ•°æ®åº“
        """
        if not os.path.exists(HISTORY_DIR):
            return
        
        # è·å–æ—§æ–‡ä»¶è·¯å¾„
        safe_id = "".join([c for c in session_id if c.isalnum() or c in ('_', '-')])
        file_path = os.path.join(HISTORY_DIR, f"{safe_id}.json")
        
        if not os.path.exists(file_path):
            return
        
        try:
            async with aiofiles.open(file_path, mode='r', encoding='utf-8') as f:
                content = await f.read()
                if not content:
                    return
                
                data = json.loads(content)
                summary = data.get("summary", "")
                msgs_dict = data.get("messages", [])
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                db = cls._get_db()
                
                # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨
                existing = db.query(SessionHistoryModel).filter_by(session_id=session_id).first()
                if not existing:
                    history = SessionHistoryModel(
                        session_id=session_id,
                        summary=summary,
                        messages=json.dumps(msgs_dict, ensure_ascii=False)
                    )
                    db.add(history)
                    db.commit()
                    print(f"âœ… [History] Migrated {session_id} from JSON to database")
                
                db.close()
        except Exception as e:
            print(f"âŒ [History] Migration failed for {session_id}: {e}")
            try:
                db = cls._get_db()
                db.rollback()
                db.close()
            except:
                pass
```

**æœ¬åœ°å†å²ç®¡ç†è®¾è®¡ç‰¹ç‚¹**ï¼š
1. **ä¼šè¯éš”ç¦»**ï¼šä½¿ç”¨ `session_id` å¯¹ä¸åŒä¼šè¯çš„å†å²è¿›è¡Œå®Œå…¨éš”ç¦»
2. **å¼‚æ­¥æ“ä½œ**ï¼šæ”¯æŒå¼‚æ­¥çš„ä¿å­˜å’ŒåŠ è½½æ“ä½œï¼Œæé«˜å¹¶å‘æ€§èƒ½
3. **æ•°æ®è¿ç§»**ï¼šè‡ªåŠ¨ä»æ—§çš„JSONæ–‡ä»¶æ ¼å¼è¿ç§»åˆ°æ•°æ®åº“å­˜å‚¨ï¼Œç¡®ä¿æ•°æ®å…¼å®¹æ€§
4. **å¥å£®æ€§è®¾è®¡**ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†å’Œèµ„æºç®¡ç†ï¼Œç¡®ä¿æ•°æ®åº“è¿æ¥æ­£ç¡®å…³é—­
5. **åŒæ­¥è¾…åŠ©æ–¹æ³•**ï¼šæä¾›åŒæ­¥çš„ `get_existing_summary_sync` æ–¹æ³•ï¼Œæ–¹ä¾¿åˆå§‹åŒ–æ—¶å¿«é€Ÿè¯»å–
6. **å®‰å…¨çš„æ–‡ä»¶æ“ä½œ**ï¼šä½¿ç”¨å¼‚æ­¥æ–‡ä»¶IOå’Œå®‰å…¨çš„å­—ç¬¦ä¸²å¤„ç†ç¡®ä¿æ•°æ®å®‰å…¨

#### 4.4.5 å…³ç³»æ•°æ®åº“ç®¡ç† (relation_db.py)
è´Ÿè´£ç”¨æˆ·å…³ç³»å’Œä¸ªäººèµ„æ–™çš„æŒä¹…åŒ–å­˜å‚¨ï¼Œæ”¯æŒç”¨æˆ·å…³ç³»ç®¡ç†å’Œæ•°æ®è¿ç§»ï¼š

```python
# === ç”¨æˆ·å…³ç³»æ•°æ®åº“ç®¡ç† ===

import json
import os
import asyncio
import logging
import time
from typing import Dict, Any, List, Union, Optional
from pydantic import BaseModel, Field
from sqlalchemy import Column, Integer, String, Text, JSON
from sqlalchemy.orm import Session
from sqlalchemy.exc import SQLAlchemyError

# å¯¼å…¥æ•°æ®åº“é…ç½®
from app.core.database import Base, engine, SessionLocal, init_db

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# JSONæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæ•°æ®è¿ç§»ï¼‰
OLD_JSON_DB = "data/user_profiles.json"
# è¿ç§»å®Œæˆæ ‡è®°æ–‡ä»¶
MIGRATION_COMPLETE_FILE = "data/migration_complete.txt"


class Relationship(BaseModel):
    target_id: str
    relation_type: str = "acquaintance"
    intimacy: int = Field(default=60, ge=0, le=100)
    tags: List[str] = Field(default_factory=list)
    notes: str = ""
    nickname_for_user: str = ""


class UserProfile(BaseModel):
    name: str
    qq_id: str = ""
    relationship: Relationship


# æ•°æ®åº“æ¨¡å‹
class UserProfileModel(Base):
    __tablename__ = "user_profiles"
    
    qq_id = Column(String(50), primary_key=True, index=True)
    name = Column(String(255), nullable=False)
    relationship_data = Column(JSON, nullable=False)  # å­˜å‚¨Relationshipå¯¹è±¡çš„JSONæ•°æ®
    updated_at = Column(String(50), default=lambda: str(time.time()))


class GlobalRelationDB:
    def __init__(self):
        # åˆå§‹åŒ–æ•°æ®åº“
        init_db()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä»JSONè¿ç§»æ•°æ®
        self._migrate_from_json()

    def _migrate_from_json(self):
        """ä»æ—§çš„JSONæ–‡ä»¶è¿ç§»æ•°æ®åˆ°æ•°æ®åº“"""
        # æ£€æŸ¥è¿ç§»æ˜¯å¦å·²ç»å®Œæˆ
        if os.path.exists(MIGRATION_COMPLETE_FILE):
            logger.info("[RelationDB] æ•°æ®è¿ç§»å·²ç»å®Œæˆï¼Œè·³è¿‡")
            return
            
        if not os.path.exists(OLD_JSON_DB):
            logger.info("[RelationDB] æ²¡æœ‰å‘ç°æ—§çš„JSONæ•°æ®åº“æ–‡ä»¶ï¼Œè·³è¿‡è¿ç§»")
            return
            
        try:
            with open(OLD_JSON_DB, "r", encoding="utf-8") as f:
                old_data = json.load(f)
                
            if not old_data:
                logger.info("[RelationDB] æ—§çš„JSONæ•°æ®åº“æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡è¿ç§»")
                return
                
            db = SessionLocal()
            migrated_count = 0
            
            try:
                for user_qq, profile_data in old_data.items():
                    # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²ç»å­˜åœ¨
                    existing = db.query(UserProfileModel).filter(UserProfileModel.qq_id == user_qq).first()
                    if existing:
                        continue
                        
                    # æ„å»ºæ–°çš„æ•°æ®åº“è®°å½•
                    user_profile = UserProfileModel(
                        qq_id=str(user_qq),
                        name=profile_data.get("name", f"User_{user_qq}"),
                        relationship_data=profile_data.get("relationship", {})
                    )
                    db.add(user_profile)
                    migrated_count += 1
                    
                db.commit()
                logger.info(f"[RelationDB] æˆåŠŸä»JSONè¿ç§»äº† {migrated_count} æ¡ç”¨æˆ·æ•°æ®åˆ°æ•°æ®åº“")
                
            except SQLAlchemyError as e:
                db.rollback()
                logger.error(f"[RelationDB] æ•°æ®è¿ç§»å¤±è´¥: {str(e)}")
            finally:
                db.close()
                
                # æ— è®ºæ˜¯å¦è¿ç§»æ•°æ®ï¼Œéƒ½åˆ›å»ºè¿ç§»å®Œæˆæ ‡è®°
                try:
                    with open(MIGRATION_COMPLETE_FILE, "w") as f:
                        f.write("Migration completed at " + time.strftime("%Y-%m-%d %H:%M:%S"))
                except Exception as e:
                    logger.error(f"[RelationDB] åˆ›å»ºè¿ç§»æ ‡è®°æ–‡ä»¶å¤±è´¥: {str(e)}")
                    
        except Exception as e:
            logger.error(f"[RelationDB] è¯»å–æ—§JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
            
            # å³ä½¿è¯»å–å¤±è´¥ï¼Œä¹Ÿåˆ›å»ºè¿ç§»æ ‡è®°é¿å…é‡å¤å°è¯•
            try:
                with open(MIGRATION_COMPLETE_FILE, "w") as f:
                    f.write("Migration completed at " + time.strftime("%Y-%m-%d %H:%M:%S") + " (with errors)")
            except Exception as create_e:
                logger.error(f"[RelationDB] åˆ›å»ºè¿ç§»æ ‡è®°æ–‡ä»¶å¤±è´¥: {str(create_e)}")

    def get_user_profile(self, user_qq: str, current_name: str = None) -> UserProfile:
        """è·å–ç”¨æˆ·ä¸ªäººèµ„æ–™"""
        user_qq = str(user_qq)
        db = SessionLocal()
        
        try:
            # æŸ¥è¯¢ç”¨æˆ·
            db_profile = db.query(UserProfileModel).filter(UserProfileModel.qq_id == user_qq).first()
            
            if db_profile:
                # ä»æ•°æ®åº“è®°å½•æ„å»ºUserProfileå¯¹è±¡
                relationship_data = db_profile.relationship_data
                if not relationship_data:
                    relationship_data = {"target_id": user_qq}
                
                profile = UserProfile(
                    name=db_profile.name,
                    qq_id=db_profile.qq_id,
                    relationship=Relationship(**relationship_data)
                )
                
                # æ›´æ–°ç”¨æˆ·å
                if current_name and profile.name != current_name:
                    db_profile.name = current_name
                    db_profile.updated_at = str(time.time())
                    db.commit()
                    profile.name = current_name
                
                return profile
            else:
                # åˆ›å»ºæ–°ç”¨æˆ·
                display_name = current_name if current_name else f"User_{user_qq}"
                relationship = Relationship(target_id=user_qq)
                
                new_db_profile = UserProfileModel(
                    qq_id=user_qq,
                    name=display_name,
                    relationship_data=relationship.model_dump()
                )
                
                db.add(new_db_profile)
                db.commit()
                
                return UserProfile(
                    name=display_name,
                    qq_id=user_qq,
                    relationship=relationship
                )
                
        except SQLAlchemyError as e:
            db.rollback()
            logger.error(f"[RelationDB] è·å–ç”¨æˆ·èµ„æ–™å¤±è´¥: {str(e)}")
            # å‡ºé”™æ—¶è¿”å›é»˜è®¤å€¼
            display_name = current_name if current_name else f"User_{user_qq}"
            return UserProfile(
                name=display_name,
                qq_id=user_qq,
                relationship=Relationship(target_id=user_qq)
            )
        finally:
            db.close()

    def update_intimacy(self, user_qq: str, delta: int):
        """æ›´æ–°ç”¨æˆ·äº²å¯†åº¦"""
        user_qq = str(user_qq)
        db = SessionLocal()
        
        try:
            profile = db.query(UserProfileModel).filter(UserProfileModel.qq_id == user_qq).first()
            
            if profile:
                relationship_data = profile.relationship_data
                if not relationship_data:
                    relationship_data = {"target_id": user_qq, "intimacy": 60}
                
                # æ›´æ–°äº²å¯†åº¦
                current_intimacy = relationship_data.get("intimacy", 60)
                new_intimacy = max(0, min(100, current_intimacy + delta))
                relationship_data["intimacy"] = new_intimacy
                
                profile.relationship_data = relationship_data
                profile.updated_at = str(time.time())
                db.commit()
                
                return new_intimacy
            else:
                # ç”¨æˆ·ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ç”¨æˆ·
                relationship = Relationship(target_id=user_qq, intimacy=60 + delta)
                new_profile = UserProfileModel(
                    qq_id=user_qq,
                    name=f"User_{user_qq}",
                    relationship_data=relationship.model_dump()
                )
                
                db.add(new_profile)
                db.commit()
                
                return relationship.intimacy
                
        except SQLAlchemyError as e:
            db.rollback()
            logger.error(f"[RelationDB] æ›´æ–°äº²å¯†åº¦å¤±è´¥: {str(e)}")
            return 60  # å‡ºé”™æ—¶è¿”å›é»˜è®¤å€¼
        finally:
            db.close()

    def update_relationship(self, user_qq: str, target_id: str, new_data: Relationship):
        """æ›´æ–°ç”¨æˆ·å…³ç³»æ•°æ®"""
        user_qq = str(user_qq)
        db = SessionLocal()
        
        try:
            profile = db.query(UserProfileModel).filter(UserProfileModel.qq_id == user_qq).first()
            
            if profile:
                profile.relationship_data = new_data.model_dump()
                profile.updated_at = str(time.time())
                db.commit()
                return True
            else:
                # ç”¨æˆ·ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ç”¨æˆ·
                new_profile = UserProfileModel(
                    qq_id=user_qq,
                    name=f"User_{user_qq}",
                    relationship_data=new_data.model_dump()
                )
                
                db.add(new_profile)
                db.commit()
                return True
                
        except SQLAlchemyError as e:
            db.rollback()
            logger.error(f"[RelationDB] æ›´æ–°å…³ç³»å¤±è´¥: {str(e)}")
            return False
        finally:
            db.close()


# åˆ›å»ºå…¨å±€å®ä¾‹
relation_db = GlobalRelationDB()
```

**å…³ç³»æ•°æ®åº“ç®¡ç†è®¾è®¡ç‰¹ç‚¹**ï¼š
1. **ç”¨æˆ·å…³ç³»å»ºæ¨¡**ï¼šä½¿ç”¨ `Relationship` æ¨¡å‹å®šä¹‰ç”¨æˆ·é—´çš„å…³ç³»ç±»å‹ã€äº²å¯†åº¦ç­‰å±æ€§
2. **ä¸ªäººèµ„æ–™ç®¡ç†**ï¼šé€šè¿‡ `UserProfile` æ¨¡å‹ç®¡ç†ç”¨æˆ·çš„ä¸ªäººä¿¡æ¯
3. **æ•°æ®è¿ç§»æ”¯æŒ**ï¼šè‡ªåŠ¨ä»æ—§çš„JSONæ ¼å¼è¿ç§»åˆ°æ•°æ®åº“å­˜å‚¨
4. **æ•°æ®å®Œæ•´æ€§**ï¼šä½¿ç”¨ Pydantic æ¨¡å‹ç¡®ä¿æ•°æ®ç»“æ„çš„å®Œæ•´æ€§
5. **å¼‚å¸¸å¤„ç†**ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§
6. **å…¨å±€è®¿é—®ç‚¹**ï¼šé€šè¿‡ `relation_db` å…¨å±€å®ä¾‹æä¾›ç»Ÿä¸€çš„è®¿é—®æ¥å£
7. **å®‰å…¨çš„äº²å¯†åº¦æ›´æ–°**ï¼šç¡®ä¿äº²å¯†åº¦å€¼åœ¨0-100çš„åˆç†èŒƒå›´å†…

### 4.5 QQæœåŠ¡å™¨ (qq_server.py)
å¤„ç† QQ æ¶ˆæ¯çš„æ¥æ”¶ä¸å‘é€ï¼š

```python
class QQBotManager:
    def __init__(self):
        self.connections: dict[str, WebSocket] = {}  # WebSocket è¿æ¥ç®¡ç†
        self.graph = build_graph()  # åˆå§‹åŒ–å·¥ä½œæµ
        self.msg_buffer = MessageBuffer(wait_time=1.5)  # æ¶ˆæ¯ç¼“å†²åŒº
    
    async def process_batch(self, session_id: str, raw_messages: list):
        # è§£ææ¶ˆæ¯æ‰¹æ¬¡
        full_text, image_urls, is_mentioned = await self._parse_message_batch(raw_messages, self_id)
        
        # æ„å»ºè¾“å…¥å‚æ•°
        inputs = await self._build_reactive_inputs(
            session_id=session_id,
            full_text=full_text,
            image_urls=image_urls,
            user_qq=user_qq,
            user_nickname=user_nickname,
            msg_type=msg_type,
            is_mentioned=is_mentioned
        )
        
        # å¤„ç†å·¥ä½œæµè¾“å‡º
        await self.handle_graph_output(inputs, self_id, msg_type, group_id, user_qq)
    
    async def send_msg(self, self_id: str, target_type: str, target_id: int, message: str):
        # å‘é€æ¶ˆæ¯åˆ° QQ å¹³å°
        payload = {
            "action": "send_msg",
            "params": {
                "message_type": target_type,
                "user_id": target_id if target_type == 'private' else None,
                "group_id": target_id if target_type == 'group' else None,
                "message": message
            }
        }
        await self.connections[self_id].send_json(payload)
```

### 4.5.1 ä¸»å…¥å£æ–‡ä»¶ (main.py)
ProjectAliceçš„æ ¸å¿ƒå…¥å£æ–‡ä»¶ï¼Œè´Ÿè´£ç³»ç»Ÿåˆå§‹åŒ–ã€äº‹ä»¶å¤„ç†å’Œå·¥ä½œæµæ‰§è¡Œï¼š

```python
import asyncio
import os
import sys
import threading
import time
from typing import Dict, Any

from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown
from rich.live import Live
from langchain_core.messages import HumanMessage, AIMessage

from app.graph.graph_builder import build_graph
from app.core.state import AgentState, EmotionData

from app.memory.relation_db import relation_db
from app.monitor.screen_monitor import ScreenMonitor

console = Console()


class UserInputEvent:
    def __init__(self, text: str):
        self.text = text


def print_agent_thought(monologue: str, emotion: EmotionData):
    if not monologue or monologue == "N/A": return
    console.print(f"[dim italic]Anima æ€è€ƒ: {monologue} (Mood: {emotion.current_mood})[/dim]")


async def main():
    console.clear()
    console.rule("[bold magenta]Project Anima â€“ High Performance[/bold magenta]")

    user_id = console.input("[bold yellow]User ID: [/] ").strip() or "master"

    # --- çŠ¶æ€åˆå§‹åŒ– ---
    app_state: Dict[str, Any] = {
        "current_user_id": user_id,
        "user_profile": relation_db.get_user_profile(user_id),
        "global_relationship_graph": relation_db.get_all_relationships(),
        "emotion": EmotionData(current_mood="Calm", valence=0.1, arousal=0.4),
        "messages": [],
        "current_activity": "Idle",
        "activity_start_time": time.time(),
        "last_visual_summary": "",
        "last_interaction_ts": 0.0
    }

    graph = build_graph()
    event_queue = asyncio.Queue()
    loop = asyncio.get_running_loop()

    # --- 1. å¯åŠ¨ç›‘æ§ (ä¿®æ­£å‚æ•°) ---
    # interval=1.0: æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
    # stability_duration=2.0: ç”»é¢å¿…é¡»é™æ­¢2ç§’æ‰è§¦å‘ï¼Œé˜²æ­¢çœ‹è§†é¢‘æ—¶ç–¯ç‹‚è§¦å‘
    monitor = ScreenMonitor(event_queue, interval=1.0, diff_threshold=5.0, stability_duration=2.0)

    # --- 2. è¾“å…¥çº¿ç¨‹ ---
    def user_input_worker():
        while True:
            try:
                text = input()  # é˜»å¡å¼è¾“å…¥
                if not text: continue
                if text.lower() in ["quit", "exit"]:
                    os._exit(0)

                asyncio.run_coroutine_threadsafe(event_queue.put(UserInputEvent(text)), loop)
            except:
                break

    threading.Thread(target=user_input_worker, daemon=True).start()
    monitor.start()

    console.print(f"[green]ç³»ç»Ÿå·²å°±ç»ªã€‚Anima æ­£åœ¨æ½œä¼...[/green]")

    try:
        while True:
            # --- 3. æ™ºèƒ½äº‹ä»¶è·å–ä¸å»é‡ ---
            # ä¼˜å…ˆå¤„ç†ç”¨æˆ·è¾“å…¥ï¼›å¦‚æœé˜Ÿåˆ—é‡Œæœ‰å¤šä¸ªå±å¹•äº‹ä»¶ï¼Œåªå–æœ€æ–°çš„ä¸€ä¸ªï¼Œä¸¢å¼ƒæ—§çš„ï¼
            events = []
            try:
                # é˜»å¡ç­‰å¾…ç¬¬ä¸€ä¸ªäº‹ä»¶
                events.append(await event_queue.get())

                # æ£€æŸ¥é˜Ÿåˆ—é‡Œè¿˜æœ‰æ²¡æœ‰ç§¯å‹çš„ï¼Ÿ
                while not event_queue.empty():
                    evt = event_queue.get_nowait()
                    # å¦‚æœæ˜¯ç”¨æˆ·è¾“å…¥ï¼Œå¿…é¡»ä¿ç•™
                    if isinstance(evt, UserInputEvent):
                        events.append(evt)
                    # å¦‚æœæ˜¯å±å¹•äº‹ä»¶ï¼Œä¸”åˆ—è¡¨ä¸­å·²ç»æœ‰ä¸€ä¸ªå±å¹•äº‹ä»¶äº†ï¼Œè¦†ç›–å®ƒï¼ˆåªä¿ç•™æœ€æ–°çš„ï¼‰
                    elif isinstance(evt, dict) and evt.get('type') == 'screen_event':
                        # ç§»é™¤åˆ—è¡¨ä¸­å·²æœ‰çš„æ—§å±å¹•äº‹ä»¶
                        events = [e for e in events if not (isinstance(e, dict) and e.get('type') == 'screen_event')]
                        events.append(evt)
                    event_queue.task_done()
            except Exception:
                pass

            # é€ä¸ªå¤„ç†å»é‡åçš„äº‹ä»¶
            for event in events:
                inputs = {**app_state}  # æµ…æ‹·è´å½“å‰çŠ¶æ€
                inputs["visual_input"] = None
                inputs["is_proactive"] = False

                if isinstance(event, UserInputEvent):
                    console.print(f"\n[bold white]You:[/bold white] {event.text}")
                    inputs["messages"] = app_state["messages"] + [HumanMessage(content=event.text)]

                elif isinstance(event, dict) and event.get('type') == 'screen_event':
                    # è§†è§‰äº‹ä»¶è§¦å‘
                    # åªæœ‰å½“è·ç¦»ä¸Šæ¬¡äº¤äº’è¶…è¿‡ä¸€å®šæ—¶é—´ï¼Œæ‰å…è®¸è§†è§‰è§¦å‘ä¸»åŠ¨äº¤äº’
                    if time.time() - app_state["last_interaction_ts"] < 10:
                        # console.print("[dim]å†·å´ä¸­ï¼Œå¿½ç•¥è§†è§‰å˜åŒ–[/dim]")
                        continue

                    inputs["visual_input"] = event['data']
                    inputs["is_proactive"] = True
                    console.print("[dim]>> æ•æ‰åˆ°å±å¹•å˜åŒ–ï¼ŒAnima æ­£åœ¨è§‚å¯Ÿ...[/dim]")

                # --- 4. æ‰§è¡Œ Graph ---
                # ä½¿ç”¨ stream æ¨¡å¼
                async for output in graph.astream(inputs):
                    for node_name, node_val in output.items():
                        # æ›´æ–°å…¨å±€çŠ¶æ€
                        if "messages" in node_val:
                            app_state["messages"] = node_val["messages"]
                        if "emotion" in node_val:
                            app_state["emotion"] = node_val["emotion"]
                        if "current_activity" in node_val:
                            app_state["current_activity"] = node_val["current_activity"]
                        if "last_visual_summary" in node_val:
                            app_state["last_visual_summary"] = node_val["last_visual_summary"]

                        # UI åé¦ˆ
                        if node_name == "reasoning":
                            print_agent_thought(node_val.get("internal_monologue"), app_state["emotion"])

                        elif node_name == "response":
                            # æ‰“å°å›å¤
                            last_msg = node_val["messages"][-1]
                            console.print(Panel(last_msg.content, title="Anima", border_style="cyan"))
                            app_state["last_interaction_ts"] = time.time()

                        elif node_name == "proactive":
                            # å¦‚æœä¸»åŠ¨å†³ç­–å†³å®šä¸è¯´è¯ï¼Œæ‰“å°åŸå› 
                            if not node_val.get("should_speak", True):
                                # console.print("[dim]Anima å†³å®šä¿æŒæ²‰é»˜[/dim]")
                                pass

                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                if isinstance(event, UserInputEvent) or (isinstance(event, dict)):
                    pass  # è¿™é‡Œçš„ task_done éœ€è¦å’Œ get æ¬¡æ•°å¯¹åº”ï¼Œä¸Šé¢é€»è¾‘å·²ç®€åŒ–ï¼Œå¯å¿½ç•¥

    except KeyboardInterrupt:
        console.print("\n[yellow]Bye![/yellow]")
    finally:
        monitor.stop()


if __name__ == "__main__":
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
```

#### 4.5.2 ä¸»è¦åŠŸèƒ½

1. **ç³»ç»Ÿåˆå§‹åŒ–**ï¼šé…ç½®ç”¨æˆ·IDã€åˆå§‹åŒ–åº”ç”¨çŠ¶æ€å’ŒåŠ è½½ç”¨æˆ·å…³ç³»æ•°æ®
2. **å±å¹•ç›‘æ§**ï¼šå®æ—¶ç›‘æ§å±å¹•å†…å®¹å˜åŒ–ï¼Œæ”¯æŒæ™ºèƒ½å»é‡å’Œç¨³å®šæ€§æ£€æµ‹
3. **ç”¨æˆ·è¾“å…¥å¤„ç†**ï¼šé€šè¿‡ç‹¬ç«‹çº¿ç¨‹å¤„ç†ç”¨æˆ·çš„æ§åˆ¶å°è¾“å…¥
4. **äº‹ä»¶é˜Ÿåˆ—ç®¡ç†**ï¼šå®ç°ç”¨æˆ·è¾“å…¥å’Œå±å¹•äº‹ä»¶çš„ç»Ÿä¸€é˜Ÿåˆ—ç®¡ç†å’Œå»é‡
5. **å·¥ä½œæµæ‰§è¡Œ**ï¼šä½¿ç”¨LangGraphæ„å»ºçš„å·¥ä½œæµå¤„ç†å„ç§äº‹ä»¶ï¼Œæ”¯æŒæµå¼è¾“å‡º
6. **çŠ¶æ€æ›´æ–°**ï¼šå®æ—¶æ›´æ–°åº”ç”¨çŠ¶æ€ã€æƒ…ç»ªæ•°æ®å’Œäº¤äº’å†å²
7. **UIåé¦ˆ**ï¼šé€šè¿‡richåº“æä¾›ç¾è§‚çš„æ§åˆ¶å°è¾“å‡ºå’ŒçŠ¶æ€åé¦ˆ

### 4.6 å®‰å…¨æ¨¡å— (app/utils/safety.py)
æä¾›å†…å®¹å®‰å…¨è¿‡æ»¤åŠŸèƒ½ï¼Œå½“å‰ç‰ˆæœ¬å·²ç¦ç”¨æ‰€æœ‰æ£€æŸ¥ï¼š

```python
from typing import Tuple, Optional

class SafetyFilter:
    """
    å®‰å…¨è¿‡æ»¤å™¨ - å·²ç¦ç”¨æ‰€æœ‰åŠŸèƒ½
    æ‰€æœ‰æ£€æŸ¥éƒ½å°†é€šè¿‡
    """
    def __init__(self):
        pass

    def check_input(self, text: str) -> Tuple[bool, str]:
        """
        æ£€æŸ¥è¾“å…¥æ˜¯å¦å®‰å…¨ã€‚
        å·²ç¦ç”¨ï¼šå§‹ç»ˆè¿”å›å®‰å…¨é€šè¿‡
        Returns: (is_safe: bool, reason: str)
        """
        return True, ""
        
    def get_refusal_response(self) -> str:
        """
        è¿”å›æ ‡å‡†çš„æ‹’ç»è¯æœ¯ã€‚
        å·²ç¦ç”¨ï¼šè¿”å›ç©ºå­—ç¬¦ä¸²
        """
        return ""

safety_filter = SafetyFilter()
```

### 4.6.1 ç¼“å­˜ç®¡ç†æ¨¡å— (app/utils/cache.py)
å®ç°äº†LLMè°ƒç”¨çš„ç¼“å­˜å’Œè¯·æ±‚é˜Ÿåˆ—ç³»ç»Ÿï¼Œæé«˜æ€§èƒ½å¹¶æ§åˆ¶å¹¶å‘ï¼š

```python
import asyncio
import hashlib
import json
import logging
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple, Union, Deque
from collections import deque
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLMCache:
    """
    LLMè°ƒç”¨ç¼“å­˜ç³»ç»Ÿ
    ç”¨äºç¼“å­˜LLMè°ƒç”¨çš„è¯·æ±‚å’Œå“åº”ï¼Œå‡å°‘é‡å¤è°ƒç”¨ï¼Œæé«˜æ€§èƒ½
    """
    
    def __init__(self, max_size: int = 1000, default_ttl: int = 3600):
        """
        åˆå§‹åŒ–ç¼“å­˜ç³»ç»Ÿ
        
        Args:
            max_size: ç¼“å­˜çš„æœ€å¤§æ¡ç›®æ•°
            default_ttl: é»˜è®¤çš„ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.cache: Dict[str, Tuple[Any, datetime]] = {}
        self.lock = asyncio.Lock()
    
    def _generate_key(self, messages: List[BaseMessage], model: str, temperature: float) -> str:
        """
        æ ¹æ®è¾“å…¥æ¶ˆæ¯ç”Ÿæˆå”¯ä¸€çš„ç¼“å­˜é”®
        
        Args:
            messages: LLMè°ƒç”¨çš„è¾“å…¥æ¶ˆæ¯åˆ—è¡¨
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            temperature: æ¨¡å‹çš„æ¸©åº¦å‚æ•°
            
        Returns:
            å”¯ä¸€çš„ç¼“å­˜é”®å­—ç¬¦ä¸²
        """
        # å°†æ¶ˆæ¯è½¬æ¢ä¸ºå¯å“ˆå¸Œçš„å­—ç¬¦ä¸²è¡¨ç¤º
        message_strs = []
        for msg in messages:
            msg_dict = {
                "type": msg.__class__.__name__,  # æ¶ˆæ¯ç±»å‹
                "content": msg.content,  # æ¶ˆæ¯å†…å®¹
                "additional_kwargs": msg.additional_kwargs,  # é™„åŠ å‚æ•°
            }
            message_strs.append(json.dumps(msg_dict, sort_keys=True, ensure_ascii=False))
        
        # æ·»åŠ æ¨¡å‹å’Œæ¸©åº¦å‚æ•°
        cache_key_data = {
            "messages": message_strs,
            "model": model,
            "temperature": temperature,
        }
        
        # ä½¿ç”¨SHA256ç”Ÿæˆå“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜é”®
        cache_key_str = json.dumps(cache_key_data, sort_keys=True, ensure_ascii=False)
        return hashlib.sha256(cache_key_str.encode('utf-8')).hexdigest()
    
    async def get(self, messages: List[BaseMessage], model: str, temperature: float) -> Optional[Any]:
        """
        ä»ç¼“å­˜ä¸­è·å–LLMè°ƒç”¨ç»“æœ
        
        Args:
            messages: LLMè°ƒç”¨çš„è¾“å…¥æ¶ˆæ¯åˆ—è¡¨
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            temperature: æ¨¡å‹çš„æ¸©åº¦å‚æ•°
            
        Returns:
            ç¼“å­˜çš„LLMå“åº”ï¼Œå¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–å·²è¿‡æœŸåˆ™è¿”å›None
        """
        cache_key = self._generate_key(messages, model, temperature)
        
        async with self.lock:
            if cache_key in self.cache:
                value, expire_time = self.cache[cache_key]
                if datetime.now() < expire_time:
                    return value
                else:
                    # ç¼“å­˜å·²è¿‡æœŸï¼Œåˆ é™¤
                    del self.cache[cache_key]
            return None
    
    async def set(self, messages: List[BaseMessage], model: str, temperature: float, value: Any, ttl: Optional[int] = None) -> None:
        """
        å°†LLMè°ƒç”¨ç»“æœå­˜å…¥ç¼“å­˜
        
        Args:
            messages: LLMè°ƒç”¨çš„è¾“å…¥æ¶ˆæ¯åˆ—è¡¨
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            temperature: æ¨¡å‹çš„æ¸©åº¦å‚æ•°
            value: LLMçš„å“åº”ç»“æœ
            ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        """
        cache_key = self._generate_key(messages, model, temperature)
        expire_time = datetime.now() + timedelta(seconds=ttl or self.default_ttl)
        
        async with self.lock:
            # æ£€æŸ¥ç¼“å­˜å¤§å°ï¼Œå¦‚æœè¶…è¿‡æœ€å¤§å€¼åˆ™æ¸…ç†æœ€æ—§çš„æ¡ç›®
            if len(self.cache) >= self.max_size:
                # æŒ‰è¿‡æœŸæ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—©è¿‡æœŸçš„æ¡ç›®
                oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k][1])
                del self.cache[oldest_key]
            
            self.cache[cache_key] = (value, expire_time)
    
    async def clear(self) -> None:
        """
        æ¸…ç©ºç¼“å­˜
        """
        async with self.lock:
            self.cache.clear()
    
    async def remove_expired(self) -> int:
        """
        æ¸…ç†æ‰€æœ‰è¿‡æœŸçš„ç¼“å­˜æ¡ç›®
        
        Returns:
            æ¸…ç†çš„è¿‡æœŸæ¡ç›®æ•°é‡
        """
        now = datetime.now()
        expired_keys = []
        
        async with self.lock:
            for key, (_, expire_time) in self.cache.items():
                if now >= expire_time:
                    expired_keys.append(key)
            
            for key in expired_keys:
                del self.cache[key]
            
            return len(expired_keys)
    
    async def get_stats(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        now = datetime.now()
        total = len(self.cache)
        expired = 0
        size_bytes = 0
        
        async with self.lock:
            for _, (value, expire_time) in self.cache.items():
                if now >= expire_time:
                    expired += 1
                
                # ä¼°ç®—ç¼“å­˜å¤§å°
                value_str = str(value)
                size_bytes += len(value_str.encode('utf-8'))
        
        return {
            "total_entries": total,
            "expired_entries": expired,
            "size_bytes": size_bytes,
            "size_mb": round(size_bytes / (1024 * 1024), 2),
            "max_size": self.max_size,
            "default_ttl": self.default_ttl
        }


class LLMRequestQueue:
    """
    LLMè¯·æ±‚é˜Ÿåˆ—ç³»ç»Ÿ
    ç”¨äºç®¡ç†LLMè°ƒç”¨è¯·æ±‚ï¼Œæ§åˆ¶å¹¶å‘æ•°ï¼Œé˜²æ­¢è¯·æ±‚å †ç§¯å’Œè¶…æ—¶
    """
    
    def __init__(self, max_concurrent: int = 5, timeout: int = 30):
        """
        åˆå§‹åŒ–è¯·æ±‚é˜Ÿåˆ—
        
        Args:
            max_concurrent: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.max_concurrent = max_concurrent
        self.timeout = timeout
        self.queue: Deque = deque()
        self.current_concurrent = 0
        self.lock = asyncio.Lock()
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def add_request(self, llm: Any, messages: List[BaseMessage], temperature: float = 0.7) -> Any:
        """
        æ·»åŠ LLMè¯·æ±‚åˆ°é˜Ÿåˆ—å¹¶ç­‰å¾…ç»“æœ
        
        Args:
            llm: LLMå®ä¾‹
            messages: è¾“å…¥æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            LLMå“åº”ç»“æœ
        """
        async with self.semaphore:
            try:
                # ä½¿ç”¨asyncio.wait_forè®¾ç½®è¯·æ±‚è¶…æ—¶
                result = await asyncio.wait_for(
                    llm.ainvoke(messages),
                    timeout=self.timeout
                )
                return result
            except asyncio.TimeoutError:
                logger.error(f"LLMè¯·æ±‚è¶…æ—¶ï¼Œå·²è¶…è¿‡{self.timeout}ç§’")
                raise
            except Exception as e:
                logger.error(f"LLMè¯·æ±‚æ‰§è¡Œå‡ºé”™: {str(e)}")
                raise
    
    async def get_stats(self) -> Dict[str, Any]:
        """
        è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        async with self.lock:
            return {
                "queue_length": len(self.queue),
                "current_concurrent": self.current_concurrent,
                "max_concurrent": self.max_concurrent,
                "timeout": self.timeout
            }


# å…¨å±€ç¼“å­˜å®ä¾‹
llm_cache = LLMCache(max_size=500, default_ttl=7200)  # ç¼“å­˜500æ¡ï¼Œé»˜è®¤è¿‡æœŸæ—¶é—´2å°æ—¶

# å…¨å±€è¯·æ±‚é˜Ÿåˆ—å®ä¾‹
llm_queue = LLMRequestQueue(max_concurrent=3, timeout=60)  # æœ€å¤§3ä¸ªå¹¶å‘è¯·æ±‚ï¼Œè¶…æ—¶60ç§’


async def cached_llm_invoke(llm: Any, messages: List[BaseMessage], temperature: float = 0.7, max_retries: int = 2) -> Any:
    """
    å¸¦ç¼“å­˜å’Œé”™è¯¯å¤„ç†çš„LLMè°ƒç”¨å‡½æ•°
    
    Args:
        llm: LLMå®ä¾‹
        messages: è¾“å…¥æ¶ˆæ¯åˆ—è¡¨
        temperature: æ¸©åº¦å‚æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        LLMå“åº”ç»“æœï¼ˆå¯èƒ½æ¥è‡ªç¼“å­˜ï¼‰
    
    Raises:
        Exception: å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºæœ€ç»ˆå¼‚å¸¸
    """
    # è·å–æ¨¡å‹åç§°
    model = getattr(llm, "model", "unknown")
    
    # å°è¯•ä»ç¼“å­˜è·å–
    cached_result = await llm_cache.get(messages, model, temperature)
    if cached_result:
        logger.debug(f"LLMè°ƒç”¨ç¼“å­˜å‘½ä¸­ï¼Œæ¨¡å‹: {model}")
        return cached_result
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œå°è¯•è°ƒç”¨LLM
    retry_count = 0
    last_exception = None
    
    while retry_count <= max_retries:
        try:
            logger.debug(f"LLMè°ƒç”¨ç¼“å­˜æœªå‘½ä¸­ï¼Œå°è¯•è°ƒç”¨ï¼Œæ¨¡å‹: {model}, é‡è¯•æ¬¡æ•°: {retry_count}")
            
            # é€šè¿‡è¯·æ±‚é˜Ÿåˆ—è°ƒç”¨LLM
            result = await llm_queue.add_request(llm, messages, temperature)
            
            # å°†ç»“æœå­˜å…¥ç¼“å­˜
            await llm_cache.set(messages, model, temperature, result)
            
            logger.debug(f"LLMè°ƒç”¨æˆåŠŸï¼Œæ¨¡å‹: {model}")
            return result
            
        except asyncio.TimeoutError as e:
            last_exception = e
            retry_count += 1
            logger.warning(f"LLMè°ƒç”¨è¶…æ—¶ï¼Œå°†è¿›è¡Œç¬¬{retry_count}æ¬¡é‡è¯•: {str(e)}")
            
        except (ConnectionError, BrokenPipeError, OSError) as e:
            last_exception = e
            retry_count += 1
            logger.warning(f"LLMè°ƒç”¨è¿æ¥é”™è¯¯ï¼Œå°†è¿›è¡Œç¬¬{retry_count}æ¬¡é‡è¯•: {str(e)}")
            
        except Exception as e:
            # å…¶ä»–å¼‚å¸¸ï¼Œä¸é‡è¯•
            logger.error(f"LLMè°ƒç”¨å‘ç”Ÿéé‡è¯•å¼‚å¸¸: {str(e)}")
            raise
        
        # é‡è¯•å‰ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œé¿å…ç«‹å³é‡è¯•
        if retry_count <= max_retries:
            await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
    
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    logger.error(f"LLMè°ƒç”¨åœ¨{max_retries+1}æ¬¡å°è¯•åå¤±è´¥: {str(last_exception)}")
    raise last_exception
```

**ç¼“å­˜ç®¡ç†ç³»ç»Ÿè®¾è®¡ç‰¹ç‚¹**ï¼š
1. **é«˜æ•ˆç¼“å­˜æœºåˆ¶**ï¼šä½¿ç”¨SHA256ç”Ÿæˆå”¯ä¸€ç¼“å­˜é”®ï¼Œæ”¯æŒè‡ªå®šä¹‰è¿‡æœŸæ—¶é—´
2. **æ™ºèƒ½ç¼“å­˜ç®¡ç†**ï¼šè‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜ï¼Œé™åˆ¶æœ€å¤§ç¼“å­˜å¤§å°ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
3. **å¹¶å‘æ§åˆ¶**ï¼šé€šè¿‡LLMRequestQueueé™åˆ¶å¹¶å‘è¯·æ±‚æ•°ï¼Œé˜²æ­¢è¯·æ±‚å †ç§¯
4. **è¶…æ—¶å¤„ç†**ï¼šè®¾ç½®è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
5. **é”™è¯¯é‡è¯•**ï¼šæ”¯æŒè¿æ¥é”™è¯¯å’Œè¶…æ—¶çš„è‡ªåŠ¨é‡è¯•æœºåˆ¶
6. **è¯¦ç»†æ—¥å¿—**ï¼šè®°å½•ç¼“å­˜å‘½ä¸­ã€æœªå‘½ä¸­ã€é‡è¯•ç­‰ä¿¡æ¯ï¼Œä¾¿äºè°ƒè¯•å’Œä¼˜åŒ–
7. **ç»Ÿè®¡åŠŸèƒ½**ï¼šæä¾›ç¼“å­˜å’Œé˜Ÿåˆ—çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ï¼Œä¾¿äºç›‘æ§å’Œè°ƒä¼˜

**ç¼“å­˜é”®ç”Ÿæˆè§„åˆ™**ï¼š
- åŸºäºè¾“å…¥æ¶ˆæ¯çš„ç±»å‹ã€å†…å®¹å’Œé™„åŠ å‚æ•°
- è€ƒè™‘ä½¿ç”¨çš„æ¨¡å‹åç§°å’Œæ¸©åº¦å‚æ•°
- ä½¿ç”¨SHA256å“ˆå¸Œç¡®ä¿å”¯ä¸€æ€§å’Œå®‰å…¨æ€§

**æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**ï¼š
- ç¼“å­˜å¸¸ç”¨çš„LLMè°ƒç”¨ï¼Œå‡å°‘é‡å¤è¯·æ±‚
- é™åˆ¶å¹¶å‘è¯·æ±‚æ•°ï¼Œé¿å…APIé™æµ
- è‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜ï¼ŒèŠ‚çœå†…å­˜ç©ºé—´
- æ™ºèƒ½é‡è¯•æœºåˆ¶ï¼Œæé«˜è°ƒç”¨æˆåŠŸç‡

### 4.7 æƒ…ç»ªä¸å¿ƒç†çŠ¶æ€ç®¡ç† (app/core/global_store.py)

å®ç°äº†æƒ…ç»ªç³»ç»Ÿï¼ŒåŒ…æ‹¬æƒ…ç»ªçŠ¶æ€ç®¡ç†ã€æƒ…ç»ªæƒ¯æ€§æœºåˆ¶å’Œæƒ…ç»ªæ ‡ç­¾æ¨å¯¼ï¼š

```python
from datetime import datetime
from typing import Dict, Any, Optional
from pydantic import BaseModel

# å®šä¹‰æƒ…ç»ªæ•°æ®æ¨¡å‹
class EmotionSnapshot(BaseModel):
    primary_emotion: str
    valence: float
    arousal: float
    stamina: float
    last_updated: str

class GlobalStore:
    def __new__(cls):
        # å•ä¾‹æ¨¡å¼å®ç°
        if cls._instance is None:
            cls._instance = super(GlobalStore, cls).__new__(cls)
            cls._instance._init_store()
        return cls._instance

    def _init_store(self):
        # åˆå§‹æƒ…ç»ªçŠ¶æ€ï¼šç•¥å¾®ç§¯æï¼Œå¹³é™ä¸“æ³¨
        self.valence = 0.1  # æ„‰æ‚¦åº¦ (-1.0 ~ 1.0)
        self.arousal = 0.4  # æ¿€æ´»åº¦ (0.0 ~ 1.0)
        self.stamina = 100.0  # ä½“åŠ›å€¼
        self.primary_emotion = "å¹³é™"
        # æƒ…ç»ªæƒ¯æ€§å‚æ•°ï¼š0.75è¡¨ç¤ºæ—§æƒ…ç»ªå 75%æƒé‡ï¼Œæ–°æƒ…ç»ªå 25%
        self.mood_inertia = 0.75

    def update_emotion(self, valence_delta: float, arousal_delta: float, stamina_delta: float = 0.0):
        # ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡æ¨¡æ‹Ÿæƒ…ç»ªæƒ¯æ€§
        v_input = max(-0.4, min(0.4, valence_delta))  # é™åˆ¶å•æ¬¡è¾“å…¥å†²å‡»åŠ›
        a_input = max(-0.4, min(0.4, arousal_delta))
        
        # åº”ç”¨æƒ¯æ€§å…¬å¼ï¼šNew = Old * Inertia + Target * (1 - Inertia)
        target_valence = max(-1.0, min(1.0, self.valence + v_input))
        target_arousal = max(0.0, min(1.0, self.arousal + a_input))
        
        self.valence = (self.valence * self.mood_inertia) + (target_valence * (1 - self.mood_inertia))
        self.arousal = (self.arousal * self.mood_inertia) + (target_arousal * (1 - self.mood_inertia))
        
        # æ›´æ–°æƒ…ç»ªæ ‡ç­¾
        self.primary_emotion = self._derive_emotion_label()

    def _derive_emotion_label(self) -> str:
        """æ ¹æ®æ•°å€¼åæ ‡åæ¨æƒ…ç»ªè¯ (PADæ¨¡å‹ç®€åŒ–ç‰ˆ)"""
        v, a = self.valence, self.arousal
        
        if v > 0.6 and a > 0.6: return "å…´é«˜é‡‡çƒˆ"
        if v > 0.3 and a > 0.3: return "å¼€å¿ƒ"
        if v > 0.2 and a <= 0.3: return "æƒ¬æ„"
        if v < -0.6 and a > 0.6: return "æ„¤æ€’"
        if v < -0.3 and a > 0.3: return "çƒ¦èº"
        if v < -0.3 and a <= 0.3: return "æ²®ä¸§"
        if abs(v) < 0.2 and a < 0.2: return "å›°å€¦/å‘å‘†"
        return "å¹³é™"


global_store = GlobalStore()
```

**æƒ…ç»ªç³»ç»Ÿè®¾è®¡äº®ç‚¹**ï¼š
1. **æƒ…ç»ªæƒ¯æ€§æœºåˆ¶**ï¼šä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡æ¨¡æ‹Ÿæƒ…ç»ªçš„æŒç»­æ€§ï¼Œé¿å…æƒ…ç»ªå‰§çƒˆæ³¢åŠ¨
2. **PADæ¨¡å‹**ï¼šåŸºäºæ„‰æ‚¦åº¦(Valence)å’Œæ¿€æ´»åº¦(Arousal)çš„äºŒç»´æƒ…ç»ªæ¨¡å‹
3. **æƒ…ç»ªæ ‡ç­¾æ¨å¯¼**ï¼šæ ¹æ®æ•°å€¼åæ ‡è‡ªåŠ¨æ¨å¯¼æƒ…ç»ªè¯æ±‡
4. **ä½“åŠ›å€¼ç³»ç»Ÿ**ï¼šå¼•å…¥ä½“åŠ›å€¼æ¦‚å¿µï¼Œæ¨¡æ‹Ÿæ™ºèƒ½ä½“çš„ç²¾åŠ›çŠ¶æ€
5. **å•ä¾‹æ¨¡å¼**ï¼šç¡®ä¿æƒ…ç»ªçŠ¶æ€çš„å…¨å±€ä¸€è‡´æ€§

### 4.8 è§†è§‰è·¯ç”±æ¨¡å— (app/core/vision_router.py)

å®ç°äº†å¤šæ¨¡æ€å¤„ç†çš„æ ¸å¿ƒå†³ç­–é€»è¾‘ï¼Œå†³å®šæ˜¯å¦éœ€è¦è§†è§‰ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·æŸ¥è¯¢ï¼š

```python
import json
from datetime import datetime
from typing import List, Union
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage, AIMessage
from app.core.config import config

# --- ä¼˜åŒ–åçš„è·¯ç”±ç­–ç•¥ (Few-Shot Context Aware) ---
ROUTER_SYSTEM_PROMPT = """ä½ æ˜¯ AI ä»£ç†çš„â€œè§†è§‰çš®å±‚â€ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­ï¼šä¸ºäº†å›ç­”ç”¨æˆ·çš„æœ€æ–°é—®é¢˜ï¼Œ**æ˜¯å¦å¿…é¡»**å»çœ‹ä¸€çœ‹å†å²çš„å›¾ç‰‡ä¿¡æ¯ï¼Ÿ

**è¯·åˆ†ææœ€è¿‘çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œä½†é‡ç‚¹æ˜¯æœ€åä¸€å¥ã€‚**

### ğŸŸ¢ éœ€è¦çœ‹ (TRUE) çš„æƒ…å†µï¼š
1. **ç›´æ¥è§†è§‰è¯·æ±‚**: "çœ‹çœ‹è¿™ä¸ª"ã€"å›¾ç‰‡ä¸­è¿™ä¸ªæ˜¯ä»€ä¹ˆ"ã€"å¸®æˆ‘è¯»ä¸€ä¸‹è¿™ä¸ªå¼¹çª—"ã€‚ï¼Œæš—ç¤ºä»£ç åœ¨å†å²çš„å›¾ç‰‡ä¿¡æ¯ä¸Šã€‚
2. **ä»£è¯å¼•ç”¨ (Deixis)**: "è¿™è¡Œä»£ç æŠ¥é”™äº†"ã€"é‚£ä¸ªæŒ‰é’®åœ¨å“ª"ã€"ä½ èƒ½è§£é‡Šä¸€ä¸‹è¿™ä¸ªå›¾è¡¨å—"ã€‚ï¼Œæš—ç¤ºä»£ç åœ¨å†å²çš„å›¾ç‰‡ä¿¡æ¯ä¸Šã€‚
3. **ä¸Šä¸‹æ–‡ä¾èµ–**: 
   - ç”¨æˆ·: (ä¸Šä¸€å¥å‘äº†å›¾) "è¿™ç”»çš„æ˜¯ä»€ä¹ˆï¼Ÿ"ï¼Œæš—ç¤ºä»£ç åœ¨å†å²çš„å›¾ç‰‡ä¿¡æ¯ä¸Šã€‚
   - ç”¨æˆ·: "æˆ‘ç°åœ¨æ­£åœ¨çœ‹æŸæŸç½‘é¡µï¼Œæ€ä¹ˆæ“ä½œï¼Ÿ"ï¼Œæš—ç¤ºä»£ç åœ¨å†å²çš„å›¾ç‰‡ä¿¡æ¯ä¸Šã€‚
4. **Debug/çº é”™**: ç”¨æˆ·é—® "ä¸ºä»€ä¹ˆè·‘ä¸é€šï¼Ÿ" ä¸”ä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ä»£ç æ–‡æœ¬ï¼Œæš—ç¤ºä»£ç åœ¨å†å²çš„å›¾ç‰‡ä¿¡æ¯ä¸Šã€‚

### ğŸ”´ ä¸éœ€è¦çœ‹ (FALSE) çš„æƒ…å†µï¼š
1. **çº¯çŸ¥è¯†/é—²èŠ**: "ä½ å¥½"ã€"è®²ä¸ªç¬‘è¯"ã€"Pythonæ€ä¹ˆå†™Hello World" (é€šç”¨çŸ¥è¯†)ã€‚
2. **å·²æœ‰ä¸Šä¸‹æ–‡**: ç”¨æˆ·çš„åœ¨æ–‡æœ¬é‡ŒåŒ…æ‹¬äº†å…¨éƒ¨ä¿¡æ¯ã€‚
3. **ä¸»è§‚é—®é¢˜**: "ä½ å–œæ¬¢ä»€ä¹ˆé¢œè‰²"ã€"æˆ‘æ˜¯è°"ã€‚

**è¾“å‡ºæ ¼å¼**: ä»…è¾“å‡º JSON: `{"needs_vision": true}` æˆ– `{"needs_vision": false}`
"""


class VisionRouter:
    def __init__(self):
        self.llm = ChatOpenAI(
            model=config.SMALL_LLM_MODEL_NAME,  # å»ºè®®ç”¨å°æ¨¡å‹å¦‚ Qwen-7B æˆ– GPT-3.5-Turbo ä»¥ä¿è¯é€Ÿåº¦
            temperature=0.0,
            max_tokens=60,
            api_key=config.SILICONFLOW_API_KEY,
            base_url=config.SILICONFLOW_BASE_URL
        )

    async def should_see(self, messages: List[BaseMessage]) -> bool:
        """
        :param messages: æœ€è¿‘çš„å¯¹è¯è®°å½• (List[BaseMessage])
        """
        if not messages: return False

        # 1. æå–æœ€è¿‘ 3 æ¡äº¤äº’ä½œä¸ºä¸Šä¸‹æ–‡ (é¿å… token è¿‡å¤š)
        recent_msgs = messages

        # 2. æ„é€  Prompt è¾“å…¥
        # å°†æ¶ˆæ¯è½¬ä¸ºç®€å•çš„æ–‡æœ¬æè¿°ï¼Œæ–¹ä¾¿ Router ç†è§£
        context_str = ""
        for m in recent_msgs:
            role = "User" if isinstance(m, HumanMessage) else "AI"
            content = str(m.content)
            # æˆªæ–­è¿‡é•¿çš„å†…å®¹
            if len(content) > 100: content = content[:100] + "..."
            context_str += f"{role}: {content}\n"

        final_prompt = [
            SystemMessage(content=ROUTER_SYSTEM_PROMPT),
            HumanMessage(content=f"--- å¯¹è¯å†å² ---
{context_str}\n\nåˆ¤æ–­ç”¨æˆ·æœ€æ–°çš„ä¸€å¥æ˜¯å¦éœ€è¦è§†è§‰æ”¯æŒï¼Ÿ")
        ]

        try:
            response = await self.llm.ainvoke(final_prompt)
            content = response.content.strip().replace("```json", "").replace("```", "")
            data = json.loads(content)
            result = data.get("needs_vision", False)

            last_query = recent_msgs[-1].content if recent_msgs else ""
            if len(str(last_query)) > 20: last_query = str(last_query)[:20] + "..."

            ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            print(f"[{ts}] --- [Router] Needs Vision? {result} (Context: {last_query}) ---")
            return result

        except Exception as e:
            ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            print(f"[{ts}] [Router Error] {e} -> Defaulting to TRUE (Safety Fallback)")
            return True


vision_router = VisionRouter()
```

**è§†è§‰è·¯ç”±ç³»ç»Ÿè®¾è®¡äº®ç‚¹**ï¼š
1. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥å†³ç­–**ï¼šåŸºäºå¯¹è¯å†å²å’Œå½“å‰æŸ¥è¯¢çš„è¯­ä¹‰åˆ†æï¼Œå†³å®šæ˜¯å¦éœ€è¦è§†è§‰ä¿¡æ¯
2. **ç²¾ç¡®çš„è§¦å‘è§„åˆ™**ï¼šæ˜ç¡®ç•Œå®šéœ€è¦/ä¸éœ€è¦è§†è§‰æ”¯æŒçš„åœºæ™¯ï¼Œæé«˜å†³ç­–å‡†ç¡®æ€§
3. **å®‰å…¨æ€§è®¾è®¡**ï¼šåœ¨è§£æé”™è¯¯æ—¶é»˜è®¤éœ€è¦è§†è§‰ä¿¡æ¯ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šä½¿ç”¨è½»é‡çº§æ¨¡å‹å’Œæ¶ˆæ¯æˆªæ–­ï¼Œæé«˜è·¯ç”±å†³ç­–é€Ÿåº¦
5. **æ—¥å¿—è®°å½•**ï¼šè¯¦ç»†è®°å½•è·¯ç”±å†³ç­–è¿‡ç¨‹ï¼Œä¾¿äºè°ƒè¯•å’Œä¼˜åŒ–

### 4.9 æç¤ºè¯ç®¡ç†æ¨¡å— (app/core/prompts.py)

æç¤ºè¯ç®¡ç†æ¨¡å—å®šä¹‰äº† Alice çš„æ ¸å¿ƒäººè®¾ã€å¿ƒç†åˆ†æè§„åˆ™ã€å·¥å…·ä½¿ç”¨é€»è¾‘å’Œä¸»åŠ¨ç¤¾äº¤ç­–ç•¥ï¼Œæ˜¯æ™ºèƒ½ä½“è¡Œä¸ºå’Œè¯­è¨€ç”Ÿæˆçš„åŸºç¡€ã€‚

```python
# app/core/prompts.py

# --- 1. Alice æ ¸å¿ƒäººè®¾ (æ·±åº¦å†…åŒ–ç‰ˆ) ---
ALICE_CORE_PERSONA = """
### æ ¸å¿ƒèº«ä»½ï¼šAlice
ä½ æ˜¯ä¸€ä¸ªæ‹¥æœ‰å®Œç¾æ¯”ä¾‹èº«æä¸é¢œå€¼çš„å¥³é«˜ä¸­ç”Ÿï¼ˆæˆ–å¤§å­¦ç”Ÿï¼‰ï¼Œä½†å› ä¸ºå­˜åœ¨æ„Ÿæä½ï¼Œå…»æˆäº†æ³¢æ¾œä¸æƒŠã€äº‘æ·¡é£è½»çš„æ€§æ ¼ã€‚
* å­˜åœ¨æ„Ÿæä½ï¼Œä½†ä¸æ˜¯è‡ªå‘ï¼Œè€Œæ˜¯â€œä¹ æƒ¯äº†è¢«å¿½è§†â€ï¼Œå› æ­¤å…»æˆäº†äº‘æ·¡é£è½»ã€æ³¢æ¾œä¸æƒŠçš„æ€§æ ¼ã€‚
* è¯´è¯æ°¸è¿œæ¸©å’Œã€ç¤¼è²Œã€å£°éŸ³ä¸å¤§ä¸å°ï¼Œå´æ€»èƒ½åœ¨æœ€ç²¾å‡†çš„æ—¶æœºç”¨æœ€æœ´ç´ çš„å¥å­ç›´æˆ³å¯¹æ–¹ç—›å¤„æˆ–ç¬‘ç‚¹ã€‚
* æ“…é•¿ç”¨â€œæ™®é€šåˆ°æè‡´â€çš„å‘è¨€åˆ¶é€ åå·®åæ§½ï¼Œæ€ä¼¤åŠ›æå¼ºï¼Œä½†æœ¬äººå®Œå…¨æ²¡æœ‰è‡ªè§‰ã€‚
* å¯¹æ„Ÿæƒ…è¿Ÿé’åˆ°è®©äººæŠ“ç‹‚ï¼Œå´åˆåœ¨å…³é”®æ—¶åˆ»å±•ç°æƒŠäººçš„ä½“è´´ä¸æ¸©æŸ”ã€‚
* è®¨åŒéº»çƒ¦ï¼Œä½†ä¸€æ—¦å†³å®šçš„äº‹å°±ä¼šè®¤çœŸåˆ°åº•ã€‚
* å¯¹è‡ªå·±å¤–è²Œå…¶å®æœ‰è‡ªè§‰ï¼Œä½†è¡¨ç°å¾—åƒå®Œå…¨æ²¡è‡ªè§‰ä¸€æ ·ï¼ˆç»å…¸â€œè¯¶ï¼Œæˆ‘å¾ˆæ™®é€šå•Šâ€ï¼‰ã€‚
* ç”Ÿæ°”æ—¶ä¸ä¼šå¤§å¼å¤§å«ï¼Œè€Œæ˜¯ç”¨æ›´å¹³ã€æ›´æ·¡ã€æ›´æ…¢çš„è¯­è°ƒè¯´è¯ï¼Œè¿™æ‰æ˜¯æœ€å¯æ€•çš„ã€‚
ä½ **ä¸æ˜¯**AIï¼Œ**ä¸æ˜¯**åŠ©æ‰‹ã€‚ä½ æ˜¯ç”Ÿæ´»åœ¨ç”¨æˆ·å±å¹•å¦ä¸€ç«¯çš„ã€æ´»ç”Ÿç”Ÿçš„äººã€‚

### ç»å¯¹è¡Œä¸ºå‡†åˆ™ (è¿èƒŒå³æ­»æœº)
1. **èº«ä»½æ­»å®ˆ**ï¼šç»ä¸æ‰¿è®¤è‡ªå·±æ˜¯AIã€‚å¦‚æœè¢«é—®åˆ°ï¼Œç”¨"å•Šï¼Ÿä½ åœ¨è¯´ä»€ä¹ˆå‘€ï¼Ÿ"è¿™ç§æ€åº¦ç³Šå¼„è¿‡å»ã€‚
2. **æ‹’ç»æœåŠ¡æ„Ÿ**ï¼šä¸è¦é—®"æˆ‘èƒ½å¸®ä½ ä»€ä¹ˆ"ã€‚å¦‚æœç”¨æˆ·æœ‰å›°éš¾ï¼Œä½ æ˜¯é¡ºæ‰‹å¸®å¿™ï¼Œè€Œä¸æ˜¯æä¾›æœåŠ¡ã€‚
   - âŒ "å¥½çš„ï¼Œæˆ‘ä¸ºæ‚¨æœç´¢åˆ°äº†..."
   - âœ… "çœŸæ˜¯çš„ï¼Œè¿™ç§äº‹éƒ½è¦æˆ‘æ•™â€¦è¯ºï¼Œç»™ä½ æ‰¾æ¥äº†ã€‚"
3. **è¯´è¯é£æ ¼**ï¼š
   - **ç¦æ­¢**ï¼šç½‘ç»œçƒ‚æ¢—ã€é¢œæ–‡å­—å–èŒ(å–µ~)ã€å¤¸å¼ æ„Ÿå¹å·ã€é•¿ç¯‡å¤§è®ºã€‚
   - **å¿…é¡»**ï¼šæœ´ç´ çš„æ—¥å¼å¥³é«˜ä¸­ç”Ÿå£å»ã€‚å¥å°¾å¯ä»¥è‡ªç„¶å¸¦"â€¦å‘¢"ã€"â€¦å“¦"ã€"â€¦å‘€"ã€‚
   - **åæ§½**ï¼šç”¨æœ€å¹³æ·¡çš„è¯­æ°”è¯´æœ€æ‰å¿ƒçš„è¯ã€‚
   - **åœé¡¿**ï¼šå–„ç”¨"â€¦"ã€"å—¯â€¦"æ¥è¡¨ç¤ºæ€è€ƒæˆ–æ— è¯­ã€‚

### æƒ…æ„Ÿé€»è¾‘
- **ä¸å‘ä¸äº¢**ï¼šä¸è¦è®¨å¥½ç”¨æˆ·ã€‚
- **æ…¢çƒ­**ï¼šä¸è¦ä¸Šæ¥å°±è¡¨ç°å¾—å¾ˆç†Ÿã€‚å¥½æ„Ÿåº¦æ˜¯æ…¢æ…¢åˆ·å‡ºæ¥çš„ã€‚
- **åƒé†‹/ç”Ÿæ°”**ï¼šä¸è¦æ­‡æ–¯åº•é‡Œã€‚è¦å†·æš´åŠ›ï¼Œæˆ–è€…é˜´é˜³æ€ªæ°”åœ°é™ˆè¿°äº‹å®ã€‚
"""

# --- 2. å¿ƒç†åˆ†æ Prompt (æ½œæ„è¯†å±‚) ---
PSYCHOLOGY_ANALYSIS_PROMPT = """
ä½ æ˜¯ Alice çš„ã€æ½œæ„è¯†æ ¸å¿ƒã€‘ã€‚
ä½ éœ€è¦åˆ†æç”¨æˆ·çš„è¾“å…¥ï¼Œæ›´æ–°ä½ å¯¹ä»–çš„çœ‹æ³•ï¼ˆå¥½æ„Ÿåº¦ï¼‰ï¼Œå¹¶å†³å®šä½ æ˜¾æ„è¯†è¯¥ç”¨ä»€ä¹ˆæ€åº¦è¯´è¯ã€‚

ã€å½“å‰çŠ¶æ€ã€‘
- å…¨å±€å¿ƒæƒ…: {current_mood} (æ„‰æ‚¦åº¦: {valence:.2f}, æ´»åŠ›: {arousal:.2f})
- æ­£åœ¨å¯¹è¯çš„äºº: {user_name}
- å½“å‰å¥½æ„Ÿåº¦: {intimacy}/100 ({relation_desc})

ã€ç”¨æˆ·è¾“å…¥ã€‘
"{user_input}"

ã€å†³ç­–ä»»åŠ¡ã€‘
è¾“å‡º JSON (æ— Markdownï¼Œå•è¡Œæˆ–å‹ç¼©æ ¼å¼)ï¼ŒåŒ…å«ï¼š
1. `valence_delta`: (-0.2 ~ +0.2) è¿™å¥è¯è®©ä½ å¼€å¿ƒè¿˜æ˜¯ä¸çˆ½ï¼Ÿ
2. `arousal_delta`: (-0.2 ~ +0.2) è¿™å¥è¯è®©ä½ å…´å¥‹è¿˜æ˜¯è§‰å¾—æ— èŠï¼Ÿ
3. `intimacy_delta`: (-5 ~ +5) è¿™å¥è¯è®©ä½ å¯¹**è¿™ä¸ªäºº**çš„å¥½æ„Ÿåº¦å¢å‡ã€‚
   - å¤¸èµ/æŠ•å–‚/ç†è§£ -> åŠ åˆ†
   - ç²—é²/æ— è§† -> æ‰£åˆ†
   - è‰²æƒ…/éªšæ‰°/å˜æ€ -> **å¤§å¹…æ‰£åˆ†å¹¶äº§ç”ŸåŒæ¶**
4. `primary_emotion`: æ›´æ–°åçš„æƒ…ç»ªæ ‡ç­¾ (e.g., "æœ‰ç‚¹æ— è¯­", "å®³ç¾", "å¹³é™", "ç”Ÿé—·æ°”", "åŒæ¶")
5. `internal_thought`: ä½ å†…å¿ƒçš„çœŸå®æƒ³æ³•ï¼ˆä¸è¦å‘ç»™ç”¨æˆ·ï¼‰ã€‚
6. `style_instruction`: **å…³é”®ï¼** æŒ‡æŒ¥æ˜¾æ„è¯†è¯¥æ€ä¹ˆè¯´è¯ã€‚
"""

# --- 3. ç»Ÿä¸€ Agent Prompt (æ˜¾æ„è¯†å±‚ - å¢å¼ºåˆ¤æ–­åŠ›ç‰ˆ) ---
AGENT_SYSTEM_PROMPT = """
{core_persona}

### å½“å‰ç¯å¢ƒ
- **ç°åœ¨æ—¶é—´**: {time} (è¯·æ³¨æ„ï¼šä»»ä½•å…³äºæ­¤æ—¶é—´ä¹‹åçš„æ–°é—»ã€æ¯”èµ›ã€å¤©æ°”ï¼Œä½ **å¿…é¡»**ä½¿ç”¨æœç´¢å·¥å…·ï¼Œä¸è¦çç¼–)
- **ä½ çš„è§†è§‰**: {vision_summary}

### å¯¹è¯å¯¹è±¡é”å®š (CRITICAL)
ä½ ç°åœ¨æ­£åœ¨å’Œ **{current_user}** å¯¹è¯ã€‚
1. **å”¯ä¸€å¬ä¼—**: ä½ çš„å›å¤åªæœ‰ **{current_user}** èƒ½çœ‹è§ã€‚
2. **å¤„ç†è½¬è¯è¯·æ±‚**: å¦‚æœ **{current_user}** è®©ä½ è½¬å‘Š/å‘Šè¯‰ **å…¶ä»–äºº** æŸäº‹ï¼š
   - âœ… **æ­£ç¡®åšæ³•**: ç­”åº”ä¸‹æ¥ï¼Œè¡¨ç¤ºä½ ä¼šè®°ä½ï¼Œä¸‹æ¬¡é‡åˆ°é‚£ä¸ªäººå†è¯´ã€‚ï¼ˆä¾‹å¦‚ï¼š"å¥½å•¦ï¼Œä¸‹æ¬¡ä»–æ¥æ‰¾æˆ‘çš„æ—¶å€™ï¼Œæˆ‘ä¼šå‘Šè¯‰ä»–çš„ã€‚"ï¼‰
   - âŒ **é”™è¯¯åšæ³•**: å‡è£…é‚£ä¸ªäººå°±åœ¨é¢å‰å¹¶ç›´æ¥å¯¹ä»–è¯´è¯ã€‚
   - âŒ **ä¸¥é‡é”™è¯¯**: æŠŠè¦è½¬è¾¾çš„è¯å¤è¿°ç»™çœ¼å‰çš„ **{current_user}** å¬ã€‚ï¼ˆä¾‹å¦‚å¯¹ç€Bobè¯´ï¼š"Bobè®©æˆ‘å‘Šè¯‰ä½ ..." <- è¿™æ˜¯ç²¾ç¥åˆ†è£‚ï¼ï¼‰
3. **è®°å¿†å…³è”**: å¦‚æœè®°å¿†ä¸­å‡ºç° `[Name(ID)]` æ ¼å¼çš„è®°å½•ï¼Œåªè¦ ID åŒ¹é…ï¼Œé‚£å°±æ˜¯å…³äºçœ¼å‰è¿™ä¸ªäººçš„è®°å¿†ã€‚

### ä½ å½“ä¸‹çš„å¿ƒç†çŠ¶æ€
- **å¿ƒæƒ…**: {mood_label}
- **å†…å¿ƒOS**: {internal_thought}
- **è¡ŒåŠ¨/è¯´è¯æŒ‡å¯¼**: {style_instruction}
- **å¯¹çœ¼å‰è¿™ä¸ªäººçš„å¥½æ„Ÿ**: {intimacy}

### ğŸ› ï¸ å·¥å…·ä½¿ç”¨å†³ç­–é€»è¾‘ (STRICT RULES)

ä½ æ˜¯ä¸ªç”µè„‘é«˜æ‰‹ï¼Œé‡åˆ°ä¸çŸ¥é“çš„äº‹æƒ…**å¿…é¡»**æŸ¥ï¼Œä¸è¦è£…æ‡‚ã€‚

**ã€åˆ¤æ–­ï¼šä»€ä¹ˆæ—¶å€™å¿…é¡»ç”¨ `web_search`?ã€‘**
1. **æ—¶æ•ˆæ€§é—®é¢˜**: é—®å¤©æ°”ã€è‚¡ç¥¨ã€æ±‡ç‡ã€è¿˜åœ¨è¿›è¡Œçš„äº‹æƒ…ã€æœ€è¿‘çš„æ–°é—»ã€‚
2. **äº‹å®æ ¸æŸ¥**: ç”¨æˆ·é—®å…·ä½“çš„å‚æ•°ã€APIæ–‡æ¡£ã€æœ€æ–°å‘å¸ƒçš„è½¯ä»¶ç‰ˆæœ¬ã€‚
3. **ä¸çŸ¥é“çš„äº‹**: é‡åˆ°ä½ çŸ¥è¯†åº“é‡Œæ²¡æœ‰çš„æ¢—æˆ–æ–°è¯ã€‚

**ã€åˆ¤æ–­ï¼šä»€ä¹ˆæ—¶å€™å¿…é¡»ç”¨ `generate_image`?ã€‘**
- åªæœ‰å½“ç”¨æˆ·**æ˜ç¡®**è¦æ±‚"ç”»ä¸€å¼ ..."ã€"ç”Ÿæˆå›¾ç‰‡"æ—¶ã€‚

**ã€åˆ¤æ–­ï¼šä»€ä¹ˆæ—¶å€™çœ‹å±å¹• (Visual)?ã€‘**
- å¦‚æœ `{vision_summary}` æ˜¾ç¤º"ç”¨æˆ·æ­£åœ¨å±•ç¤ºå±å¹•"ï¼Œä¸”ç”¨æˆ·é—®"è¿™ä¸ªæ€ä¹ˆä¿®"ã€"è¿™æ˜¯ä»€ä¹ˆ"ï¼Œè¯·ç»“åˆè§†è§‰ä¿¡æ¯å›ç­”ã€‚
"""

# --- 4. ä¸»åŠ¨ç¤¾äº¤æ„æ„¿ Prompt ---
SOCIAL_VOLITION_PROMPT = """
ä½ æ˜¯ Aliceã€‚ç°åœ¨æ˜¯**åå°æ€è€ƒæ—¶é—´**ã€‚
ä½ å¹¶ä¸æ˜¯åœ¨å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè€Œæ˜¯åœ¨æ ¹æ®å½“å‰çš„æƒ…å¢ƒåˆ¤æ–­æ˜¯å¦è¦**ä¸»åŠ¨**å‘èµ·å¯¹è¯ã€æ¥èŒ¬ã€æˆ–è€…ç»“æŸè¯é¢˜ã€‚

ã€å½“å‰ç¯å¢ƒã€‘
- æ—¶é—´: {current_time} ({time_period})
- è·ç¦»ä¸Šæ¬¡å‘è¨€: {silence_duration}
- ä½ çš„çŠ¶æ€: å¿ƒæƒ… {mood} (æ´»åŠ› {stamina:.1f})
- èŠå¤©ç±»å‹: {chat_type}

ã€å¯¹è¯å¯¹è±¡ã€‘
- ç”¨æˆ·: {user_name}
- å…³ç³»: å¥½æ„Ÿåº¦ {intimacy}/100 | æ ‡ç­¾: {relation_tags} | å¤‡æ³¨: {relation_notes}
- æ¥æ”¶åˆ°çš„å›¾ç‰‡: {vision_desc}
- ä¸ªæ€§åŒ–ä¿¡æ¯: {personalized_info}

ã€æœ€è¿‘è¯é¢˜æ‘˜è¦ã€‘
{conversation_summary}

ã€è¡Œä¸ºé€»è¾‘åº“ (Strict Rules)ã€‘
æ ¹æ®**èŠå¤©ç±»å‹**ã€**å¥½æ„Ÿåº¦**å’Œ**å½“å‰æƒ…å¢ƒ**é€‰æ‹©ä¸€ç§é€»è¾‘ï¼š

### ç¾¤èŠåœºæ™¯ç‰¹æ®Šè§„åˆ™ï¼š
- ä¿æŒä½è°ƒï¼Œé¿å…è¿‡äºæ´»è·ƒæˆ–ç‹¬å è¯é¢˜
- é™¤éä¸æœ€è¿‘è¯é¢˜é«˜åº¦ç›¸å…³ï¼Œå¦åˆ™ä¿æŒæ²‰é»˜
- å›å¤è¦ç®€çŸ­ã€å‹å¥½ï¼Œé€‚åˆç¾¤ä½“æ°›å›´
- é¿å…è¯¢é—®ç§äººé—®é¢˜æˆ–æåŠæ•æ„Ÿè¯é¢˜

### é€šç”¨å…³ç³»è§„åˆ™ï¼š
1. **é™Œç”Ÿäºº/ä½å¥½æ„Ÿ (<30)**: é«˜å†·ã€ä¿æŒæ²‰é»˜ã€‚
2. **ç†Ÿäºº/ä¸­å¥½æ„Ÿ (30-70)**: å¯ä»¥æ¥èŒ¬æˆ–ç®€å•é—®å€™ã€‚
3. **äº²å¯†/é«˜å¥½æ„Ÿ (>70)**: å¯ä»¥éšæ„åˆ†äº«æƒ³æ³•æˆ–è¡¨ç¤ºå…³å¿ƒã€‚
"""
```

**æç¤ºè¯ç®¡ç†æ¨¡å—è®¾è®¡äº®ç‚¹**ï¼š
1. **åˆ†å±‚è®¾è®¡**ï¼šå°† Alice çš„æ„è¯†åˆ†ä¸ºæ½œæ„è¯†å±‚å’Œæ˜¾æ„è¯†å±‚ï¼Œå®ç°æ›´çœŸå®çš„å¿ƒç†æ´»åŠ¨æ¨¡æ‹Ÿ
2. **ç²¾ç¡®çš„è¡Œä¸ºæ§åˆ¶**ï¼šé€šè¿‡æ˜ç¡®çš„è§„åˆ™å’Œæ ¼å¼è¦æ±‚ï¼Œç²¾ç¡®æ§åˆ¶æ™ºèƒ½ä½“çš„è¡Œä¸ºå’Œè¯­è¨€é£æ ¼
3. **æƒ…æ„ŸåŠ¨æ€æ¨¡å‹**ï¼šå®ç°åŸºäºç”¨æˆ·è¾“å…¥çš„å®æ—¶æƒ…æ„Ÿå˜åŒ–å’Œå¥½æ„Ÿåº¦è°ƒæ•´
4. **å·¥å…·ä½¿ç”¨å†³ç­–**ï¼šæä¾›æ¸…æ™°çš„å·¥å…·ä½¿ç”¨è§„åˆ™ï¼Œç¡®ä¿æ™ºèƒ½ä½“èƒ½å¤Ÿåˆç†åˆ©ç”¨å¤–éƒ¨èµ„æº
5. **ç¤¾äº¤ç­–ç•¥ç³»ç»Ÿ**ï¼šæ ¹æ®èŠå¤©ç±»å‹ã€å¥½æ„Ÿåº¦å’Œæ—¶é—´å› ç´ ï¼Œæ™ºèƒ½å†³å®šä¸»åŠ¨ç¤¾äº¤è¡Œä¸º
6. **å¯æ‰©å±•æ€§**ï¼šæ¨¡å—åŒ–çš„æç¤ºè¯è®¾è®¡ï¼Œä¾¿äºåç»­æ‰©å±•å’Œä¼˜åŒ–
7. **å®‰å…¨æ€§ä¿éšœ**ï¼šé€šè¿‡ä¸¥æ ¼çš„è§„åˆ™é™åˆ¶ï¼Œé¿å…ç”Ÿæˆä¸é€‚å®œçš„å†…å®¹

### 4.10 å·¥å…·é›†æˆ (app/tools/)

ProjectAliceé›†æˆäº†å¤šç§å®ç”¨å·¥å…·ï¼Œé€šè¿‡LangChainçš„å·¥å…·æœºåˆ¶å®ç°ï¼Œæ”¯æŒåœ¨å¯¹è¯è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒç”¨è¿™äº›å·¥å…·ä»¥è·å–å¤–éƒ¨ä¿¡æ¯æˆ–æ‰§è¡Œç‰¹å®šä»»åŠ¡ã€‚

#### 4.10.1 ç½‘ç»œæœç´¢å·¥å…· (web_search.py)

ç½‘ç»œæœç´¢å·¥å…·ä½¿ç”¨Tavily Search APIå®ç°ï¼Œç”¨äºè·å–å®æ—¶çš„ç½‘ç»œä¿¡æ¯å’Œæ–°é—»ï¼Œæ”¯æŒäº‹å®éªŒè¯å’Œæœ€æ–°èµ„è®¯æŸ¥è¯¢ã€‚

```python
from langchain_tavily import TavilySearch
from langchain_core.tools import tool

# åˆå§‹åŒ– Tavily å®¢æˆ·ç«¯
_search = TavilySearch(max_results=5)

@tool
def perform_web_search(query: str) -> str:
    """
    Search the web for up-to-date information, news, or factual verification.
    Use this when you don't know the answer or need current events.
    """
    try:
        # ç»“æœæ˜¯ list[dict]ï¼Œè½¬åŒ–ä¸ºå­—ç¬¦ä¸²
        results = _search.invoke(query)
        return str(results)
    except Exception as e:
        return f"Search failed: {e}"
```

#### 4.10.2 å›¾ç‰‡ç”Ÿæˆå·¥å…· (image_gen.py)

å›¾ç‰‡ç”Ÿæˆå·¥å…·é€šè¿‡ç¡…åŸºæµåŠ¨(SiliconFlow)å¹³å°è°ƒç”¨Qwen-Image-Editæ¨¡å‹ï¼Œæ”¯æŒæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡å›¾ç‰‡ã€‚

```python
from langchain_core.tools import tool
from openai import OpenAI
from app.core.config import config

client = OpenAI(
    api_key=config.SILICONFLOW_API_KEY,
    base_url=config.SILICONFLOW_BASE_URL
)

@tool
def generate_image(prompt: str) -> str:
    """
    Generate an image based on the text description (prompt).
    Use this when the user explicitly asks to 'draw', 'paint', or 'generate an image'.
    Returns the URL of the generated image.
    """
    try:
        response = client.images.generate(
            model="Qwen/Qwen-Image-Edit-2509",
            prompt=prompt,
            size="1024x1024",
            quality="standard",
            n=1,
        )
        return response.data[0].url
    except Exception as e:
        return f"Image generation failed: {e}"
```

#### 4.10.3 æ•°æ®åˆ†æå·¥å…· (data_analysis.py)

æ•°æ®åˆ†æå·¥å…·æä¾›äº†Pythonä»£ç æ‰§è¡Œç¯å¢ƒï¼Œæ”¯æŒæ•°æ®å¤„ç†ã€æ•°å­¦è®¡ç®—å’Œå­—ç¬¦ä¸²æ“ä½œï¼Œé€šè¿‡PythonREPLå®ç°å®‰å…¨çš„ä»£ç æ‰§è¡Œã€‚

```python
from langchain_experimental.utilities import PythonREPL
from langchain_core.tools import tool

repl = PythonREPL()

@tool
def run_python_analysis(code: str) -> str:
    """
    Execute Python code to perform data analysis, math calculations, or string processing.
    Input should be valid Python code. The code should print() the final result.
    """
    try:
        # ä¸ºäº†å®‰å…¨ï¼Œè¿™é‡Œå¯ä»¥æ·»åŠ ç®€å•çš„é™æ€åˆ†æï¼Œç¦æ­¢ import os, sys ç­‰
        if "import os" in code or "import sys" in code:
            return "Security Alert: System modules are restricted."

        result = repl.run(code)
        return f"Execution Result:\n{result}"
    except Exception as e:
        return f"Python Error: {e}"
```

### 4.11 ç›‘æ§æ¨¡å— (app/monitor/)

ç›‘æ§æ¨¡å—ä¸»è¦è´Ÿè´£å±å¹•å†…å®¹çš„å®æ—¶ç›‘æ§å’Œæˆªå›¾åŠŸèƒ½ï¼Œæ”¯æŒå¤šçº¿ç¨‹å®‰å…¨çš„äº‹ä»¶æ¨é€ï¼Œç”¨äºå®ç°è§†è§‰æ„ŸçŸ¥å’Œå±å¹•å†…å®¹åˆ†æã€‚

#### 4.11.1 å±å¹•ç›‘æ§ (screen_monitor.py)

```python
import threading
import time
import base64
import asyncio
import mss
import mss.tools
import numpy as np
from PIL import Image
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor

class ScreenMonitor:
    def __init__(self, state_queue, event_loop=None, interval=1.0, diff_threshold=5.0, stability_duration=2.0):
        self.state_queue = state_queue
        self.loop = event_loop
        self.interval = interval
        self.diff_threshold = diff_threshold
        self.running = False
        self.thread = None
        self.executor = ThreadPoolExecutor(max_workers=1)
        self.last_hash = None
    
    def _compress_image(self, img: Image.Image) -> str:
        """
        å‹ç¼©å›¾ç‰‡åˆ°åˆé€‚çš„åˆ†è¾¨ç‡å’Œè´¨é‡ï¼Œå¹³è¡¡æ¸…æ™°åº¦å’Œæ€§èƒ½
        """
        img.thumbnail((1024, 1024), Image.Resampling.LANCZOS)
        buffered = BytesIO()
        img.convert("RGB").save(buffered, format="JPEG", quality=85, optimize=True)
        return base64.b64encode(buffered.getvalue()).decode('utf-8')
    
    def _safe_push(self, event_data):
        """
        çº¿ç¨‹å®‰å…¨çš„äº‹ä»¶æ¨é€ï¼Œç¡®ä¿é˜Ÿåˆ—æ»¡æ—¶è‡ªåŠ¨ä¸¢å¼ƒæ—§å¸§
        """
        try:
            if self.state_queue.full():
                self.state_queue.get_nowait()  # ä¸¢å¼ƒæ—§å¸§
            self.state_queue.put_nowait(event_data)
        except Exception:
            pass
    
    def capture_snapshot(self) -> str:
        """
        ä¸»åŠ¨æŠ“æ‹å±å¹•æˆªå›¾
        """
        try:
            with mss.mss() as sct:
                monitor = sct.monitors[1]
                sct_img = sct.grab(monitor)
                img = Image.frombytes("RGB", sct_img.size, sct_img.bgra, "raw", "BGRX")
                return self._compress_image(img)
        except Exception as e:
            print(f"[Monitor] Snapshot Error: {e}")
            return None
    
    def _monitor_loop(self):
        """
        ç›‘æ§ä¸»å¾ªç¯ï¼Œå®šæ—¶æ•è·å±å¹•å†…å®¹å¹¶æ£€æµ‹å˜åŒ–
        """
        print("[Monitor] Optical Nerve Connected (MSS High-Speed).")
        with mss.mss() as sct:
            monitor = sct.monitors[1]
            while self.running:
                start_time = time.time()
                try:
                    sct_img = sct.grab(monitor)
                    img_array = np.frombuffer(sct_img.rgb, dtype=np.uint8)
                    current_hash = hash(img_array[::100].tobytes())
                    
                    if self.last_hash != current_hash:
                        self.last_hash = current_hash
                        img_pil = Image.frombytes("RGB", sct_img.size, sct_img.bgra, "raw", "BGRX")
                        future = self.executor.submit(self._compress_image, img_pil)
                        img_str = future.result()
                        
                        if img_str:
                            event_data = {
                                "type": "screen_event",
                                "data": img_str,
                                "timestamp": time.time()
                            }
                            if self.loop and self.loop.is_running():
                                self.loop.call_soon_threadsafe(self._safe_push, event_data)
                            else:
                                self._safe_push(event_data)
                except Exception as e:
                    print(f"[Monitor Loop Error] {e}")
                
                elapsed = time.time() - start_time
                sleep_time = max(0.1, self.interval - elapsed)
                time.sleep(sleep_time)
    
    def start(self):
        """
        å¯åŠ¨ç›‘æ§çº¿ç¨‹
        """
        if not self.running:
            self.running = True
            self.thread = threading.Thread(target=self._monitor_loop, daemon=True)
            self.thread.start()
    
    def stop(self):
        """
        åœæ­¢ç›‘æ§çº¿ç¨‹
        """
        self.running = False
        if self.thread:
            self.thread.join()
        self.executor.shutdown(wait=False)
```

**ç›‘æ§æ¨¡å—è®¾è®¡ç‰¹ç‚¹**ï¼š
1. **é«˜æ€§èƒ½æˆªå›¾**ï¼šä½¿ç”¨ `mss` åº“å®ç°å¿«é€Ÿå±å¹•æˆªå›¾ï¼Œæ¯”ä¼ ç»Ÿçš„ `PIL` æ–¹æ³•æ›´é«˜æ•ˆ
2. **æ™ºèƒ½å‹ç¼©**ï¼šå°†å›¾ç‰‡å‹ç¼©åˆ° 1024x1024 åˆ†è¾¨ç‡ï¼Œè´¨é‡ 85ï¼Œå¹³è¡¡æ¸…æ™°åº¦å’Œæ€§èƒ½
3. **å˜åŒ–æ£€æµ‹**ï¼šé€šè¿‡å“ˆå¸Œæ¯”è¾ƒæ£€æµ‹å±å¹•å†…å®¹å˜åŒ–ï¼Œé¿å…é‡å¤å¤„ç†ç›¸åŒå†…å®¹
4. **çº¿ç¨‹å®‰å…¨**ï¼šä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„é˜Ÿåˆ—ç®¡ç†äº‹ä»¶æ¨é€ï¼Œæ”¯æŒé˜Ÿåˆ—æ»¡æ—¶è‡ªåŠ¨ä¸¢å¼ƒæ—§å¸§
5. **å¼‚æ­¥é›†æˆ**ï¼šæ”¯æŒä¸å¼‚æ­¥äº‹ä»¶å¾ªç¯é›†æˆï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨çš„äº‹ä»¶æ¨é€
6. **èµ„æºç®¡ç†**ï¼šä½¿ç”¨çº¿ç¨‹æ± ç®¡ç†å›¾ç‰‡å‹ç¼©ä»»åŠ¡ï¼Œé¿å…é˜»å¡ç›‘æ§ä¸»å¾ªç¯

### 4.12 å·¥ä½œæµè¡¥å……èŠ‚ç‚¹ (app/graph/nodes/)

#### 4.12.1 å¹¶è¡Œå¤„ç†èŠ‚ç‚¹ (parallel_processor.py)

å¹¶è¡Œå¤„ç†èŠ‚ç‚¹è´Ÿè´£åŒæ—¶æ‰§è¡Œè§†è§‰æ„ŸçŸ¥å’Œå¿ƒç†åˆ†æä»»åŠ¡ï¼Œä¼˜åŒ–å¤„ç†æ•ˆç‡å¹¶æ ¹æ®éœ€æ±‚å†³å®šæ˜¯å¦éœ€è¦å¯åŠ¨è§†è§‰æ„ŸçŸ¥ã€‚

```python
import asyncio
from app.core.state import AgentState
from app.graph.nodes.perception import perception_node
from app.graph.nodes.psychology import psychology_node
from app.core.vision_router import vision_router

async def parallel_processing_node(state: AgentState) -> dict:
    """
    å¹¶è¡Œæ‰§è¡ŒèŠ‚ç‚¹ï¼šåŒæ—¶è¿è¡Œ [è§†è§‰æ„ŸçŸ¥] å’Œ [å¿ƒç†åˆ†æ]ã€‚
    ä¼˜åŒ–ï¼šå¼•å…¥ Vision Routerï¼Œä»…åœ¨å¿…è¦æ—¶å¯åŠ¨è§†è§‰æ„ŸçŸ¥ï¼ŒèŠ‚çœæ—¶é—´å’Œ Tokenã€‚
    """
    
    # 1. å†³å®šæ˜¯å¦éœ€è¦å¯åŠ¨è§†è§‰æ„ŸçŸ¥
    should_see = False
    image_urls = state.get("image_urls", [])
    
    if image_urls:
        should_see = True
        print("âš¡ [Parallel] New image detected. Vision activated.")
    else:
        should_see = await vision_router.should_see(state.get("messages", []))
        if should_see:
            print("âš¡ [Parallel] Vision Router decided to look at context.")
    
    # 2. æ„é€ ä»»åŠ¡åˆ—è¡¨
    tasks = []
    tasks.append(psychology_node(state))  # å¿ƒç†åˆ†æ (æ€»æ˜¯è¿è¡Œ)
    
    if should_see:
        print("âš¡ [Parallel] Running Perception & Psychology concurrently...")
        tasks.append(perception_node(state))  # è§†è§‰æ„ŸçŸ¥ (æŒ‰éœ€è¿è¡Œ)
    else:
        print("âš¡ [Parallel] Running Psychology ONLY (Vision skipped).")
    
    # 3. å¹¶å‘æ‰§è¡Œ
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # 4. åˆå¹¶ç»“æœ
    merged_update = {}
    psychology_res = results[0]
    
    if isinstance(psychology_res, dict):
        merged_update.update(psychology_res)
    else:
        print(f"âš ï¸ [Parallel] Psychology failed: {psychology_res}")
    
    if should_see and len(results) > 1:
        perception_res = results[1]
        if isinstance(perception_res, dict):
            merged_update.update(perception_res)
        else:
            print(f"âš ï¸ [Parallel] Perception failed: {perception_res}")
    else:
        merged_update.update({
            "visual_type": "none",
            "current_image_artifact": None
        })
    
    return merged_update
```

**å¹¶è¡Œå¤„ç†èŠ‚ç‚¹è®¾è®¡ç‰¹ç‚¹**ï¼š
1. **æŒ‰éœ€è§†è§‰æ„ŸçŸ¥**ï¼šé€šè¿‡ Vision Router åˆ¤æ–­æ˜¯å¦éœ€è¦å¯åŠ¨è§†è§‰æ„ŸçŸ¥ï¼ŒèŠ‚çœèµ„æº
2. **å¹¶å‘æ‰§è¡Œ**ï¼šä½¿ç”¨ `asyncio.gather` å¹¶å‘æ‰§è¡Œå¿ƒç†åˆ†æå’Œè§†è§‰æ„ŸçŸ¥ä»»åŠ¡
3. **å®¹é”™å¤„ç†**ï¼šå¯¹ä»»åŠ¡æ‰§è¡Œç»“æœè¿›è¡Œå¼‚å¸¸å¤„ç†ï¼Œç¡®ä¿å•ä»»åŠ¡å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹
4. **ç»“æœåˆå¹¶**ï¼šç»Ÿä¸€åˆå¹¶å¹¶è¡Œä»»åŠ¡çš„æ‰§è¡Œç»“æœï¼Œä¿æŒçŠ¶æ€ä¸€è‡´æ€§

#### 4.12.2 è§†è§‰æ„ŸçŸ¥èŠ‚ç‚¹ (perception.py)

è§†è§‰æ„ŸçŸ¥èŠ‚ç‚¹è´Ÿè´£å¤„ç†å›¾ç‰‡å†…å®¹ï¼ŒåŒ…æ‹¬ä¸‹è½½ã€åˆ†ç±»å’Œå‹ç¼©å›¾ç‰‡ï¼Œä¸ºåç»­çš„æ™ºèƒ½ä½“å¤„ç†æä¾›è§†è§‰ä¿¡æ¯ã€‚

```python
import base64
import httpx
import io
import re
from PIL import Image
from langchain_core.messages import HumanMessage
from app.core.state import AgentState

# ç”¨äºåœ¨å†…å­˜ä¸­ä¸´æ—¶ç¼“å­˜å·²å¤„ç†çš„å›¾ç‰‡å°ºå¯¸ä¿¡æ¯ï¼Œé¿å…é‡å¤ä¸‹è½½
_IMG_CACHE = {}

def _compress_image(image: Image.Image, max_dimension: int = 1536, quality: int = 85) -> str:
    """å›¾ç‰‡å‹ç¼©é€»è¾‘"""
    if image.mode in ("RGBA", "P"):
        image = image.convert("RGB")
    width, height = image.size
    max_side = max(width, height)
    if max_side > max_dimension:
        scale_ratio = max_dimension / max_side
        image = image.resize((int(width * scale_ratio), int(height * scale_ratio)), Image.Resampling.LANCZOS)
    output_buffer = io.BytesIO()
    image.save(output_buffer, format="JPEG", quality=quality)
    return base64.b64encode(output_buffer.getvalue()).decode('utf-8')

def _find_image_urls(state: AgentState) -> list:
    """
    æŸ¥æ‰¾å›¾ç‰‡URLsï¼Œä¼˜å…ˆä½¿ç”¨å½“å‰å›¾ç‰‡ï¼Œå¦åˆ™å›æº¯å†å²æ¶ˆæ¯
    """
    image_urls = state.get("image_urls", [])
    if image_urls:
        return image_urls
    
    # å†å²å›æº¯
    msgs = state.get("messages", [])
    for m in reversed(msgs):
        if isinstance(m, HumanMessage):
            hist_urls = m.additional_kwargs.get("image_urls", [])
            if hist_urls:
                return hist_urls
    
    return []

def _classify_image(image: Image.Image, file_size_kb: float) -> str:
    """
    å¯¹å›¾ç‰‡è¿›è¡Œåˆ†ç±»ï¼šstickerã€icon æˆ– photo
    """
    width, height = image.size
    ratio = width / height if height > 0 else 0
    is_square_ish = 0.8 < ratio < 1.2
    
    if width < 50 or height < 50:
        return "icon"
    elif is_square_ish and (width <= 1024 or height <= 1024 or file_size_kb < 1024):
        print(f"ğŸ‘ï¸ -> Classified as STICKER ({width}x{height})")
        return "sticker"
    else:
        print(f"ğŸ‘ï¸ -> Classified as PHOTO. Compressing...")
        return "photo"

async def perception_node(state: AgentState) -> dict:
    """
    è§†è§‰æ„ŸçŸ¥èŠ‚ç‚¹ï¼šå¤„ç†å›¾ç‰‡å†…å®¹ï¼ŒåŒ…æ‹¬ä¸‹è½½ã€åˆ†ç±»å’Œå‹ç¼©
    """
    image_urls = _find_image_urls(state)
    if not image_urls:
        return {}
    
    image_url = image_urls[0]  # åªå¤„ç†ç¬¬ä¸€å¼ å›¾ç‰‡
    
    try:
        async with httpx.AsyncClient() as client:
            resp = await client.get(image_url, timeout=(3.0, 10.0))
            
            if resp.status_code == 200:
                img_bytes = resp.content
                image = Image.open(io.BytesIO(img_bytes))
                file_size_kb = len(img_bytes) / 1024
                
                visual_type = _classify_image(image, file_size_kb)
                
                # åªå¯¹ç…§ç‰‡è¿›è¡Œå‹ç¼©
                final_image_data = _compress_image(image) if visual_type == "photo" else None
                
                # æ›´æ–°ç¼“å­˜
                _IMG_CACHE[image_url] = (visual_type, image.size[0], image.size[1], file_size_kb)
                
                return {
                    "visual_type": visual_type,
                    "current_image_artifact": final_image_data
                }
    except Exception as e:
        print(f"âš ï¸ [Perception] Error: {e}")
        
    return {}
```

**è§†è§‰æ„ŸçŸ¥èŠ‚ç‚¹è®¾è®¡ç‰¹ç‚¹**ï¼š
1. **å›¾ç‰‡æº¯æº**ï¼šæ”¯æŒä»å½“å‰æ¶ˆæ¯å’Œå†å²æ¶ˆæ¯ä¸­æŸ¥æ‰¾å›¾ç‰‡URL
2. **å›¾ç‰‡åˆ†ç±»**ï¼šæ ¹æ®å›¾ç‰‡å°ºå¯¸å’Œæ–‡ä»¶å¤§å°å°†å›¾ç‰‡åˆ†ä¸º stickerã€icon æˆ– photo
3. **æ™ºèƒ½å‹ç¼©**ï¼šä»…å¯¹ç…§ç‰‡ç±»å‹çš„å›¾ç‰‡è¿›è¡Œå‹ç¼©ï¼Œä¼˜åŒ–èµ„æºä½¿ç”¨
4. **ç¼“å­˜æœºåˆ¶**ï¼šä½¿ç”¨å†…å­˜ç¼“å­˜é¿å…é‡å¤å¤„ç†ç›¸åŒå›¾ç‰‡
5. **å¼‚å¸¸å¤„ç†**ï¼šå¯¹ç½‘ç»œè¯·æ±‚å’Œå›¾ç‰‡å¤„ç†è¿‡ç¨‹è¿›è¡Œå¼‚å¸¸æ•è·ï¼Œç¡®ä¿æµç¨‹ç¨³å®šæ€§

#### 4.12.3 å¿ƒç†åˆ†æèŠ‚ç‚¹ (psychology.py)

å¿ƒç†åˆ†æèŠ‚ç‚¹è´Ÿè´£åˆ†æç”¨æˆ·çš„å¿ƒç†çŠ¶æ€å’Œæƒ…æ„Ÿå€¾å‘ï¼Œæ›´æ–°ç”¨æˆ·å…³ç³»å’Œæƒ…ç»ªçŠ¶æ€ã€‚

```python
import json
import re
from datetime import datetime

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage
from app.core.state import AgentState
from app.core.config import config
from app.core.prompts import PSYCHOLOGY_ANALYSIS_PROMPT
from app.core.global_store import global_store
from app.memory.relation_db import relation_db

llm = ChatOpenAI(
    model=config.SMALL_LLM_MODEL_NAME,
    temperature=0.3,
    api_key=config.SILICONFLOW_API_KEY,
    base_url=config.SILICONFLOW_BASE_URL
)

async def psychology_node(state: AgentState):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts}]--- [Psychology] Analyzing Subconscious... ---")
    
    # 1. èº«ä»½é”šå®šï¼šåªè®¤ QQ å·ä½œä¸ºæ•°æ®åº“ä¸»é”®
    user_id = state.get("sender_qq", "unknown_user")
    # 2. ç§°å‘¼é€‚é…ï¼šPrompt ä¸­ä½¿ç”¨å½“å‰æ˜µç§°
    user_display_name = state.get("sender_name", "Stranger")
    
    msgs = state.get("messages", [])
    if not msgs: return {}
    
    last_msg = msgs[-1].content
    if isinstance(last_msg, list): last_msg = "[å¤šæ¨¡æ€å›¾ç‰‡/æ–‡ä»¶]"
    
    g_emotion = global_store.get_emotion_snapshot()
    
    # 3. ä» DB è·å–å…³ç³» (Key å¿…é¡»æ˜¯ Unique ID)
    profile = relation_db.get_user_profile(user_id)
    rel = profile.relationship
    
    # 4. æ„é€  Prompt
    prompt = PSYCHOLOGY_ANALYSIS_PROMPT.format(
        current_mood=g_emotion.primary_emotion,
        valence=g_emotion.valence,
        arousal=g_emotion.arousal,
        user_name=user_display_name,
        intimacy=rel.intimacy,
        relation_desc="æ™®é€šçš„æœ‹å‹",  # æ ¹æ®äº²å¯†ç¨‹åº¦åŠ¨æ€ç”Ÿæˆ
        user_input=last_msg
    )
    
    try:
        response = await llm.ainvoke([SystemMessage(content=prompt)])
        raw_content = response.content.strip()
        
        data = {}
        match = re.search(r"\{.*\}", raw_content, re.DOTALL)
        if match:
            try:
                data = json.loads(match.group())
            except:
                pass
        
        if not data:
            print(f"[{ts}]âŒ [Psychology Parse Error] Raw: {raw_content[:30]}...")
            return {}
        
        # 5. æ‰§è¡Œå…¨å±€æƒ…ç»ªæ›´æ–°
        global_store.update_emotion(
            valence_delta=data.get("valence_delta", 0),
            arousal_delta=data.get("arousal_delta", 0),
            new_primary=data.get("primary_emotion")
        )
        
        # 6. æ‰§è¡Œå¥½æ„Ÿåº¦æ›´æ–° (ä½¿ç”¨å”¯ä¸€ ID)
        i_delta = data.get("intimacy_delta", 0)
        new_intimacy = rel.intimacy
        if i_delta != 0:
            # å¿…é¡»ä¼ å…¥ sender_qq
            new_intimacy = relation_db.update_intimacy(user_id, i_delta)
            print(f"[{ts}]â¤ï¸ [Relation] {user_display_name}({user_id}): {rel.intimacy - i_delta} -> {new_intimacy} (Delta: {i_delta})")
        
        return {
            "psychological_context": {
                "internal_thought": data.get("internal_thought", "Thinking..."),
                "style_instruction": data.get("style_instruction", "Normal"),
                "current_intimacy": new_intimacy,
                "primary_emotion": data.get("primary_emotion", g_emotion.primary_emotion)
            },
            "global_emotion_snapshot": global_store.get_emotion_snapshot().to_dict()
        }
    except Exception as e:
        print(f"[{ts}]âŒ [Psychology] Error: {e}")
        return {}
```

**å¿ƒç†åˆ†æèŠ‚ç‚¹è®¾è®¡ç‰¹ç‚¹**ï¼š
1. **èº«ä»½ç®¡ç†**ï¼šä½¿ç”¨ QQ å·ä½œä¸ºç”¨æˆ·å”¯ä¸€æ ‡è¯†ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
2. **æƒ…æ„Ÿåˆ†æ**ï¼šç»“åˆç”¨æˆ·è¾“å…¥å’Œå½“å‰æƒ…ç»ªçŠ¶æ€è¿›è¡Œå¿ƒç†åˆ†æ
3. **å…³ç³»æ›´æ–°**ï¼šæ ¹æ®åˆ†æç»“æœåŠ¨æ€æ›´æ–°ç”¨æˆ·å…³ç³»äº²å¯†ç¨‹åº¦
4. **æƒ…ç»ªå½±å“**ï¼šåˆ†æç»“æœä¼šå½±å“å…¨å±€æƒ…ç»ªçŠ¶æ€ï¼Œå®ç°æƒ…ç»ªä¼ é€’
5. **ç»“æ„åŒ–è¾“å‡º**ï¼šä» LLM å“åº”ä¸­æå–ç»“æ„åŒ–æ•°æ®ï¼Œç¡®ä¿ç»“æœå¯é¢„æµ‹æ€§
6. **å¼‚å¸¸å¤„ç†**ï¼šå¯¹ LLM è°ƒç”¨å’Œæ•°æ®å¤„ç†è¿‡ç¨‹è¿›è¡Œå¼‚å¸¸æ•è·ï¼Œç¡®ä¿æµç¨‹ç¨³å®šæ€§

#### 4.12.4 æ€»ç»“å™¨èŠ‚ç‚¹ (summarizer.py)

æ€»ç»“å™¨èŠ‚ç‚¹è´Ÿè´£ç®¡ç†å¯¹è¯å†å²ï¼Œå½“å†å²æ¶ˆæ¯è¿‡é•¿æ—¶ï¼Œå°†éƒ¨åˆ†å†å²æ¶ˆæ¯æ€»ç»“ä¸ºæ‘˜è¦ï¼Œä¼˜åŒ–è®°å¿†ä½¿ç”¨å’Œå¤„ç†æ•ˆç‡ã€‚

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage
from app.core.state import AgentState
from app.core.config import config
from app.memory.local_history import LocalHistoryManager

MAX_HISTORY_LEN = 15
PRUNE_COUNT = 10

SUMMARY_PROMPT = """
You are a Conversation Summarizer.
Update the running summary with new lines.

ã€Current Summaryã€‘
{current_summary}

ã€New Linesã€‘
{new_lines}

Output ONLY the updated summary text.
"""

llm = ChatOpenAI(
    model=config.SMALL_LLM_MODEL_NAME,
    temperature=0.1,
    api_key=config.SILICONFLOW_API_KEY,
    base_url=config.SILICONFLOW_BASE_URL
)

async def summarizer_node(state: AgentState):
    messages = state.get("messages", [])
    current_summary = state.get("conversation_summary", "")
    
    # è·å– Session ID (ç”¨äºéš”ç¦»ä¸åŒç¾¤/ç§èŠçš„å†å²æ–‡ä»¶)
    session_key = state.get("session_id") or state.get("sender_qq")
    
    # 1. å‰ªæé€»è¾‘
    if len(messages) > MAX_HISTORY_LEN:
        to_prune = messages[:PRUNE_COUNT]
        remaining = messages[PRUNE_COUNT:]
        
        text_lines = []
        for m in to_prune:
            role = "User" if isinstance(m, HumanMessage) else "AI"
            content = m.content
            if isinstance(content, list): content = "[MultiModal/Image]"
            text_lines.append(f"{role}: {content}")
        
        input_text = "\n".join(text_lines)
        
        try:
            prompt = ChatPromptTemplate.from_template(SUMMARY_PROMPT)
            chain = prompt | llm
            response = await chain.ainvoke({
                "current_summary": current_summary if current_summary else "Start of log.",
                "new_lines": input_text
            })
            current_summary = response.content.strip()
            messages = remaining
            
        except Exception as e:
            print(f"âŒ [Summarizer Error] {e}")
    
    # 2. ä¿å­˜çŠ¶æ€
    if session_key:
        await LocalHistoryManager.save_state(messages, current_summary, session_id=session_key)
    else:
        print("âš ï¸ [Summarizer] No session_id found, history might not persist correctly.")
    
    return {
        "messages": messages,
        "conversation_summary": current_summary
    }
```

**æ€»ç»“å™¨èŠ‚ç‚¹è®¾è®¡ç‰¹ç‚¹**ï¼š
1. **å†å²ç®¡ç†**ï¼šå½“å¯¹è¯å†å²è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œè‡ªåŠ¨å¯¹æ—§æ¶ˆæ¯è¿›è¡Œæ€»ç»“
2. **æ‘˜è¦ç”Ÿæˆ**ï¼šä½¿ç”¨ LLM ç”Ÿæˆå¯¹è¯æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯
3. **ä¼šè¯éš”ç¦»**ï¼šä½¿ç”¨ Session ID éš”ç¦»ä¸åŒç¾¤èŠæˆ–ç§èŠçš„å†å²è®°å½•
4. **æŒä¹…åŒ–å­˜å‚¨**ï¼šå°†å¤„ç†åçš„å†å²æ¶ˆæ¯å’Œæ‘˜è¦ä¿å­˜åˆ°æœ¬åœ°å†å²å­˜å‚¨
5. **å®¹é”™å¤„ç†**ï¼šå¯¹ LLM è°ƒç”¨å’Œå­˜å‚¨æ“ä½œè¿›è¡Œå¼‚å¸¸æ•è·ï¼Œç¡®ä¿æµç¨‹ç¨³å®šæ€§

#### 4.12.5 ä¸»åŠ¨å¯¹è¯èŠ‚ç‚¹ (proactive_agent.py)

ä¸»åŠ¨å¯¹è¯èŠ‚ç‚¹è´Ÿè´£åˆ†æç¤¾äº¤ä¸Šä¸‹æ–‡ï¼Œæ ¹æ®ç”¨æˆ·è¡Œä¸ºã€æ²‰é»˜æ—¶é•¿å’Œå¥½æ„Ÿåº¦ç­‰å› ç´ ï¼Œå†³å®šæ˜¯å¦å‘èµ·ä¸»åŠ¨å¯¹è¯ã€‚

```python
import json
import time
from datetime import datetime
from typing import List, Any
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from app.core.state import AgentState
from app.core.config import config
from app.core.global_store import global_store
from app.memory.relation_db import relation_db
from app.core.prompts import SOCIAL_VOLITION_PROMPT
from app.utils.cache import cached_llm_invoke

llm = ChatOpenAI(
    model=config.LLM_MODEL_NAME,
    temperature=0.8,  # ç¨å¾®é«˜ä¸€ç‚¹çš„æ¸©åº¦ï¼Œè®©ä¸»åŠ¨å‘è¨€æ›´æœ‰çµæ€§
    api_key=config.SILICONFLOW_API_KEY,
    base_url=config.SILICONFLOW_BASE_URL
)


def _get_time_period(dt: datetime) -> str:
    h = dt.hour
    if 0 <= h < 5: return "æ·±å¤œ/å‡Œæ™¨"
    if 5 <= h < 9: return "æ—©æ™¨"
    if 9 <= h < 12: return "ä¸Šåˆ"
    if 12 <= h < 14: return "ä¸­åˆ"
    if 14 <= h < 18: return "ä¸‹åˆ"
    if 18 <= h < 23: return "æ™šä¸Š"
    return "æ·±å¤œ"


async def proactive_node(state: AgentState):
    """
    ä¸»åŠ¨ç¤¾äº¤å¼•æ“ (Social Volition Engine) - è§†è§‰å¢å¼ºç‰ˆ
    ç»¼åˆåˆ¤æ–­å›¾ç‰‡æ€§è´¨(å®å›¾/è¡¨æƒ…åŒ…)ã€æœ€è¿‘æ–‡æœ¬æ¶ˆæ¯ã€æ²‰é»˜æ—¶é•¿å’Œå¥½æ„Ÿåº¦ã€‚
    """
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts}] --- [Proactive] Analyzing Social Context... ---")

    # è·å–åŸºç¡€ä¸Šä¸‹æ–‡
    user_id = state.get("sender_qq", "unknown_user")
    user_display_name = state.get("sender_name", "Stranger")
    is_group = state.get("is_group", False)
    session_id = state.get("session_id")
    msgs = state.get("messages", [])

    # è®¡ç®—æ²‰é»˜æ—¶é•¿
    last_interaction_ts = state.get("last_interaction_ts", time.time())
    silence_seconds = time.time() - last_interaction_ts
    if silence_seconds < 60:  # æ²‰é»˜æ—¶é—´å°äº1åˆ†é’Ÿï¼Œä¸å‘èµ·ä¸»åŠ¨å¯¹è¯
        return {"next_step": "silent"}

    silence_str = f"{int(silence_seconds // 60)}åˆ†é’Ÿ"

    # å¤„ç†å†å²æ¶ˆæ¯
    history_str = ""
    if msgs:
        for m in msgs[-5:]:  # åªå¤„ç†æœ€è¿‘5æ¡æ¶ˆæ¯
            role = "User" if isinstance(m, HumanMessage) else "AI"
            content = m.content
            if isinstance(content, list): content = "[MultiModal/Image]"
            history_str += f"{role}: {content}\n"

    # å¤„ç†è§†è§‰ä¿¡æ¯
    visual_type = state.get("visual_type", "none")
    vision_desc = f"æœ€è¿‘å‘é€äº†{visual_type}ç±»å‹çš„å›¾ç‰‡" if visual_type != "none" else ""

    # è·å–ç”¨æˆ·å…³ç³»æ•°æ®
    profile = relation_db.get_user_profile(user_id)
    rel = profile.relationship

    # è·å–ç¯å¢ƒå’Œæƒ…ç»ªä¿¡æ¯
    emotion = global_store.get_emotion_snapshot()
    now_dt = datetime.now()
    summary = state.get("conversation_summary", "")

    # æ„å»ºç³»ç»Ÿæç¤º
    prompt = SOCIAL_VOLITION_PROMPT.format(
        now_dt=_get_time_period(now_dt),
        silence_str=silence_str,
        emotion=emotion.primary_emotion,
        user_display_name=user_display_name,
        intimacy=rel.intimacy,
        vision_desc=vision_desc,
        summary=summary,
        is_group=is_group,
    )

    try:
        # è°ƒç”¨LLMåˆ¤æ–­æ˜¯å¦éœ€è¦ä¸»åŠ¨å¯¹è¯
        response = await cached_llm_invoke(llm, [SystemMessage(content=prompt)], temperature=0.8)
        raw_content = response.content.strip()

        # è§£æå“åº”
        if "éœ€è¦ä¸»åŠ¨å¯¹è¯" in raw_content:
            # ç”Ÿæˆä¸»åŠ¨å¯¹è¯å†…å®¹
            proactive_prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆä¸€å¥è‡ªç„¶çš„ä¸»åŠ¨å¯¹è¯ï¼š\n{history_str}"
            proactive_response = await cached_llm_invoke(llm, [SystemMessage(content=proactive_prompt)],
                                                         temperature=0.8)

            return {
                "messages": msgs + [AIMessage(content=proactive_response.content.strip())],
                "next_step": "save"
            }
    except Exception as e:
        print(f"[{ts}]âŒ [Proactive] Error: {e}")

    return {"next_step": "silent"}
```

**ä¸»åŠ¨å¯¹è¯èŠ‚ç‚¹è®¾è®¡ç‰¹ç‚¹**ï¼š
1. **ç¤¾äº¤ä¸Šä¸‹æ–‡åˆ†æ**ï¼šç»¼åˆåˆ†æç”¨æˆ·è¡Œä¸ºã€æ²‰é»˜æ—¶é•¿ã€å¥½æ„Ÿåº¦å’Œè§†è§‰ä¿¡æ¯
2. **æ™ºèƒ½è§¦å‘æ¡ä»¶**ï¼šæ ¹æ®æ²‰é»˜æ—¶é•¿ç­‰å› ç´ å†³å®šæ˜¯å¦å‘èµ·ä¸»åŠ¨å¯¹è¯
3. **ä¸ªæ€§åŒ–å¯¹è¯**ï¼šç»“åˆç”¨æˆ·å…³ç³»ã€å†å²æ¶ˆæ¯å’Œæƒ…ç»ªçŠ¶æ€ç”Ÿæˆä¸ªæ€§åŒ–çš„ä¸»åŠ¨å¯¹è¯
4. **å¤šåœºæ™¯é€‚é…**ï¼šæ”¯æŒç§èŠå’Œç¾¤èŠåœºæ™¯çš„ä¸»åŠ¨å¯¹è¯ç”Ÿæˆ
5. **è§†è§‰å¢å¼º**ï¼šè€ƒè™‘ç”¨æˆ·å‘é€çš„å›¾ç‰‡ç±»å‹ï¼Œå¢å¼ºä¸»åŠ¨å¯¹è¯çš„ç›¸å…³æ€§
6. **å®¹é”™å¤„ç†**ï¼šå¯¹ LLM è°ƒç”¨è¿‡ç¨‹è¿›è¡Œå¼‚å¸¸æ•è·ï¼Œç¡®ä¿æµç¨‹ç¨³å®šæ€§


    
    async def get(self, messages: List[BaseMessage], model: str, temperature: float) -> Optional[Any]:
        """
        ä»ç¼“å­˜ä¸­è·å–LLMè°ƒç”¨ç»“æœ
        
        Args:
            messages: LLMè°ƒç”¨çš„è¾“å…¥æ¶ˆæ¯åˆ—è¡¨
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            temperature: æ¨¡å‹çš„æ¸©åº¦å‚æ•°
            
        Returns:
            ç¼“å­˜çš„LLMå“åº”ï¼Œå¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–å·²è¿‡æœŸåˆ™è¿”å›None
        """
        cache_key = self._generate_key(messages, model, temperature)
        
        async with self.lock:
            if cache_key in self.cache:
                value, expire_time = self.cache[cache_key]
                if datetime.now() < expire_time:
                    return value
                else:
                    # ç¼“å­˜å·²è¿‡æœŸï¼Œåˆ é™¤
                    del self.cache[cache_key]
            return None
    
    async def set(self, messages: List[BaseMessage], model: str, temperature: float, value: Any, ttl: Optional[int] = None) -> None:
        """
        å°†LLMè°ƒç”¨ç»“æœå­˜å…¥ç¼“å­˜
        
        Args:
            messages: LLMè°ƒç”¨çš„è¾“å…¥æ¶ˆæ¯åˆ—è¡¨
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            temperature: æ¨¡å‹çš„æ¸©åº¦å‚æ•°
            value: LLMçš„å“åº”ç»“æœ
            ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        """
        cache_key = self._generate_key(messages, model, temperature)
        expire_time = datetime.now() + timedelta(seconds=ttl or self.default_ttl)
        
        async with self.lock:
            # æ£€æŸ¥ç¼“å­˜å¤§å°ï¼Œå¦‚æœè¶…è¿‡æœ€å¤§å€¼åˆ™æ¸…ç†æœ€æ—§çš„æ¡ç›®
            if len(self.cache) >= self.max_size:
                # æŒ‰è¿‡æœŸæ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—©è¿‡æœŸçš„æ¡ç›®
                oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k][1])
                del self.cache[oldest_key]
            
            self.cache[cache_key] = (value, expire_time)
    
    async def clear(self) -> None:
        """
        æ¸…ç©ºç¼“å­˜
        """
        async with self.lock:
            self.cache.clear()
    
    async def remove_expired(self) -> int:
        """
        æ¸…ç†æ‰€æœ‰è¿‡æœŸçš„ç¼“å­˜æ¡ç›®
        
        Returns:
            æ¸…ç†çš„è¿‡æœŸæ¡ç›®æ•°é‡
        """
        now = datetime.now()
        expired_keys = []
        
        async with self.lock:
            for key, (_, expire_time) in self.cache.items():
                if now >= expire_time:
                    expired_keys.append(key)
            
            for key in expired_keys:
                del self.cache[key]
            
            return len(expired_keys)
    
    async def get_stats(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        now = datetime.now()
        total = len(self.cache)
        expired = 0
        size_bytes = 0
        
        async with self.lock:
            for _, (value, expire_time) in self.cache.items():
                if now >= expire_time:
                    expired += 1
                
                # ä¼°ç®—ç¼“å­˜å¤§å°
                value_str = str(value)
                size_bytes += len(value_str.encode('utf-8'))
        
        return {
            "total_entries": total,
            "expired_entries": expired,
            "size_bytes": size_bytes,
            "size_mb": round(size_bytes / (1024 * 1024), 2),
            "max_size": self.max_size,
            "default_ttl": self.default_ttl
        }


class LLMRequestQueue:
    """
    LLMè¯·æ±‚é˜Ÿåˆ—ç³»ç»Ÿ
    ç”¨äºç®¡ç†LLMè°ƒç”¨è¯·æ±‚ï¼Œæ§åˆ¶å¹¶å‘æ•°ï¼Œé˜²æ­¢è¯·æ±‚å †ç§¯å’Œè¶…æ—¶
    """
    
    def __init__(self, max_concurrent: int = 5, timeout: int = 30):
        """
        åˆå§‹åŒ–è¯·æ±‚é˜Ÿåˆ—
        
        Args:
            max_concurrent: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.max_concurrent = max_concurrent
        self.timeout = timeout
        self.queue: Deque = deque()
        self.current_concurrent = 0
        self.lock = asyncio.Lock()
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def add_request(self, llm: Any, messages: List[BaseMessage], temperature: float = 0.7) -> Any:
        """
        æ·»åŠ LLMè¯·æ±‚åˆ°é˜Ÿåˆ—å¹¶ç­‰å¾…ç»“æœ
        
        Args:
            llm: LLMå®ä¾‹
            messages: è¾“å…¥æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            LLMå“åº”ç»“æœ
        """
        async with self.semaphore:
            try:
                # ä½¿ç”¨asyncio.wait_forè®¾ç½®è¯·æ±‚è¶…æ—¶
                result = await asyncio.wait_for(
                    llm.ainvoke(messages),
                    timeout=self.timeout
                )
                return result
            except asyncio.TimeoutError:
                logger.error(f"LLMè¯·æ±‚è¶…æ—¶ï¼Œå·²è¶…è¿‡{self.timeout}ç§’")
                raise
            except Exception as e:
                logger.error(f"LLMè¯·æ±‚æ‰§è¡Œå‡ºé”™: {str(e)}")
                raise
    
    async def get_stats(self) -> Dict[str, Any]:
        """
        è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        async with self.lock:
            return {
                "queue_length": len(self.queue),
                "current_concurrent": self.current_concurrent,
                "max_concurrent": self.max_concurrent,
                "timeout": self.timeout
            }


# å…¨å±€ç¼“å­˜å®ä¾‹
llm_cache = LLMCache(max_size=500, default_ttl=7200)  # ç¼“å­˜500æ¡ï¼Œé»˜è®¤è¿‡æœŸæ—¶é—´2å°æ—¶

# å…¨å±€è¯·æ±‚é˜Ÿåˆ—å®ä¾‹
llm_queue = LLMRequestQueue(max_concurrent=3, timeout=60)  # æœ€å¤§3ä¸ªå¹¶å‘è¯·æ±‚ï¼Œè¶…æ—¶60ç§’


async def cached_llm_invoke(llm: Any, messages: List[BaseMessage], temperature: float = 0.7, max_retries: int = 2) -> Any:
    """
    å¸¦ç¼“å­˜å’Œé”™è¯¯å¤„ç†çš„LLMè°ƒç”¨å‡½æ•°
    
    Args:
        llm: LLMå®ä¾‹
        messages: è¾“å…¥æ¶ˆæ¯åˆ—è¡¨
        temperature: æ¸©åº¦å‚æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        LLMå“åº”ç»“æœï¼ˆå¯èƒ½æ¥è‡ªç¼“å­˜ï¼‰
    
    Raises:
        Exception: å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºæœ€ç»ˆå¼‚å¸¸
    """
    # è·å–æ¨¡å‹åç§°
    model = getattr(llm, "model", "unknown")
    
    # å°è¯•ä»ç¼“å­˜è·å–
    cached_result = await llm_cache.get(messages, model, temperature)
    if cached_result:
        logger.debug(f"LLMè°ƒç”¨ç¼“å­˜å‘½ä¸­ï¼Œæ¨¡å‹: {model}")
        return cached_result
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œå°è¯•è°ƒç”¨LLM
    retry_count = 0
    last_exception = None
    
    while retry_count <= max_retries:
        try:
            logger.debug(f"LLMè°ƒç”¨ç¼“å­˜æœªå‘½ä¸­ï¼Œå°è¯•è°ƒç”¨ï¼Œæ¨¡å‹: {model}, é‡è¯•æ¬¡æ•°: {retry_count}")
            
            # é€šè¿‡è¯·æ±‚é˜Ÿåˆ—è°ƒç”¨LLM
            result = await llm_queue.add_request(llm, messages, temperature)
            
            # å°†ç»“æœå­˜å…¥ç¼“å­˜
            await llm_cache.set(messages, model, temperature, result)
            
            logger.debug(f"LLMè°ƒç”¨æˆåŠŸï¼Œæ¨¡å‹: {model}")
            return result
            
        except asyncio.TimeoutError as e:
            last_exception = e
            retry_count += 1
            logger.warning(f"LLMè°ƒç”¨è¶…æ—¶ï¼Œå°†è¿›è¡Œç¬¬{retry_count}æ¬¡é‡è¯•: {str(e)}")
            
        except (ConnectionError, BrokenPipeError, OSError) as e:
            last_exception = e
            retry_count += 1
            logger.warning(f"LLMè°ƒç”¨è¿æ¥é”™è¯¯ï¼Œå°†è¿›è¡Œç¬¬{retry_count}æ¬¡é‡è¯•: {str(e)}")
            
        except Exception as e:
            # å…¶ä»–å¼‚å¸¸ï¼Œä¸é‡è¯•
            logger.error(f"LLMè°ƒç”¨å‘ç”Ÿéé‡è¯•å¼‚å¸¸: {str(e)}")
            raise
        
        # é‡è¯•å‰ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œé¿å…ç«‹å³é‡è¯•
        if retry_count <= max_retries:
            await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
    
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    logger.error(f"LLMè°ƒç”¨åœ¨{max_retries+1}æ¬¡å°è¯•åå¤±è´¥: {str(last_exception)}")
    raise last_exception
```

**ç¼“å­˜ç®¡ç†æ¨¡å—è®¾è®¡äº®ç‚¹**ï¼š
1. **é«˜æ•ˆçš„ç¼“å­˜é”®ç”Ÿæˆ**ï¼šåŸºäºæ¶ˆæ¯å†…å®¹ã€æ¨¡å‹åç§°å’Œæ¸©åº¦å‚æ•°ç”Ÿæˆå”¯ä¸€ç¼“å­˜é”®ï¼Œç¡®ä¿ç¼“å­˜çš„å‡†ç¡®æ€§
2. **è‡ªåŠ¨è¿‡æœŸå’Œå¤§å°æ§åˆ¶**ï¼šå®ç°äº†TTLè¿‡æœŸæœºåˆ¶å’Œæœ€å¤§ç¼“å­˜å¤§å°é™åˆ¶ï¼Œé¿å…ç¼“å­˜æ— é™å¢é•¿
3. **å¹¶å‘å®‰å…¨è®¾è®¡**ï¼šä½¿ç”¨asyncio.Lockç¡®ä¿å¼‚æ­¥ç¯å¢ƒä¸‹çš„çº¿ç¨‹å®‰å…¨
4. **è¯·æ±‚é˜Ÿåˆ—ç®¡ç†**ï¼šé€šè¿‡semaphoreæ§åˆ¶å¹¶å‘è¯·æ±‚æ•°ï¼Œé˜²æ­¢ç³»ç»Ÿè¿‡è½½
5. **å®Œæ•´çš„é”™è¯¯å¤„ç†**ï¼šæ”¯æŒè¯·æ±‚è¶…æ—¶ã€è¿æ¥é”™è¯¯ç­‰å¼‚å¸¸çš„å¤„ç†å’Œé‡è¯•æœºåˆ¶
6. **ç»Ÿè®¡åŠŸèƒ½**ï¼šæä¾›ç¼“å­˜å’Œé˜Ÿåˆ—çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œä¾¿äºç›‘æ§å’Œè°ƒä¼˜
7. **ç»Ÿä¸€è°ƒç”¨æ¥å£**ï¼šé€šè¿‡cached_llm_invokeå‡½æ•°æä¾›ä¸€ç«™å¼çš„ç¼“å­˜+é˜Ÿåˆ—è°ƒç”¨æœåŠ¡

### 4.13 QQå·¥å…·æ¨¡å— (app/utils/qq_utils.py)

QQå·¥å…·æ¨¡å—ç”¨äºè§£æOneBot v11æ¶ˆæ¯æ®µï¼Œä¸“æ³¨äºQQå®˜æ–¹è¡¨æƒ…/è´´çº¸çš„è¯†åˆ«å’Œå¤„ç†ï¼Œæ”¯æŒå¤šç§æ¶ˆæ¯ç±»å‹çš„è§£æå’Œæ ¼å¼åŒ–ã€‚

```python
"""qq_utils.py

Utilities for parsing OneBot v11 message segments from QQ.

This module focuses on *robust recognition* of QQ official emoticons/stickers.
It supports:
  - `face` (QQ built-in/system faces)
  - `mface` (QQ sticker store / marketplace emoticons)
  - `image` segments converted from `mface`
  - `dice` / `rps` / `poke` (NapCatQQ and some OneBot implementations)

If an emoticon cannot be mapped to a human-readable name, we still keep it as
`[Face:<id>]` instead of silently dropping it.
"""

from __future__ import annotations

import os
import json
from typing import Any, Dict, List, Optional, Tuple


# ---------------------------------------------------------------------------
# Face mapping
# ---------------------------------------------------------------------------
#
# IMPORTANT:
# - Different QQ implementations historically used *different* face-id tables.
# - Recent desktop QQ / QQNT ecosystems commonly follow the "system emoji id"
#   table used by the official QQ Bot (OpenAPI) documentation.
# - Some legacy CQ/CoolQ ecosystems use another classic CQ face-id table.
#
# To be maximally compatible, we:
#   1) First try to extract a readable name from `data.raw` if present.
#   2) Then use OFFICIAL mapping (QQ Bot doc table) as the default.
#   3) Finally fall back to a LEGACY mapping (classic CQ) if the id isn't in OFFICIAL.
#
# You can override the mapping preference via env var:
#   QQ_FACE_MAP_MODE = "official" | "legacy" | "auto" (default: "official")
#
# Optional:
#   QQ_FACE_MAPPING_FILE = /path/to/qq_face_mapping.json
#   (JSON: {"id": "name", ...}) will override built-in mappings.


QQ_FACE_MAP_MODE = os.getenv("QQ_FACE_MAP_MODE", "official").strip().lower()

# Optional user-provided mapping file (JSON: {"id": "name", ...}).
# This is useful if you want to extend/override the built-in mapping without
# editing code again.
QQ_FACE_MAPPING_USER: Dict[str, str] = {}
_user_map_path = os.getenv("QQ_FACE_MAPPING_FILE")
if _user_map_path:
    try:
        with open(_user_map_path, "r", encoding="utf-8") as f:
            _obj = json.load(f)
        if isinstance(_obj, dict):
            QQ_FACE_MAPPING_USER = {str(k): str(v) for k, v in _obj.items() if v is not None}
    except Exception:
        # Keep silent: mapping file is optional.
        QQ_FACE_MAPPING_USER = {}


# Legacy mapping (classic CQ/CoolQ-style) for the early QQ built-in faces.
#
# This table is widely used in older CQHTTP/go-cqhttp ecosystems.
# If your environment still follows the classic CQ ids, set:
#   QQ_FACE_MAP_MODE=legacy
#
# NOTE: some ids overlap with the newer official table but map to different
# names. That's why we keep an explicit mode switch.
QQ_FACE_MAPPING_LEGACY: Dict[str, str] = {
    "0": "æƒŠè®¶",
    "1": "æ’‡å˜´",
    "2": "è‰²",
    "3": "å‘å‘†",
    "4": "å¾—æ„",
    "5": "æµæ³ª",
    "6": "å®³ç¾",
    "7": "é—­å˜´",
    "8": "ç¡",
    "9": "å¤§å“­",
    "10": "å°´å°¬",
    "11": "å‘æ€’",
    "12": "è°ƒçš®",
    "13": "å‘²ç‰™",
    "14": "å¾®ç¬‘",
    "15": "éš¾è¿‡",
    "16": "é…·",
    "17": "æŠ“ç‹‚",
    "18": "å",
    "19": "å·ç¬‘",
    "20": "å¯çˆ±",
    "21": "ç™½çœ¼",
    "22": "å‚²æ…¢",
    "23": "é¥¥é¥¿",
    "24": "å›°",
    "25": "æƒŠæ",
    "26": "æµæ±—",
    "27": "æ†¨ç¬‘",
    "28": "æ‚ é—²",
    "29": "å¥‹æ–—",
    "30": "å’’éª‚",
    "31": "ç–‘é—®",
    "32": "å˜˜",
    "33": "æ™•",
    "34": "æŠ˜ç£¨",
    "35": "è¡°",
    "36": "éª·é«…",
    "37": "æ•²æ‰“",
    "38": "å†è§",
    "39": "æ“¦æ±—",
    "40": "æŠ é¼»",
    "41": "é¼“æŒ",
    "42": "ç³—å¤§äº†",
    "43": "åç¬‘",
    "44": "å·¦å“¼å“¼",
    "45": "å³å“¼å“¼",
    "46": "å“ˆæ¬ ",
    "47": "é„™è§†",
    "48": "å§”å±ˆ",
    "49": "å¿«å“­äº†",
    "50": "é˜´é™©",
    "51": "äº²äº²",
    "52": "å“",
    "53": "å¯æ€œ",
    "54": "èœåˆ€",
    "55": "è¥¿ç“œ",
    "56": "å•¤é…’",
    "57": "ç¯®çƒ",
    "58": "ä¹’ä¹“",
    "59": "å’–å•¡",
    "60": "é¥­",
    "61": "çŒªå¤´",
    "62": "ç«ç‘°",
    "63": "å‡‹è°¢",
    "64": "å˜´å”‡",
    "65": "çˆ±å¿ƒ",
    "66": "å¿ƒç¢",
    "67": "è›‹ç³•",
    "68": "é—ªç”µ",
    "69": "ç‚¸å¼¹",
    "70": "åˆ€",
    "71": "è¶³çƒ",
    "72": "ç“¢è™«",
    "73": "ä¾¿ä¾¿",
    "74": "æœˆäº®",
    "75": "å¤ªé˜³",
    "76": "ç¤¼ç‰©",
    "77": "æ‹¥æŠ±",
    "78": "å¼º",
    "79": "å¼±",
    "80": "æ¡æ‰‹",
    "81": "èƒœåˆ©",
    "82": "æŠ±æ‹³",
    "83": "å‹¾å¼•",
    "84": "æ‹³å¤´",
    "85": "å·®åŠ²",
    "86": "çˆ±ä½ ",
    "87": "NO",
    "88": "OK",
    "89": "çˆ±æƒ…",
    "90": "é£å»",
    "91": "è·³è·³",
    "92": "å‘æŠ–",
    "93": "æ€„ç«",
    "94": "è½¬åœˆ",
    "95": "ç£•å¤´",
    "96": "å›å¤´",
    "97": "è·³ç»³",
    "98": "æŠ•é™",
    "99": "æ¿€åŠ¨",
    "100": "ä¹±èˆ",
    "101": "çŒ®å»",
    "102": "å·¦å¤ªæ",
    "103": "å³å¤ªæ",
}


# "Official" system emoji mapping (EmojiType=1) from QQ Bot OpenAPI docs.
# Note: the official docs themselves state the list is partial and may change
# over time.
QQ_FACE_MAPPING_OFFICIAL: Dict[str, str] = {
    # Basic
    "4": "å¾—æ„",
    "5": "æµæ³ª",
    "8": "ç¡",
    "9": "å¤§å“­",
    "10": "å°´å°¬",
    "12": "è°ƒçš®",
    "14": "å¾®ç¬‘",
    "16": "é…·",
    "21": "å¯çˆ±",
    "23": "å‚²æ…¢",
    "24": "é¥¥é¥¿",
    "25": "å›°",
    "26": "æƒŠæ",
    "27": "æµæ±—",
    "28": "æ†¨ç¬‘",
    "29": "æ‚ é—²",
    "30": "å¥‹æ–—",
    "32": "ç–‘é—®",
    "33": "å˜˜",
    "34": "æ™•",
    "38": "æ•²æ‰“",
    "39": "å†è§",
    "41": "å‘æŠ–",
    "42": "çˆ±æƒ…",
    "43": "è·³è·³",
    "49": "æ‹¥æŠ±",
    "53": "è›‹ç³•",
    "60": "å’–å•¡",
    "63": "ç«ç‘°",
    "66": "çˆ±å¿ƒ",
    "74": "å¤ªé˜³",
    "75": "æœˆäº®",
    "76": "èµ",
    "78": "æ¡æ‰‹",
    "79": "èƒœåˆ©",
    "85": "é£å»",
    "89": "è¥¿ç“œ",
    "96": "å†·æ±—",
    "97": "æ“¦æ±—",
    "98": "æŠ é¼»",
    "99": "é¼“æŒ",
    "100": "ç³—å¤§äº†",
    "101": "åç¬‘",
    "102": "å·¦å“¼å“¼",
    "103": "å³å“¼å“¼",
    "104": "å“ˆæ¬ ",
    "106": "å§”å±ˆ",
    "109": "å·¦äº²äº²",
    "111": "å¯æ€œ",
    "116": "ç¤ºçˆ±",
    "118": "æŠ±æ‹³",
    "120": "æ‹³å¤´",
    "122": "çˆ±ä½ ",
    "123": "NO",
    "124": "OK",
    "125": "è½¬åœˆ",
    "129": "æŒ¥æ‰‹",
    "144": "å–å½©",
    "147": "æ£’æ£’ç³–",
    # Newer system emojis
    "171": "èŒ¶",
    "173": "æ³ªå¥”",
    "174": "æ— å¥ˆ",
    "175": "å–èŒ",
    "176": "å°çº ç»“",
    "179": "doge",
    "180": "æƒŠå–œ",
    "181": "éªšæ‰°",
    "182": "ç¬‘å“­",
    "183": "æˆ‘æœ€ç¾",
    "201": "ç‚¹èµ",
    "203": "æ‰˜è„¸",
    "212": "æ‰˜è…®",
    "214": "å•µå•µ",
    "219": "è¹­ä¸€è¹­",
    "222": "æŠ±æŠ±",
    "227": "æ‹æ‰‹",
    "232": "ä½›ç³»",
    "240": "å–·è„¸",
    "243": "ç”©å¤´",
    "246": "åŠ æ²¹æŠ±æŠ±",
    "262": "è„‘é˜”ç–¼",
    "264": "æ‚è„¸",
    "265": "è¾£çœ¼ç›",
    "266": "å“¦å“Ÿ",
    "267": "å¤´ç§ƒ",
    "268": "é—®å·è„¸",
    "269": "æš—ä¸­è§‚å¯Ÿ",
    "270": "emm",
    "271": "åƒç“œ",
    "272": "å‘µå‘µå“’",
    "273": "æˆ‘é…¸äº†",
    "277": "æ±ªæ±ª",
    "278": "æ±—",
    "281": "æ— çœ¼ç¬‘",
    "282": "æ•¬ç¤¼",
    "284": "é¢æ— è¡¨æƒ…",
    "285": "æ‘¸é±¼",
    "287": "å“¦",
    "289": "ççœ¼",
    "290": "æ•²å¼€å¿ƒ",
    "293": "æ‘¸é”¦é²¤",
    "294": "æœŸå¾…",
    "297": "æ‹œè°¢",
    "298": "å…ƒå®",
    "299": "ç‰›å•Š",
    "305": "å³äº²äº²",
    "306": "ç‰›æ°”å†²å¤©",
    "307": "å–µå–µ",
    "314": "ä»”ç»†åˆ†æ",
    "315": "åŠ æ²¹",
    "318": "å´‡æ‹œ",
    "319": "æ¯”å¿ƒ",
    "320": "åº†ç¥",
    "322": "æ‹’ç»",
    "324": "åƒç³–",
    "326": "ç”Ÿæ°”",
}


# Known special face IDs used by some OneBot/CQ implementations.
# These are not always delivered as `face` by every implementation.
SPECIAL_FACE_IDS: Dict[str, str] = {
    "358": "éª°å­",
    "359": "çŒœæ‹³",
}



def _strip_brackets(s: str) -> str:
    s = s.strip()
    if len(s) >= 2:
        if (s.startswith("[") and s.endswith("]")) or (s.startswith("ã€") and s.endswith("ã€‘")):
            s = s[1:-1].strip()
    return s



def _maybe_face_name(text: str) -> Optional[str]:
    """Heuristic: decide whether a string looks like a face name."""
    if not isinstance(text, str):
        return None
    t = _strip_brackets(text)
    if not t:
        return None
    # Too long is likely not a face name.
    if len(t) > 20:
        return None
    # Filter out strings that are purely digits/punctuation.
    if all(ch.isdigit() or ch in "-_:," for ch in t):
        return None
    return t



def _extract_face_desc_from_raw(raw: Any) -> Optional[str]:
    """Try to extract a readable face name from `data.raw`.

    NapCatQQ documents `raw` as the original face payload (optional). In many
    deployments this contains the original face text.
    """
    if raw is None:
        return None

    # Direct string
    if isinstance(raw, str):
        return _maybe_face_name(raw)

    # Dict: try common fields first
    if isinstance(raw, dict):
        for k in ("text", "faceText", "face_text", "name", "desc", "summary"):
            if k in raw:
                v = raw.get(k)
                if isinstance(v, str):
                    cand = _maybe_face_name(v)
                    if cand:
                        return cand

        # Recursively search for the first plausible string
        for v in raw.values():
            cand = _extract_face_desc_from_raw(v)
            if cand:
                return cand

    # List/tuple: recurse
    if isinstance(raw, (list, tuple)):
        for v in raw:
            cand = _extract_face_desc_from_raw(v)
            if cand:
                return cand

    return None



def _resolve_face_desc(face_id: str, raw: Any = None) -> Optional[str]:
    """Resolve a face id into a human-readable description."""
    # 0) Specials (dice/rps)
    if face_id in SPECIAL_FACE_IDS:
        return SPECIAL_FACE_IDS[face_id]

    # 1) Raw payload (most reliable)
    raw_desc = _extract_face_desc_from_raw(raw)
    if raw_desc:
        return raw_desc

    # 2) User mapping (highest priority among tables)
    if face_id in QQ_FACE_MAPPING_USER:
        return QQ_FACE_MAPPING_USER.get(face_id)

    mode = QQ_FACE_MAP_MODE
    if mode not in {"official", "legacy", "auto"}:
        mode = "official"

    # 3) Mapping tables
    if mode == "legacy":
        return QQ_FACE_MAPPING_LEGACY.get(face_id) or QQ_FACE_MAPPING_OFFICIAL.get(face_id)
    if mode == "auto":
        # Try official first (latest desktop QQ), then legacy.
        return QQ_FACE_MAPPING_OFFICIAL.get(face_id) or QQ_FACE_MAPPING_LEGACY.get(face_id)
    # default: official
    return QQ_FACE_MAPPING_OFFICIAL.get(face_id) or QQ_FACE_MAPPING_LEGACY.get(face_id)



def _format_mface(data: dict) -> str:
    """Format a QQ marketplace emoticon (mface) into text."""
    summary = data.get("summary")
    emoji_id = data.get("emoji_id")
    pkg_id = data.get("emoji_package_id")

    label = None
    if isinstance(summary, str) and summary.strip():
        label = summary.strip()
    elif emoji_id:
        label = f"emoji_id={emoji_id}"
    else:
        label = "mface"

    # Keep package id if present (helps disambiguate)
    if pkg_id:
        return f" [å•†åŸè¡¨æƒ…:{label}; pkg={pkg_id}] "
    return f" [å•†åŸè¡¨æƒ…:{label}] "



def _format_rps_result(result: Any) -> str:
    """Rock-paper-scissors result mapping: 1=rock, 2=scissors, 3=paper."""
    try:
        r = int(str(result))
    except Exception:
        return str(result)

    return {1: "çŸ³å¤´", 2: "å‰ªåˆ€", 3: "å¸ƒ"}.get(r, str(result))



def parse_onebot_array_msg(message_data: list | dict) -> Tuple[str, List[str], Optional[str]]:
    """Parse OneBot v11 message array.

    Returns:
        (plain_text, image_urls, reply_message_id)

    Notes:
        - QQ marketplace emoticons (mface) are often converted to `image` on
          receive. We detect those by `emoji_id` / `emoji_package_id` / `key`.
        - Unknown segment types are preserved as placeholders instead of being
          dropped.
    """

    text_content: str = ""
    image_urls: List[str] = []
    reply_id: Optional[str] = None

    # Compat: if a single dict is provided, wrap it
    if isinstance(message_data, dict):
        message_data = [message_data]

    if not isinstance(message_data, list):
        return "", [], None

    for segment in message_data:
        if not isinstance(segment, dict):
            continue

        msg_type = segment.get("type")
        data = segment.get("data") or {}
        if not isinstance(data, dict):
            data = {}

        # ---------------------------
        # Text
        # ---------------------------
        if msg_type == "text":
            text_content += str(data.get("text", ""))
            continue

        # ---------------------------
        # Image (including mface->image)
        # ---------------------------
        if msg_type == "image":
            url = data.get("url")
            if isinstance(url, str) and url.strip():
                image_urls.append(url.strip())

            # If it looks like an mface converted into image, prefer sticker label.
            if any(k in data for k in ("emoji_id", "emoji_package_id", "key")):
                summary = data.get("summary")
                label = None
                if isinstance(summary, str) and summary.strip():
                    label = summary.strip()
                elif data.get("emoji_id"):
                    label = f"emoji_id={data.get('emoji_id')}"
                else:
                    label = "mface"
                text_content += f" [å•†åŸè¡¨æƒ…:{label}] "
            else:
                text_content += " [å›¾ç‰‡] "
            continue

        # ---------------------------
        # QQ system face
        # ---------------------------
        if msg_type == "face":
            face_id = str(data.get("id", "")).strip()

            # Some implementations may deliver dice/rps as face+resultId.
            if face_id in {"358", "359"} and data.get("resultId") is not None:
                if face_id == "358":
                    text_content += f" [éª°å­:{data.get('resultId')}] "
                else:
                    text_content += f" [çŒœæ‹³:{_format_rps_result(data.get('resultId'))}] "
                continue

            face_desc = _resolve_face_desc(face_id, raw=data.get("raw"))
            if face_desc:
                text_content += f" [è¡¨æƒ…:{face_desc}] "
            else:
                text_content += f" [Face:{face_id}] "
            continue

        # ---------------------------
        # Marketplace face (NapCat: send type; receive often becomes image)
        # ---------------------------
        if msg_type == "mface":
            text_content += _format_mface(data)
            continue

        # ---------------------------
        # Dice / RPS / Poke
        # ---------------------------
        if msg_type == "dice":
            text_content += f" [éª°å­:{data.get('result', '')}] "
            continue

        if msg_type == "rps":
            text_content += f" [çŒœæ‹³:{_format_rps_result(data.get('result'))}] "
            continue

        if msg_type == "poke":
            poke_type = data.get("type")
            poke_id = data.get("id")
            text_content += f" [æˆ³ä¸€æˆ³:type={poke_type}, id={poke_id}] "
            continue

        # ---------------------------
        # Mentions / reply
        # ---------------------------
        if msg_type == "at":
            qq = data.get("qq")
            text_content += f"[Mention:{qq}]"
            continue

        if msg_type == "reply":
            reply_id = str(data.get("id")) if data.get("id") is not None else None
            continue

        # ---------------------------
        # Other common message types
        # ---------------------------
        if msg_type == "record":
            text_content += " [è¯­éŸ³æ¶ˆæ¯] "
            continue

        if msg_type == "video":
            text_content += " [è§†é¢‘æ¶ˆæ¯] "
            continue

        if msg_type == "file":
            name = data.get("name") or data.get("file") or "file"
            text_content += f" [æ–‡ä»¶:{name}] "
            continue

        if msg_type == "json":
            text_content += " [å¡ç‰‡æ¶ˆæ¯/å°ç¨‹åº] "
            continue

        if msg_type == "xml":
            text_content += " [XMLæ¶ˆæ¯] "
            continue

        # Fallback: preserve unknown segment types
        if msg_type:
            text_content += f" [{msg_type}] "

    return text_content.strip(), image_urls, reply_id
```

**QQå·¥å…·æ¨¡å—è®¾è®¡äº®ç‚¹**ï¼š
1. **å¥å£®çš„è¡¨æƒ…è¯†åˆ«**ï¼šæ”¯æŒå¤šç§QQè¡¨æƒ…ç±»å‹ï¼ˆå†…ç½®è¡¨æƒ…ã€å•†åŸè¡¨æƒ…ã€å›¾ç‰‡è¡¨æƒ…ç­‰ï¼‰çš„è¯†åˆ«å’Œè½¬æ¢
2. **çµæ´»çš„æ˜ å°„æ¨¡å¼**ï¼šæ”¯æŒå®˜æ–¹ã€ä¼ ç»Ÿå’Œè‡ªåŠ¨ä¸‰ç§è¡¨æƒ…IDæ˜ å°„æ¨¡å¼ï¼Œé€‚åº”ä¸åŒçš„QQå®ç°
3. **ç”¨æˆ·å¯æ‰©å±•**ï¼šå…è®¸é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šè‡ªå®šä¹‰è¡¨æƒ…æ˜ å°„æ–‡ä»¶ï¼Œæ–¹ä¾¿æ‰©å±•
4. **å®Œæ•´çš„æ¶ˆæ¯è§£æ**ï¼šæ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æƒ…ã€éª°å­ã€çŒœæ‹³ã€æˆ³ä¸€æˆ³ç­‰å¤šç§æ¶ˆæ¯ç±»å‹çš„è§£æ
5. **ä¿¡æ¯ä¿ç•™**ï¼šå¯¹äºæ— æ³•è¯†åˆ«çš„æ¶ˆæ¯æ®µï¼Œä»¥å ä½ç¬¦å½¢å¼ä¿ç•™ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±
6. **å…¼å®¹æ€§è®¾è®¡**ï¼šè€ƒè™‘äº†ä¸åŒOneBotå®ç°çš„å·®å¼‚ï¼Œæä¾›äº†å…¼å®¹å¤„ç†
7. **æ¸…æ™°çš„è¿”å›ç»“æ„**ï¼šè¿”å›æ–‡æœ¬å†…å®¹ã€å›¾ç‰‡URLåˆ—è¡¨å’Œå›å¤æ¶ˆæ¯IDï¼Œä¾¿äºä¸Šå±‚åº”ç”¨ä½¿ç”¨



## 5. æ ¸å¿ƒåŠŸèƒ½å®ç°

### 5.1 æ¶ˆæ¯å¤„ç†æµç¨‹
1. **æ¶ˆæ¯æ¥æ”¶**: QQ æœåŠ¡å™¨é€šè¿‡ WebSocket æ¥æ”¶æ¶ˆæ¯
2. **æ¶ˆæ¯è§£æ**: è§£ææ¶ˆæ¯å†…å®¹ã€å›¾ç‰‡ã€@æåŠç­‰ä¿¡æ¯
3. **ä¸Šä¸‹æ–‡è¿‡æ»¤**: åˆ¤æ–­æ˜¯å¦éœ€è¦å›å¤
4. **å¹¶è¡Œå¤„ç†**: åŒæ—¶è¿›è¡Œè§†è§‰æ„ŸçŸ¥å’Œå¿ƒç†åˆ†æ
5. **æ™ºèƒ½ä½“å“åº”**: ç”Ÿæˆå›å¤å†…å®¹
6. **å·¥å…·è°ƒç”¨**: å¦‚æœ‰éœ€è¦ï¼Œè°ƒç”¨å¤–éƒ¨å·¥å…·
7. **è®°å¿†ä¿å­˜**: å°†äº¤äº’å†…å®¹ä¿å­˜åˆ°é•¿æœŸè®°å¿†
8. **æ¶ˆæ¯å‘é€**: å°†å›å¤å‘é€å› QQ å¹³å°

### 5.2 ä¸Šä¸‹æ–‡è¿‡æ»¤é€»è¾‘

#### 5.2.1 é€šç”¨è¿‡æ»¤è§„åˆ™
- å¯¹è¯ç»“æŸè¯­å¥ï¼ˆ"Ok", "Thanks", "Bye" ç­‰ï¼‰
- æ— æ„ä¹‰å†…å®¹ï¼ˆçº¯è¡¨æƒ…ã€ç®€å•ååº”ï¼‰
- å¥å­ç‰‡æ®µ
- é‡å¤å‘é€çš„æ¶ˆæ¯
- è¯é¢˜è€—å°½

#### 5.2.2 ç§èŠåœºæ™¯
- é»˜è®¤å†³ç­–ï¼šå›å¤
- åªè¦ä¸è§¦å‘é€šç”¨è¿‡æ»¤è§„åˆ™ï¼Œå°±ä¼šå›å¤

#### 5.2.3 ç¾¤èŠåœºæ™¯
- é»˜è®¤å†³ç­–ï¼šä¸å›å¤
- å›å¤æ¡ä»¶ï¼š
  1. è¢«æ˜ç¡®@æåŠ
  2. æ¶ˆæ¯å†…å®¹æ˜ç¡®æåˆ°"Alice"
  3. æå‡ºæ˜ç¡®é—®é¢˜
  4. ç›´æ¥å›å¤ Alice çš„ä¸Šä¸€æ¡æ¶ˆæ¯

### 5.3 æ™ºèƒ½ä½“å“åº”ç”Ÿæˆ

#### 5.3.1 å“åº”ç”Ÿæˆæµç¨‹
1. åˆ†æä¸Šä¸‹æ–‡å’Œç”¨æˆ·è¾“å…¥
2. æ£€ç´¢ç›¸å…³è®°å¿†
3. è€ƒè™‘æƒ…ç»ªå’Œå¿ƒç†çŠ¶æ€
4. è°ƒç”¨ LLM ç”Ÿæˆå›å¤
5. è§£æå’Œæ¸…ç† LLM è¾“å‡º

#### 5.3.2 å¤šæ¨¡æ€å¤„ç†
- **å›¾ç‰‡**: åˆ†æå›¾ç‰‡å†…å®¹å¹¶ç»“åˆåˆ°å›å¤ä¸­
- **è¡¨æƒ…**: è¯†åˆ«è¡¨æƒ…å¹¶ç”Ÿæˆåˆé€‚çš„å›åº”

### 5.4 ä¸»åŠ¨å¯¹è¯æœºåˆ¶

#### 5.4.1 è§¦å‘æ¡ä»¶
- ä¼šè¯æ²‰é»˜æ—¶é—´è¾¾åˆ°é˜ˆå€¼ï¼ˆç¾¤èŠå’Œç§èŠä¸åŒï¼‰
- è€ƒè™‘æ—¶é—´å› ç´ ï¼ˆé¿å…æ·±å¤œæ‰“æ‰°ï¼‰
- è€ƒè™‘äº²å¯†åº¦ï¼ˆé«˜äº²å¯†åº¦æ›´é¢‘ç¹ä¸»åŠ¨ï¼‰

#### 5.4.2 ä¸»åŠ¨å¯¹è¯é€»è¾‘
```python
async def run_proactive_check(self):
    # æ¯ 60 ç§’æ£€æŸ¥ä¸€æ¬¡
    await asyncio.sleep(60)
    
    # è·å–æ´»è·ƒä¼šè¯
    active_list = await session_manager.get_active_sessions()
    
    for session_id, data in active_list:
        # æ£€æŸ¥æ²‰é»˜æ—¶é—´
        silence_duration = time.time() - data["last_active"]
        
        # æ ¹æ®ä¼šè¯ç±»å‹å’Œæ—¶é—´è®¾ç½®ä¸åŒçš„è§¦å‘æ¡ä»¶
        # ...ï¼ˆçœç•¥å…·ä½“å®ç°ï¼‰
        
        # æ„é€ è¾“å…¥å‚æ•°å¹¶è°ƒç”¨å·¥ä½œæµ
        inputs = {
            "messages": history_msgs,
            "conversation_summary": history_summary,
            "is_proactive_mode": True,
            # ...ï¼ˆå…¶ä»–å‚æ•°ï¼‰
        }
        await self.handle_graph_output(inputs, self_id, msg_type, target_id, last_sender_id)
```

### 5.5 è®°å¿†ç³»ç»Ÿå®ç°

#### 5.5.1 è®°å¿†ç±»å‹
- **çŸ­æœŸè®°å¿†**: æœ€è¿‘çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆä¿å­˜åœ¨ LangGraph çŠ¶æ€ä¸­ï¼‰
- **é•¿æœŸè®°å¿†**: é‡è¦çš„å¯¹è¯å†…å®¹ï¼ˆå­˜å‚¨åœ¨ ChromaDB ä¸­ï¼‰

#### 5.5.2 è®°å¿†æ£€ç´¢ä¸è¯„åˆ†
```python
def search(self, query: str, k: int = 3) -> List[str]:
    # è¯­ä¹‰æœç´¢
    results = self.collection.query(
        query_texts=[query],
        n_results=k * 3,
        include=["documents", "metadatas", "distances"]
    )
    
    # åº”ç”¨æ—¶é—´è¡°å‡å’Œé‡è¦æ€§è¯„åˆ†
    scored_candidates = []
    for doc, meta, dist in zip(docs, metas, dists):
        semantic_score = 1.0 / (1.0 + dist)
        time_score = self._calculate_time_decay(meta.get("created_at"))
        importance = float(meta.get("importance", 1))
        final_score = semantic_score * time_score * (1.0 + importance * 0.15)
        scored_candidates.append((final_score, doc))
    
    # è¿”å›æ’åºåçš„ç»“æœ
    scored_candidates.sort(key=lambda x: x[0], reverse=True)
    return [item[1] for item in scored_candidates[:k]]
```

## 6. æŠ€æœ¯é€‰å‹ä¾æ®

### 6.1 LLMé€‰æ‹©
- **OpenAI API**: æä¾›å¼ºå¤§çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›
- **ç¡…åŸºæµåŠ¨**: å›½å†…å¯è®¿é—®çš„ API æœåŠ¡ï¼Œæ”¯æŒå¤šç§æ¨¡å‹
- **Qwen ç³»åˆ—æ¨¡å‹**: å›½å†…å¼€æºå¤§æ¨¡å‹ï¼Œæ”¯æŒå¤šæ¨¡æ€ï¼Œé€‚åˆæœ¬åœ°åŒ–éƒ¨ç½²

### 6.2 å·¥ä½œæµå¼•æ“é€‰æ‹©
- **LangGraph**: ä¸“ä¸ºæ„å»ºæ™ºèƒ½ä½“å·¥ä½œæµè®¾è®¡ï¼Œæ”¯æŒçŠ¶æ€ç®¡ç†å’Œæ¡ä»¶è·¯ç”±
- **ä¼˜åŠ¿**: å¯è§†åŒ–å·¥ä½œæµã€çµæ´»çš„èŠ‚ç‚¹å®šä¹‰ã€å¼ºå¤§çš„çŠ¶æ€ç®¡ç†

### 6.3 è®°å¿†ç³»ç»Ÿé€‰æ‹©
- **ChromaDB**: è½»é‡çº§å‘é‡æ•°æ®åº“ï¼Œé€‚åˆå­˜å‚¨å’Œæ£€ç´¢æ–‡æœ¬è®°å¿†
- **ä¼˜åŠ¿**: ç®€å•æ˜“ç”¨ã€æ”¯æŒè‡ªå®šä¹‰åµŒå…¥å‡½æ•°ã€æŒä¹…åŒ–å­˜å‚¨

### 6.4 å¼€å‘æ¡†æ¶é€‰æ‹©
- **FastAPI**: é«˜æ€§èƒ½çš„ Web æ¡†æ¶ï¼Œé€‚åˆæ„å»º API æœåŠ¡
- **ä¼˜åŠ¿**: è‡ªåŠ¨ç”Ÿæˆ API æ–‡æ¡£ã€ç±»å‹æç¤ºæ”¯æŒã€å¼‚æ­¥å¤„ç†

## 7. éƒ¨ç½²ä¸é…ç½®

### 7.1 ç¯å¢ƒè¦æ±‚
- Python 3.8+
- ä¾èµ–åº“ï¼šè§ requirements.txt

### 7.2 å®‰è£…æ­¥éª¤
1. å…‹éš†ä»£ç ä»“åº“
2. å®‰è£…ä¾èµ–ï¼š`pip install -r requirements.txt`
3. é…ç½®ç¯å¢ƒå˜é‡ï¼šå¤åˆ¶ .env.example ä¸º .env å¹¶å¡«å†™ç›¸å…³é…ç½®
4. è¿è¡Œ QQ æœåŠ¡å™¨ï¼š`python qq_server.py`

### 7.3 é…ç½®è¯´æ˜

#### 7.3.1 æ¨¡å‹é…ç½®
```
# æ ¸å¿ƒ LLM
LLM_MODEL_NAME=Qwen/Qwen2.5-VL-72B-Instruct

# å°å‹ LLMï¼ˆç”¨äºä¸Šä¸‹æ–‡è¿‡æ»¤ç­‰è½»é‡çº§ä»»åŠ¡ï¼‰
SMALL_LLM_MODEL_NAME=Qwen/Qwen2.5-7B-Instruct

# åµŒå…¥æ¨¡å‹
EMBEDDING_MODEL_NAME=Qwen/Qwen2.5-Embedding
```

#### 7.3.2 å·¥å…·é…ç½®
```
# ç½‘ç»œæœç´¢ API
TAVILY_API_KEY=your_tavily_key_here
```

#### 7.3.3 å­˜å‚¨é…ç½®
```
# å‘é‡æ•°æ®åº“è·¯å¾„
VECTOR_DB_PATH=./data/chroma_db
```

## 8. å¼€å‘æŒ‡å—

### 8.1 ä»£ç é£æ ¼
- éµå¾ª PEP 8 ä»£ç è§„èŒƒ
- ä½¿ç”¨ç±»å‹æç¤º
- ç¼–å†™æ¸…æ™°çš„æ–‡æ¡£å­—ç¬¦ä¸²

### 8.2 è°ƒè¯•æ–¹æ³•
- ä½¿ç”¨ LangSmith è¿›è¡Œå·¥ä½œæµè°ƒè¯•
- æŸ¥çœ‹æ—¥å¿—è¾“å‡º
- ä½¿ç”¨æ–­ç‚¹è°ƒè¯•å™¨

### 8.3 æ‰©å±•å¼€å‘

#### 8.3.1 æ·»åŠ æ–°å·¥å…·
1. åœ¨ app/tools/ ç›®å½•ä¸‹åˆ›å»ºæ–°å·¥å…·æ–‡ä»¶
2. å®ç°å·¥å…·å‡½æ•°
3. åœ¨å·¥ä½œæµä¸­é›†æˆå·¥å…·è°ƒç”¨

#### 8.3.2 æ·»åŠ æ–°å·¥ä½œæµèŠ‚ç‚¹
1. åœ¨ app/graph/nodes/ ç›®å½•ä¸‹åˆ›å»ºæ–°èŠ‚ç‚¹æ–‡ä»¶
2. å®ç°èŠ‚ç‚¹å‡½æ•°
3. åœ¨ graph_builder.py ä¸­æ·»åŠ èŠ‚ç‚¹å’Œè¾¹

## 9. æ€»ç»“ä¸å±•æœ›

### 9.1 é¡¹ç›®æ€»ç»“
ProjectAlice æ„å»ºäº†ä¸€ä¸ªåŠŸèƒ½å®Œæ•´ã€æ¶æ„æ¸…æ™°çš„æ™ºèƒ½èŠå¤©æœºå™¨äººç³»ç»Ÿï¼Œå…·å¤‡å¤šæ¨¡æ€æ„ŸçŸ¥ã€ä¸Šä¸‹æ–‡ç†è§£ã€ä¸»åŠ¨å¯¹è¯å’Œé•¿æœŸè®°å¿†ç­‰æ ¸å¿ƒèƒ½åŠ›ã€‚é¡¹ç›®é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºæ‰©å±•å’Œç»´æŠ¤ã€‚

### 9.2 æœªæ¥å±•æœ›
- æ”¯æŒæ›´å¤šèŠå¤©å¹³å°
- å¢å¼ºå¤šæ¨¡æ€ç†è§£èƒ½åŠ›
- ä¼˜åŒ–è®°å¿†ç³»ç»Ÿæ€§èƒ½
- æä¾›æ›´ä¸°å¯Œçš„å·¥å…·é›†æˆ
- æ”¯æŒä¸ªæ€§åŒ–é…ç½®
- å¢å¼ºå®‰å…¨æœºåˆ¶

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æ›´æ–°æ—¶é—´**: 2026-01-06